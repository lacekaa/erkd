{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T14:25:30.295090Z",
     "start_time": "2024-10-11T14:25:25.646339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: samples\\samples_01\\100001_keystrokes.txt\n",
      "File 100001_keystrokes.txt has 658 rows\n",
      "Processing file: samples\\samples_01\\100003_keystrokes.txt\n",
      "File 100003_keystrokes.txt has 806 rows\n",
      "Processing file: samples\\samples_01\\100007_keystrokes.txt\n",
      "File 100007_keystrokes.txt has 801 rows\n",
      "Processing file: samples\\samples_01\\100008_keystrokes.txt\n",
      "File 100008_keystrokes.txt has 687 rows\n",
      "Processing file: samples\\samples_01\\100013_keystrokes.txt\n",
      "File 100013_keystrokes.txt has 744 rows\n",
      "Processing file: samples\\samples_01\\100016_keystrokes.txt\n",
      "File 100016_keystrokes.txt has 776 rows\n",
      "Processing file: samples\\samples_01\\10001_keystrokes.txt\n",
      "File 10001_keystrokes.txt has 763 rows\n",
      "Processing file: samples\\samples_01\\100020_keystrokes.txt\n",
      "File 100020_keystrokes.txt has 649 rows\n",
      "Processing file: samples\\samples_01\\100030_keystrokes.txt\n",
      "File 100030_keystrokes.txt has 654 rows\n",
      "Processing file: samples\\samples_01\\100031_keystrokes.txt\n",
      "File 100031_keystrokes.txt has 545 rows\n",
      "Processing file: samples\\samples_01\\100032_keystrokes.txt\n",
      "File 100032_keystrokes.txt has 924 rows\n",
      "Processing file: samples\\samples_01\\100033_keystrokes.txt\n",
      "File 100033_keystrokes.txt has 829 rows\n",
      "Processing file: samples\\samples_01\\100045_keystrokes.txt\n",
      "File 100045_keystrokes.txt has 738 rows\n",
      "Processing file: samples\\samples_01\\10004_keystrokes.txt\n",
      "File 10004_keystrokes.txt has 769 rows\n",
      "Processing file: samples\\samples_01\\100050_keystrokes.txt\n",
      "File 100050_keystrokes.txt has 756 rows\n",
      "Processing file: samples\\samples_01\\100056_keystrokes.txt\n",
      "File 100056_keystrokes.txt has 760 rows\n",
      "Processing file: samples\\samples_01\\100059_keystrokes.txt\n",
      "File 100059_keystrokes.txt has 968 rows\n",
      "Processing file: samples\\samples_01\\100060_keystrokes.txt\n",
      "File 100060_keystrokes.txt has 795 rows\n",
      "Processing file: samples\\samples_01\\100068_keystrokes.txt\n",
      "File 100068_keystrokes.txt has 721 rows\n",
      "Processing file: samples\\samples_01\\100072_keystrokes.txt\n",
      "File 100072_keystrokes.txt has 752 rows\n",
      "Processing file: samples\\samples_01\\100076_keystrokes.txt\n",
      "File 100076_keystrokes.txt has 610 rows\n",
      "Processing file: samples\\samples_01\\100077_keystrokes.txt\n",
      "File 100077_keystrokes.txt has 737 rows\n",
      "Processing file: samples\\samples_01\\100081_keystrokes.txt\n",
      "File 100081_keystrokes.txt has 847 rows\n",
      "Processing file: samples\\samples_01\\100084_keystrokes.txt\n",
      "File 100084_keystrokes.txt has 892 rows\n",
      "Processing file: samples\\samples_01\\100085_keystrokes.txt\n",
      "File 100085_keystrokes.txt has 723 rows\n",
      "Processing file: samples\\samples_01\\100088_keystrokes.txt\n",
      "File 100088_keystrokes.txt has 782 rows\n",
      "Processing file: samples\\samples_01\\100091_keystrokes.txt\n",
      "File 100091_keystrokes.txt has 648 rows\n",
      "Processing file: samples\\samples_01\\100094_keystrokes.txt\n",
      "File 100094_keystrokes.txt has 725 rows\n",
      "Processing file: samples\\samples_01\\100097_keystrokes.txt\n",
      "File 100097_keystrokes.txt has 687 rows\n",
      "Processing file: samples\\samples_01\\10009_keystrokes.txt\n",
      "File 10009_keystrokes.txt has 862 rows\n",
      "Processing file: samples\\samples_01\\100102_keystrokes.txt\n",
      "File 100102_keystrokes.txt has 743 rows\n",
      "Processing file: samples\\samples_01\\100104_keystrokes.txt\n",
      "File 100104_keystrokes.txt has 618 rows\n",
      "Processing file: samples\\samples_01\\100106_keystrokes.txt\n",
      "File 100106_keystrokes.txt has 703 rows\n",
      "Processing file: samples\\samples_01\\100108_keystrokes.txt\n",
      "File 100108_keystrokes.txt has 695 rows\n",
      "Processing file: samples\\samples_01\\100116_keystrokes.txt\n",
      "File 100116_keystrokes.txt has 644 rows\n",
      "Processing file: samples\\samples_01\\100120_keystrokes.txt\n",
      "File 100120_keystrokes.txt has 669 rows\n",
      "Processing file: samples\\samples_01\\100122_keystrokes.txt\n",
      "Error processing file samples\\samples_01\\100122_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_01\\100123_keystrokes.txt\n",
      "File 100123_keystrokes.txt has 726 rows\n",
      "Processing file: samples\\samples_01\\100124_keystrokes.txt\n",
      "File 100124_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_01\\100125_keystrokes.txt\n",
      "File 100125_keystrokes.txt has 765 rows\n",
      "Processing file: samples\\samples_01\\100132_keystrokes.txt\n",
      "File 100132_keystrokes.txt has 783 rows\n",
      "Processing file: samples\\samples_01\\100134_keystrokes.txt\n",
      "File 100134_keystrokes.txt has 1143 rows\n",
      "Processing file: samples\\samples_01\\100135_keystrokes.txt\n",
      "File 100135_keystrokes.txt has 975 rows\n",
      "Processing file: samples\\samples_01\\100136_keystrokes.txt\n",
      "File 100136_keystrokes.txt has 582 rows\n",
      "Processing file: samples\\samples_01\\100138_keystrokes.txt\n",
      "File 100138_keystrokes.txt has 701 rows\n",
      "Processing file: samples\\samples_01\\100145_keystrokes.txt\n",
      "File 100145_keystrokes.txt has 788 rows\n",
      "Processing file: samples\\samples_01\\100151_keystrokes.txt\n",
      "File 100151_keystrokes.txt has 618 rows\n",
      "Processing file: samples\\samples_01\\100156_keystrokes.txt\n",
      "File 100156_keystrokes.txt has 644 rows\n",
      "Processing file: samples\\samples_01\\100157_keystrokes.txt\n",
      "File 100157_keystrokes.txt has 767 rows\n",
      "Processing file: samples\\samples_01\\100159_keystrokes.txt\n",
      "File 100159_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_01\\100162_keystrokes.txt\n",
      "File 100162_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_01\\100163_keystrokes.txt\n",
      "File 100163_keystrokes.txt has 816 rows\n",
      "Processing file: samples\\samples_01\\100165_keystrokes.txt\n",
      "File 100165_keystrokes.txt has 648 rows\n",
      "Processing file: samples\\samples_01\\100166_keystrokes.txt\n",
      "File 100166_keystrokes.txt has 737 rows\n",
      "Processing file: samples\\samples_01\\100167_keystrokes.txt\n",
      "File 100167_keystrokes.txt has 747 rows\n",
      "Processing file: samples\\samples_01\\100168_keystrokes.txt\n",
      "File 100168_keystrokes.txt has 880 rows\n",
      "Processing file: samples\\samples_01\\100169_keystrokes.txt\n",
      "File 100169_keystrokes.txt has 723 rows\n",
      "Processing file: samples\\samples_01\\100180_keystrokes.txt\n",
      "File 100180_keystrokes.txt has 636 rows\n",
      "Processing file: samples\\samples_01\\100182_keystrokes.txt\n",
      "File 100182_keystrokes.txt has 825 rows\n",
      "Processing file: samples\\samples_01\\100183_keystrokes.txt\n",
      "Error processing file samples\\samples_01\\100183_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_01\\100184_keystrokes.txt\n",
      "File 100184_keystrokes.txt has 693 rows\n",
      "Processing file: samples\\samples_01\\100185_keystrokes.txt\n",
      "File 100185_keystrokes.txt has 735 rows\n",
      "Processing file: samples\\samples_01\\100186_keystrokes.txt\n",
      "File 100186_keystrokes.txt has 596 rows\n",
      "Processing file: samples\\samples_01\\100188_keystrokes.txt\n",
      "File 100188_keystrokes.txt has 605 rows\n",
      "Processing file: samples\\samples_01\\100190_keystrokes.txt\n",
      "File 100190_keystrokes.txt has 734 rows\n",
      "Processing file: samples\\samples_01\\100191_keystrokes.txt\n",
      "File 100191_keystrokes.txt has 809 rows\n",
      "Processing file: samples\\samples_01\\100193_keystrokes.txt\n",
      "File 100193_keystrokes.txt has 728 rows\n",
      "Processing file: samples\\samples_01\\100197_keystrokes.txt\n",
      "File 100197_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_01\\10019_keystrokes.txt\n",
      "File 10019_keystrokes.txt has 615 rows\n",
      "Processing file: samples\\samples_01\\100206_keystrokes.txt\n",
      "File 100206_keystrokes.txt has 776 rows\n",
      "Processing file: samples\\samples_01\\100210_keystrokes.txt\n",
      "File 100210_keystrokes.txt has 620 rows\n",
      "Processing file: samples\\samples_01\\100211_keystrokes.txt\n",
      "File 100211_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_01\\100215_keystrokes.txt\n",
      "File 100215_keystrokes.txt has 686 rows\n",
      "Processing file: samples\\samples_01\\100224_keystrokes.txt\n",
      "File 100224_keystrokes.txt has 630 rows\n",
      "Processing file: samples\\samples_01\\100227_keystrokes.txt\n",
      "File 100227_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_01\\100230_keystrokes.txt\n",
      "File 100230_keystrokes.txt has 580 rows\n",
      "Processing file: samples\\samples_01\\100232_keystrokes.txt\n",
      "File 100232_keystrokes.txt has 588 rows\n",
      "Processing file: samples\\samples_01\\10023_keystrokes.txt\n",
      "File 10023_keystrokes.txt has 710 rows\n",
      "Processing file: samples\\samples_01\\100249_keystrokes.txt\n",
      "File 100249_keystrokes.txt has 784 rows\n",
      "Processing file: samples\\samples_01\\10024_keystrokes.txt\n",
      "File 10024_keystrokes.txt has 599 rows\n",
      "Processing file: samples\\samples_01\\100252_keystrokes.txt\n",
      "File 100252_keystrokes.txt has 827 rows\n",
      "Processing file: samples\\samples_01\\100253_keystrokes.txt\n",
      "File 100253_keystrokes.txt has 731 rows\n",
      "Processing file: samples\\samples_01\\100256_keystrokes.txt\n",
      "File 100256_keystrokes.txt has 672 rows\n",
      "Processing file: samples\\samples_01\\100257_keystrokes.txt\n",
      "File 100257_keystrokes.txt has 685 rows\n",
      "Processing file: samples\\samples_01\\100262_keystrokes.txt\n",
      "File 100262_keystrokes.txt has 875 rows\n",
      "Processing file: samples\\samples_01\\100264_keystrokes.txt\n",
      "File 100264_keystrokes.txt has 720 rows\n",
      "Processing file: samples\\samples_01\\100269_keystrokes.txt\n",
      "File 100269_keystrokes.txt has 831 rows\n",
      "Processing file: samples\\samples_01\\10026_keystrokes.txt\n",
      "File 10026_keystrokes.txt has 713 rows\n",
      "Processing file: samples\\samples_01\\100273_keystrokes.txt\n",
      "File 100273_keystrokes.txt has 794 rows\n",
      "Processing file: samples\\samples_01\\100275_keystrokes.txt\n",
      "File 100275_keystrokes.txt has 724 rows\n",
      "Processing file: samples\\samples_01\\100278_keystrokes.txt\n",
      "File 100278_keystrokes.txt has 766 rows\n",
      "Processing file: samples\\samples_01\\10027_keystrokes.txt\n",
      "File 10027_keystrokes.txt has 597 rows\n",
      "Processing file: samples\\samples_01\\100280_keystrokes.txt\n",
      "File 100280_keystrokes.txt has 750 rows\n",
      "Processing file: samples\\samples_01\\100281_keystrokes.txt\n",
      "File 100281_keystrokes.txt has 684 rows\n",
      "Processing file: samples\\samples_01\\100282_keystrokes.txt\n",
      "File 100282_keystrokes.txt has 730 rows\n",
      "Processing file: samples\\samples_01\\100283_keystrokes.txt\n",
      "File 100283_keystrokes.txt has 709 rows\n",
      "Processing file: samples\\samples_01\\100289_keystrokes.txt\n",
      "File 100289_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_01\\100294_keystrokes.txt\n",
      "File 100294_keystrokes.txt has 786 rows\n",
      "Processing file: samples\\samples_01\\100298_keystrokes.txt\n",
      "File 100298_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_01\\1002_keystrokes.txt\n",
      "File 1002_keystrokes.txt has 755 rows\n",
      "Total rows in concatenated DataFrame: 71461\n",
      "Total malformed rows removed: 0\n",
      "Valid data saved to demographics_csv/part1_uncleaned_all.csv\n",
      "Total number of .txt files processed: 100\n",
      "Total number of rows processed: 71461\n",
      "Total number of skipped files: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "      PARTICIPANT_ID  TEST_SECTION_ID  \\\n0             100001          1090979   \n1             100001          1090979   \n2             100001          1090979   \n3             100001          1090979   \n4             100001          1090979   \n...              ...              ...   \n71456           1002            10502   \n71457           1002            10502   \n71458           1002            10502   \n71459           1002            10502   \n71460           1002            10502   \n\n                                          SENTENCE  \\\n0      Was wondering if you and Natalie connected?   \n1      Was wondering if you and Natalie connected?   \n2      Was wondering if you and Natalie connected?   \n3      Was wondering if you and Natalie connected?   \n4      Was wondering if you and Natalie connected?   \n...                                            ...   \n71456                Thursday works better for me.   \n71457                Thursday works better for me.   \n71458                Thursday works better for me.   \n71459                Thursday works better for me.   \n71460                Thursday works better for me.   \n\n                                        USER_INPUT  KEYSTROKE_ID  \\\n0      Was wondering if you and Natalie connected?    51891207.0   \n1      Was wondering if you and Natalie connected?    51891214.0   \n2      Was wondering if you and Natalie connected?    51891219.0   \n3      Was wondering if you and Natalie connected?    51891226.0   \n4      Was wondering if you and Natalie connected?    51891231.0   \n...                                            ...           ...   \n71456                Thursday works better for me.      502839.0   \n71457                Thursday works better for me.      502843.0   \n71458                Thursday works better for me.      502846.0   \n71459                Thursday works better for me.      502858.0   \n71460                Thursday works better for me.      502862.0   \n\n         PRESS_TIME   RELEASE_TIME LETTER  KEYCODE  \n0      1.473275e+12  1473275372663  SHIFT       16  \n1      1.473275e+12  1473275372703      W       87  \n2      1.473275e+12  1473275372903      a       65  \n3      1.473275e+12  1473275372975      s       83  \n4      1.473275e+12  1473275373079              32  \n...             ...            ...    ...      ...  \n71456  1.471949e+12  1471948648921      r       82  \n71457  1.471949e+12  1471948648973              32  \n71458  1.471949e+12  1471948649104      m       77  \n71459  1.471949e+12  1471948649190      e       69  \n71460  1.471949e+12  1471948649297      .      190  \n\n[71461 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PARTICIPANT_ID</th>\n      <th>TEST_SECTION_ID</th>\n      <th>SENTENCE</th>\n      <th>USER_INPUT</th>\n      <th>KEYSTROKE_ID</th>\n      <th>PRESS_TIME</th>\n      <th>RELEASE_TIME</th>\n      <th>LETTER</th>\n      <th>KEYCODE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891207.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372663</td>\n      <td>SHIFT</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891214.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372703</td>\n      <td>W</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891219.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372903</td>\n      <td>a</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891226.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372975</td>\n      <td>s</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891231.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275373079</td>\n      <td></td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71456</th>\n      <td>1002</td>\n      <td>10502</td>\n      <td>Thursday works better for me.</td>\n      <td>Thursday works better for me.</td>\n      <td>502839.0</td>\n      <td>1.471949e+12</td>\n      <td>1471948648921</td>\n      <td>r</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>71457</th>\n      <td>1002</td>\n      <td>10502</td>\n      <td>Thursday works better for me.</td>\n      <td>Thursday works better for me.</td>\n      <td>502843.0</td>\n      <td>1.471949e+12</td>\n      <td>1471948648973</td>\n      <td></td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>71458</th>\n      <td>1002</td>\n      <td>10502</td>\n      <td>Thursday works better for me.</td>\n      <td>Thursday works better for me.</td>\n      <td>502846.0</td>\n      <td>1.471949e+12</td>\n      <td>1471948649104</td>\n      <td>m</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>71459</th>\n      <td>1002</td>\n      <td>10502</td>\n      <td>Thursday works better for me.</td>\n      <td>Thursday works better for me.</td>\n      <td>502858.0</td>\n      <td>1.471949e+12</td>\n      <td>1471948649190</td>\n      <td>e</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>71460</th>\n      <td>1002</td>\n      <td>10502</td>\n      <td>Thursday works better for me.</td>\n      <td>Thursday works better for me.</td>\n      <td>502862.0</td>\n      <td>1.471949e+12</td>\n      <td>1471948649297</td>\n      <td>.</td>\n      <td>190</td>\n    </tr>\n  </tbody>\n</table>\n<p>71461 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Read and process 10,000 .txt files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def try_multiple_delimiters(file_path):\n",
    "    \"\"\"\n",
    "    Attempts to read a file using different delimiters.\n",
    "    \"\"\"\n",
    "    delimiters = ['\\t', ',', ';']  # Common delimiters to try\n",
    "    for delimiter in delimiters:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=delimiter, encoding='utf-8', on_bad_lines='skip', dtype={'PARTICIPANT_ID': str})\n",
    "            # If we get more than one column, we assume we have the right delimiter\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            pass  # Try the next delimiter\n",
    "    raise ValueError(\"Could not determine delimiter\")\n",
    "\n",
    "def read_keystroke_data(samples_dir, output_csv='all_samples_combined1.csv', start_batch=1, end_batch=100):\n",
    "    \"\"\"\n",
    "    Reads keystroke data from multiple subfolders, handles errors, and saves valid data to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - samples_dir (str): The directory where the 'samples' folder resides, which contains multiple subfolders with .txt files.\n",
    "    - output_csv (str): The output path for the CSV file to save valid data.\n",
    "    - start_batch (int): The starting batch number (inclusive).\n",
    "    - end_batch (int): The ending batch number (inclusive).\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing valid keystroke data.\n",
    "    \"\"\"\n",
    "    all_data = []  # To hold data from all valid files\n",
    "    total_files_processed = 0  # Counter to keep track of the total number of files processed\n",
    "    total_rows_processed = 0  # Counter for the total number of rows across all files\n",
    "    skipped_files = 0  # Counter for the number of skipped files\n",
    "    malformed_rows = 0  # Counter for the number of malformed rows\n",
    "\n",
    "    # Expected columns in the file\n",
    "    required_columns = ['PARTICIPANT_ID', 'TEST_SECTION_ID', 'SENTENCE', 'USER_INPUT',\n",
    "                        'KEYSTROKE_ID', 'PRESS_TIME', 'RELEASE_TIME', 'LETTER', 'KEYCODE']\n",
    "\n",
    "    # Traverse each subfolder (e.g., samples_01, samples_02, ..., samples_100)\n",
    "    for batch_num in range(start_batch, end_batch + 1):\n",
    "        subdir = f'samples_{batch_num:02d}'\n",
    "        subdir_path = os.path.join(samples_dir, subdir)\n",
    "\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Traverse each .txt file in the subfolder\n",
    "            for file_name in os.listdir(subdir_path):\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = os.path.join(subdir_path, file_name)\n",
    "                    total_files_processed += 1  # Increment the counter for each file\n",
    "\n",
    "                    print(f\"Processing file: {file_path}\")  # Debugging statement\n",
    "\n",
    "                    try:\n",
    "                        # Try reading the file using multiple delimiters\n",
    "                        df = try_multiple_delimiters(file_path)\n",
    "\n",
    "                        # Log the number of rows in the current file\n",
    "                        print(f\"File {file_name} has {df.shape[0]} rows\")\n",
    "\n",
    "                        # Ensure required columns exist\n",
    "                        if not all(col in df.columns for col in required_columns):\n",
    "                            print(f\"Missing columns in {file_path}\")\n",
    "                            skipped_files += 1\n",
    "                            continue  # Skip this file if it doesn't have the required columns\n",
    "\n",
    "                        # If the number of columns is 1, it means that the file might be incorrectly formatted\n",
    "                        if len(df.columns) == 1:\n",
    "                            print(f\"Malformed data in {file_path}, skipping.\")\n",
    "                            skipped_files += 1\n",
    "                            continue  # Skip files with misformatted rows\n",
    "\n",
    "                        # Filter out rows where all values are under one column\n",
    "                        malformed_df = df[df.apply(lambda row: row.count() == 1, axis=1)]\n",
    "                        malformed_rows += len(malformed_df)\n",
    "                        df = df[df.apply(lambda row: row.count() > 1, axis=1)]\n",
    "\n",
    "                        # Append the number of rows to the total rows processed\n",
    "                        total_rows_processed += df.shape[0]\n",
    "\n",
    "                        # Append the valid DataFrame to the list\n",
    "                        all_data.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")\n",
    "                        skipped_files += 1\n",
    "                        continue  # Skip files with errors like encoding issues or missing data\n",
    "\n",
    "    # Concatenate all valid data into one DataFrame\n",
    "    if not all_data:\n",
    "        print(\"No valid data found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no valid data was found\n",
    "\n",
    "    full_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Verify the number of rows before saving to CSV\n",
    "    print(f\"Total rows in concatenated DataFrame: {full_data.shape[0]}\")\n",
    "    print(f\"Total malformed rows removed: {malformed_rows}\")\n",
    "\n",
    "    # Save the data to CSV\n",
    "    full_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Valid data saved to {output_csv}\")\n",
    "\n",
    "    # Print the total number of files processed and total rows processed\n",
    "    print(f\"Total number of .txt files processed: {total_files_processed}\")\n",
    "    print(f\"Total number of rows processed: {total_rows_processed}\")\n",
    "    print(f\"Total number of skipped files: {skipped_files}\")\n",
    "\n",
    "    return full_data\n",
    "\n",
    "# Example usage:\n",
    "samples_directory = \"samples\"  # Replace with your actual path\n",
    "read_keystroke_data(samples_directory, output_csv='demographics_csv/part1_uncleaned_all.csv', start_batch=1, end_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total malformed rows removed: 0\n",
      "Malformed rows:\n",
      "Empty DataFrame\n",
      "Columns: [PARTICIPANT_ID, TEST_SECTION_ID, SENTENCE, USER_INPUT, KEYSTROKE_ID, PRESS_TIME, RELEASE_TIME, LETTER, KEYCODE]\n",
      "Index: []\n",
      "Cleaned data saved to demographics_csv/part1_cleaned_samples_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clean the CSV file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Cleans a CSV file by removing rows where more than one column has NaN or <unset> values.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save cleaned data.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv, sep=',')\n",
    "\n",
    "    # Define a function to check for NaN or <unset> values\n",
    "    def is_unset_or_nan(value):\n",
    "        return pd.isna(value) or value == '<unset>'\n",
    "\n",
    "    # Identify and filter out rows where more than one column has NaN or <unset> values\n",
    "    malformed_rows = df[df.apply(lambda row: sum(is_unset_or_nan(val) for val in row) > 1, axis=1)]\n",
    "    cleaned_df = df[df.apply(lambda row: sum(is_unset_or_nan(val) for val in row) <= 1, axis=1)]\n",
    "\n",
    "    # Log the number of malformed rows removed\n",
    "    print(f\"Total malformed rows removed: {len(malformed_rows)}\")\n",
    "    print(\"Malformed rows:\")\n",
    "    print(malformed_rows)\n",
    "\n",
    "    # Save the cleaned DataFrame to a new CSV file\n",
    "    cleaned_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Cleaned data saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/part1_uncleaned_all.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/part1_cleaned_samples_combined.csv'  # Replace with your desired output CSV path\n",
    "clean_csv(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T14:25:35.768926Z",
     "start_time": "2024-10-11T14:25:34.181775Z"
    }
   },
   "id": "7dd6f8d31b8411ba",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data saved to demographics_csv/final5_keystroke_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_new_columns(group):\n",
    "    # Create the new columns based on transformations of the KEYPRESS_ID\n",
    "    group['D1U1'] = group['KEYSTROKE_ID'] * 1.1\n",
    "    group['D1U2'] = group['KEYSTROKE_ID'] * 1.2\n",
    "    group['D1U3'] = group['KEYSTROKE_ID'] * 1.3\n",
    "    group['D1D2'] = group['KEYSTROKE_ID'] * 1.4\n",
    "    group['D1D3'] = group['KEYSTROKE_ID'] * 1.5\n",
    "    group['U1D2'] = group['D1U1'] - group['D1D2']\n",
    "    \n",
    "    # Calculate Z-scores for each of the columns\n",
    "    for col in ['D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'U1D2']:\n",
    "        group[f'{col}_Z_SCORE'] = (group[col] - group[col].mean()) / group[col].std()\n",
    "    \n",
    "    return group\n",
    "\n",
    "def calculate_mean_values(df):\n",
    "    # Calculate mean values for the columns\n",
    "    mean_values = df.groupby('PARTICIPANT_ID').agg({\n",
    "        'D1U1': 'mean',\n",
    "        'D1U2': 'mean',\n",
    "        'D1U3': 'mean',\n",
    "        'D1D2': 'mean',\n",
    "        'D1D3': 'mean',\n",
    "        'U1D2': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate the mean of Z-scores for each column\n",
    "    z_score_columns = ['D1U1_Z_SCORE', 'D1U2_Z_SCORE', 'D1U3_Z_SCORE', 'D1D2_Z_SCORE', 'D1D3_Z_SCORE', 'U1D2_Z_SCORE']\n",
    "    z_score_mean = df.groupby('PARTICIPANT_ID')[z_score_columns].mean().reset_index()\n",
    "\n",
    "    # Merge the mean values with the Z_SCORE mean values\n",
    "    mean_values = pd.merge(mean_values, z_score_mean, on='PARTICIPANT_ID')\n",
    "\n",
    "    # Renaming the columns for clarity\n",
    "    mean_values.columns = [\n",
    "        'PARTICIPANT_ID', 'D1U1_MEAN', 'D1U2_MEAN', 'D1U3_MEAN',\n",
    "        'D1D2_MEAN', 'D1D3_MEAN', 'U1D2_MEAN',\n",
    "        'D1U1_Z_SCORE_MEAN', 'D1U2_Z_SCORE_MEAN', 'D1U3_Z_SCORE_MEAN',\n",
    "        'D1D2_Z_SCORE_MEAN', 'D1D3_Z_SCORE_MEAN', 'U1D2_Z_SCORE_MEAN'\n",
    "    ]\n",
    "    \n",
    "    return mean_values\n",
    "\n",
    "def process_keystroke_data(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Apply the calculation to each participant group\n",
    "    df = df.groupby('PARTICIPANT_ID').apply(calculate_new_columns)\n",
    "    \n",
    "    # Calculate the mean values and mean Z-scores\n",
    "    mean_values = calculate_mean_values(df)\n",
    "    \n",
    "    # Save the final results to a CSV file\n",
    "    mean_values.to_csv(output_csv, index=False)\n",
    "    print(f\"Final data saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/part1_cleaned_samples_combined.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/final5_keystroke_features.csv'  # Replace with your desired output CSV path\n",
    "process_keystroke_data(input_csv_path, output_csv_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-13T14:40:49.414712Z",
     "start_time": "2024-10-13T14:40:48.682039Z"
    }
   },
   "id": "b5379a44fb7f77d3",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to demographics_csv/metadata_extended.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_with_metadata(keystroke_csv, metadata_csv, output_csv):\n",
    "    # Read the input CSV files\n",
    "    keystroke_df = pd.read_csv(keystroke_csv)\n",
    "    metadata_df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    # Merge the DataFrames on PARTICIPANT_ID\n",
    "    merged_df = pd.merge(keystroke_df, metadata_df, on='PARTICIPANT_ID')\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged data saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "keystroke_csv_path = 'demographics_csv/final5_keystroke_features.csv'  # Replace with your actual keystroke CSV path\n",
    "metadata_csv_path = 'big_keystroke_data/metadata_participants.csv'  # Replace with your actual metadata CSV path\n",
    "output_csv_path = 'demographics_csv/metadata_extended.csv'  # Replace with your desired output CSV path\n",
    "merge_with_metadata(keystroke_csv_path, metadata_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-13T14:52:07.806945Z",
     "start_time": "2024-10-13T14:52:07.522997Z"
    }
   },
   "id": "156f0367af32c4fa",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_all_csv_files(input_folder, output_csv):\n",
    "    # List to hold all DataFrames\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_dataframes.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(output_csv, index=False)\n",
    "    print(f\"All data merged and saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "input_folder_path = 'path/to/your/folder'  # Replace with your actual folder path\n",
    "output_csv_path = 'metadata_master.csv'  # Replace with your desired output CSV path\n",
    "merge_all_csv_files(input_folder_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84b018cdd3d97a25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

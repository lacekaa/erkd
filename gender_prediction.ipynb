{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.0 Read local folder, make subfolders\n",
    "### Read all txt files from local folder on pc and divide into folders containing 100 files each"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a003ca9073776d93"
  },
  {
   "cell_type": "code",
   "source": [
    "# from a local folder read 10k files and put them in project here\n",
    "# can adjust file amount \n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def organize_samples(source_dir, project_dir, total_files=10000, files_per_folder=100):\n",
    "    \"\"\"\n",
    "    Organizes .txt files from the source directory into multiple folders\n",
    "    in the 'samples' folder in the project directory, each containing a specified number of files.\n",
    "\n",
    "    Parameters:\n",
    "    - source_dir (str): The path to the directory containing the original .txt files.\n",
    "    - project_dir (str): The root path to your project directory (which contains the 'samples' folder).\n",
    "    - total_files (int): The total number of files to process (default is 10,000).\n",
    "    - files_per_folder (int): The number of files per folder (default is 100).\n",
    "    \"\"\"\n",
    "    # Path to the 'samples' folder in the project directory\n",
    "    samples_dir = os.path.join(project_dir, 'samples')\n",
    "\n",
    "    # Ensure the 'samples' directory exists in the project directory\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "\n",
    "    # Get a list of all .txt files in the source directory\n",
    "    all_files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n",
    "\n",
    "    # Sort the files (optional, depending on whether you want them in a specific order)\n",
    "    all_files.sort()\n",
    "\n",
    "    # Limit to the total number of files specified\n",
    "    files_to_copy = all_files[:total_files]\n",
    "\n",
    "    folder_number = 1\n",
    "    file_count = 0\n",
    "    total_copied = 0\n",
    "\n",
    "    # Create the first folder (samples_01)\n",
    "    current_folder_name = f'samples_{folder_number:02d}'\n",
    "    current_folder_path = os.path.join(samples_dir, current_folder_name)\n",
    "    os.makedirs(current_folder_path, exist_ok=True)\n",
    "\n",
    "    for file_name in files_to_copy:\n",
    "        source_file = os.path.join(source_dir, file_name)\n",
    "        target_file = os.path.join(current_folder_path, file_name)\n",
    "\n",
    "        # Copy the file\n",
    "        shutil.copy2(source_file, target_file)\n",
    "        file_count += 1\n",
    "        total_copied += 1\n",
    "\n",
    "        # Check if the current folder has reached the desired number of files\n",
    "        if file_count >= files_per_folder:\n",
    "            folder_number += 1\n",
    "            if total_copied >= total_files:\n",
    "                break  # Stop if we've copied the total desired number of files\n",
    "            # Reset file count and create a new folder\n",
    "            file_count = 0\n",
    "            current_folder_name = f'samples_{folder_number:02d}'\n",
    "            current_folder_path = os.path.join(samples_dir, current_folder_name)\n",
    "            os.makedirs(current_folder_path, exist_ok=True)\n",
    "\n",
    "    print(f'Total files copied: {total_copied}')\n",
    "    print(f'Files organized into {folder_number} folders.')\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "source_directory = r\"C:\\Users\\laras\\Downloads\\Keystrokes\\Keystrokes\\files\"\n",
    "project_directory = 'samples'  # Replace with the root path to your PyCharm project\n",
    "\n",
    "organize_samples(source_directory, project_directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:53:35.820596Z",
     "start_time": "2024-09-10T13:51:28.770410Z"
    }
   },
   "id": "c17144531c9b37a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files copied: 10000\n",
      "Files organized into 101 folders.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.0 Read txt files, make csv file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb4f0ef7f4cff0c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:15:59.765666Z",
     "start_time": "2024-09-11T22:15:49.923026Z"
    }
   },
   "source": [
    "# generate a csv file with all the data from the files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def try_multiple_delimiters(file_path):\n",
    "    \"\"\"\n",
    "    Attempts to read a file using different delimiters.\n",
    "    \"\"\"\n",
    "    delimiters = ['\\t', ',', ';']  # Common delimiters to try\n",
    "    for delimiter in delimiters:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=delimiter, encoding='utf-8', on_bad_lines='skip',\n",
    "                             dtype={'PARTICIPANT_ID': str})\n",
    "            # If we get more than one column, we assume we have the right delimiter\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            pass  # Try the next delimiter\n",
    "    raise ValueError(\"Could not determine delimiter\")\n",
    "\n",
    "\n",
    "def read_keystroke_data(samples_dir, output_csv='all_samples_combined1.csv'):\n",
    "    \"\"\"\n",
    "    Reads keystroke data from multiple subfolders, handles errors, and saves valid data to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - samples_dir (str): The directory where the 'samples' folder resides, which contains multiple subfolders with .txt files.\n",
    "    - output_csv (str): The output path for the CSV file to save valid data.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing valid keystroke data.\n",
    "    \"\"\"\n",
    "    all_data = []  # To hold data from all valid files\n",
    "    total_files_processed = 0  # Counter to keep track of the total number of files processed\n",
    "    total_rows_processed = 0  # Counter for the total number of rows across all files\n",
    "    skipped_files = 0  # Counter for the number of skipped files\n",
    "    malformed_rows = 0  # Counter for the number of malformed rows\n",
    "\n",
    "    # Expected columns in the file\n",
    "    required_columns = ['PARTICIPANT_ID', 'TEST_SECTION_ID', 'SENTENCE', 'USER_INPUT',\n",
    "                        'KEYSTROKE_ID', 'PRESS_TIME', 'RELEASE_TIME', 'LETTER', 'KEYCODE']\n",
    "\n",
    "    # Traverse each subfolder (e.g., samples_01, samples_02, ..., samples_100)\n",
    "    for subdir in os.listdir(samples_dir):\n",
    "        subdir_path = os.path.join(samples_dir, subdir)\n",
    "\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Traverse each .txt file in the subfolder\n",
    "            for file_name in os.listdir(subdir_path):\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = os.path.join(subdir_path, file_name)\n",
    "                    total_files_processed += 1  # Increment the counter for each file\n",
    "\n",
    "                    print(f\"Processing file: {file_path}\")  # Debugging statement\n",
    "\n",
    "                    try:\n",
    "                        # Try reading the file using multiple delimiters\n",
    "                        df = try_multiple_delimiters(file_path)\n",
    "\n",
    "                        # Log the number of rows in the current file\n",
    "                        print(f\"File {file_name} has {df.shape[0]} rows\")\n",
    "\n",
    "                        # Ensure required columns exist\n",
    "                        if not all(col in df.columns for col in required_columns):\n",
    "                            print(f\"Missing columns in {file_path}\")\n",
    "                            skipped_files += 1\n",
    "                            continue  # Skip this file if it doesn't have the required columns\n",
    "\n",
    "                        # If the number of columns is 1, it means that the file might be incorrectly formatted\n",
    "                        if len(df.columns) == 1:\n",
    "                            print(f\"Malformed data in {file_path}, skipping.\")\n",
    "                            skipped_files += 1\n",
    "                            continue  # Skip files with misformatted rows\n",
    "\n",
    "                        # Filter out rows where all values are under one column\n",
    "                        malformed_df = df[df.apply(lambda row: row.count() == 1, axis=1)]\n",
    "                        malformed_rows += len(malformed_df)\n",
    "                        df = df[df.apply(lambda row: row.count() > 1, axis=1)]\n",
    "\n",
    "                        # Append the number of rows to the total rows processed\n",
    "                        total_rows_processed += df.shape[0]\n",
    "\n",
    "                        # Append the valid DataFrame to the list\n",
    "                        all_data.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")\n",
    "                        skipped_files += 1\n",
    "                        continue  # Skip files with errors like encoding issues or missing data\n",
    "\n",
    "    # Concatenate all valid data into one DataFrame\n",
    "    if not all_data:\n",
    "        print(\"No valid data found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no valid data was found\n",
    "\n",
    "    full_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Verify the number of rows before saving to CSV\n",
    "    print(f\"Total rows in concatenated DataFrame: {full_data.shape[0]}\")\n",
    "    print(f\"Total malformed rows removed: {malformed_rows}\")\n",
    "\n",
    "    # Save the data to CSV\n",
    "    full_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Valid data saved to {output_csv}\")\n",
    "\n",
    "    # Print the total number of files processed and total rows processed\n",
    "    print(f\"Total number of .txt files processed: {total_files_processed}\")\n",
    "    print(f\"Total number of rows processed: {total_rows_processed}\")\n",
    "    print(f\"Total number of skipped files: {skipped_files}\")\n",
    "\n",
    "    return full_data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "samples_directory = \"samples\"  # Replace with your actual path\n",
    "read_keystroke_data(samples_directory, output_csv='demographics_csv/uncleaned_all.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: samples\\samples_01\\100001_keystrokes.txt\n",
      "File 100001_keystrokes.txt has 658 rows\n",
      "Processing file: samples\\samples_01\\100003_keystrokes.txt\n",
      "File 100003_keystrokes.txt has 806 rows\n",
      "Processing file: samples\\samples_01\\100007_keystrokes.txt\n",
      "File 100007_keystrokes.txt has 801 rows\n",
      "Processing file: samples\\samples_01\\100008_keystrokes.txt\n",
      "File 100008_keystrokes.txt has 687 rows\n",
      "Processing file: samples\\samples_01\\100013_keystrokes.txt\n",
      "File 100013_keystrokes.txt has 744 rows\n",
      "Processing file: samples\\samples_01\\100016_keystrokes.txt\n",
      "File 100016_keystrokes.txt has 776 rows\n",
      "Processing file: samples\\samples_01\\10001_keystrokes.txt\n",
      "File 10001_keystrokes.txt has 763 rows\n",
      "Processing file: samples\\samples_01\\100020_keystrokes.txt\n",
      "File 100020_keystrokes.txt has 649 rows\n",
      "Processing file: samples\\samples_01\\100030_keystrokes.txt\n",
      "File 100030_keystrokes.txt has 654 rows\n",
      "Processing file: samples\\samples_01\\100031_keystrokes.txt\n",
      "File 100031_keystrokes.txt has 545 rows\n",
      "Processing file: samples\\samples_01\\100032_keystrokes.txt\n",
      "File 100032_keystrokes.txt has 924 rows\n",
      "Processing file: samples\\samples_01\\100033_keystrokes.txt\n",
      "File 100033_keystrokes.txt has 829 rows\n",
      "Processing file: samples\\samples_01\\100045_keystrokes.txt\n",
      "File 100045_keystrokes.txt has 738 rows\n",
      "Processing file: samples\\samples_01\\10004_keystrokes.txt\n",
      "File 10004_keystrokes.txt has 769 rows\n",
      "Processing file: samples\\samples_01\\100050_keystrokes.txt\n",
      "File 100050_keystrokes.txt has 756 rows\n",
      "Processing file: samples\\samples_01\\100056_keystrokes.txt\n",
      "File 100056_keystrokes.txt has 760 rows\n",
      "Processing file: samples\\samples_01\\100059_keystrokes.txt\n",
      "File 100059_keystrokes.txt has 968 rows\n",
      "Processing file: samples\\samples_01\\100060_keystrokes.txt\n",
      "File 100060_keystrokes.txt has 795 rows\n",
      "Processing file: samples\\samples_01\\100068_keystrokes.txt\n",
      "File 100068_keystrokes.txt has 721 rows\n",
      "Processing file: samples\\samples_01\\100072_keystrokes.txt\n",
      "File 100072_keystrokes.txt has 752 rows\n",
      "Processing file: samples\\samples_01\\100076_keystrokes.txt\n",
      "File 100076_keystrokes.txt has 610 rows\n",
      "Processing file: samples\\samples_01\\100077_keystrokes.txt\n",
      "File 100077_keystrokes.txt has 737 rows\n",
      "Processing file: samples\\samples_01\\100081_keystrokes.txt\n",
      "File 100081_keystrokes.txt has 847 rows\n",
      "Processing file: samples\\samples_01\\100084_keystrokes.txt\n",
      "File 100084_keystrokes.txt has 892 rows\n",
      "Processing file: samples\\samples_01\\100085_keystrokes.txt\n",
      "File 100085_keystrokes.txt has 723 rows\n",
      "Processing file: samples\\samples_01\\100088_keystrokes.txt\n",
      "File 100088_keystrokes.txt has 782 rows\n",
      "Processing file: samples\\samples_01\\100091_keystrokes.txt\n",
      "File 100091_keystrokes.txt has 648 rows\n",
      "Processing file: samples\\samples_01\\100094_keystrokes.txt\n",
      "File 100094_keystrokes.txt has 725 rows\n",
      "Processing file: samples\\samples_01\\100097_keystrokes.txt\n",
      "File 100097_keystrokes.txt has 687 rows\n",
      "Processing file: samples\\samples_01\\10009_keystrokes.txt\n",
      "File 10009_keystrokes.txt has 862 rows\n",
      "Processing file: samples\\samples_01\\100102_keystrokes.txt\n",
      "File 100102_keystrokes.txt has 743 rows\n",
      "Processing file: samples\\samples_01\\100104_keystrokes.txt\n",
      "File 100104_keystrokes.txt has 618 rows\n",
      "Processing file: samples\\samples_01\\100106_keystrokes.txt\n",
      "File 100106_keystrokes.txt has 703 rows\n",
      "Processing file: samples\\samples_01\\100108_keystrokes.txt\n",
      "File 100108_keystrokes.txt has 695 rows\n",
      "Processing file: samples\\samples_01\\100116_keystrokes.txt\n",
      "File 100116_keystrokes.txt has 644 rows\n",
      "Processing file: samples\\samples_01\\100120_keystrokes.txt\n",
      "File 100120_keystrokes.txt has 669 rows\n",
      "Processing file: samples\\samples_01\\100122_keystrokes.txt\n",
      "Error processing file samples\\samples_01\\100122_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_01\\100123_keystrokes.txt\n",
      "File 100123_keystrokes.txt has 726 rows\n",
      "Processing file: samples\\samples_01\\100124_keystrokes.txt\n",
      "File 100124_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_01\\100125_keystrokes.txt\n",
      "File 100125_keystrokes.txt has 765 rows\n",
      "Processing file: samples\\samples_01\\100132_keystrokes.txt\n",
      "File 100132_keystrokes.txt has 783 rows\n",
      "Processing file: samples\\samples_01\\100134_keystrokes.txt\n",
      "File 100134_keystrokes.txt has 1143 rows\n",
      "Processing file: samples\\samples_01\\100135_keystrokes.txt\n",
      "File 100135_keystrokes.txt has 975 rows\n",
      "Processing file: samples\\samples_01\\100136_keystrokes.txt\n",
      "File 100136_keystrokes.txt has 582 rows\n",
      "Processing file: samples\\samples_01\\100138_keystrokes.txt\n",
      "File 100138_keystrokes.txt has 701 rows\n",
      "Processing file: samples\\samples_01\\100145_keystrokes.txt\n",
      "File 100145_keystrokes.txt has 788 rows\n",
      "Processing file: samples\\samples_01\\100151_keystrokes.txt\n",
      "File 100151_keystrokes.txt has 618 rows\n",
      "Processing file: samples\\samples_01\\100156_keystrokes.txt\n",
      "File 100156_keystrokes.txt has 644 rows\n",
      "Processing file: samples\\samples_01\\100157_keystrokes.txt\n",
      "File 100157_keystrokes.txt has 767 rows\n",
      "Processing file: samples\\samples_01\\100159_keystrokes.txt\n",
      "File 100159_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_01\\100162_keystrokes.txt\n",
      "File 100162_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_01\\100163_keystrokes.txt\n",
      "File 100163_keystrokes.txt has 816 rows\n",
      "Processing file: samples\\samples_01\\100165_keystrokes.txt\n",
      "File 100165_keystrokes.txt has 648 rows\n",
      "Processing file: samples\\samples_01\\100166_keystrokes.txt\n",
      "File 100166_keystrokes.txt has 737 rows\n",
      "Processing file: samples\\samples_01\\100167_keystrokes.txt\n",
      "File 100167_keystrokes.txt has 747 rows\n",
      "Processing file: samples\\samples_01\\100168_keystrokes.txt\n",
      "File 100168_keystrokes.txt has 880 rows\n",
      "Processing file: samples\\samples_01\\100169_keystrokes.txt\n",
      "File 100169_keystrokes.txt has 723 rows\n",
      "Processing file: samples\\samples_01\\100180_keystrokes.txt\n",
      "File 100180_keystrokes.txt has 636 rows\n",
      "Processing file: samples\\samples_01\\100182_keystrokes.txt\n",
      "File 100182_keystrokes.txt has 825 rows\n",
      "Processing file: samples\\samples_01\\100183_keystrokes.txt\n",
      "Error processing file samples\\samples_01\\100183_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_01\\100184_keystrokes.txt\n",
      "File 100184_keystrokes.txt has 693 rows\n",
      "Processing file: samples\\samples_01\\100185_keystrokes.txt\n",
      "File 100185_keystrokes.txt has 735 rows\n",
      "Processing file: samples\\samples_01\\100186_keystrokes.txt\n",
      "File 100186_keystrokes.txt has 596 rows\n",
      "Processing file: samples\\samples_01\\100188_keystrokes.txt\n",
      "File 100188_keystrokes.txt has 605 rows\n",
      "Processing file: samples\\samples_01\\100190_keystrokes.txt\n",
      "File 100190_keystrokes.txt has 734 rows\n",
      "Processing file: samples\\samples_01\\100191_keystrokes.txt\n",
      "File 100191_keystrokes.txt has 809 rows\n",
      "Processing file: samples\\samples_01\\100193_keystrokes.txt\n",
      "File 100193_keystrokes.txt has 728 rows\n",
      "Processing file: samples\\samples_01\\100197_keystrokes.txt\n",
      "File 100197_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_01\\10019_keystrokes.txt\n",
      "File 10019_keystrokes.txt has 615 rows\n",
      "Processing file: samples\\samples_01\\100206_keystrokes.txt\n",
      "File 100206_keystrokes.txt has 776 rows\n",
      "Processing file: samples\\samples_01\\100210_keystrokes.txt\n",
      "File 100210_keystrokes.txt has 620 rows\n",
      "Processing file: samples\\samples_01\\100211_keystrokes.txt\n",
      "File 100211_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_01\\100215_keystrokes.txt\n",
      "File 100215_keystrokes.txt has 686 rows\n",
      "Processing file: samples\\samples_01\\100224_keystrokes.txt\n",
      "File 100224_keystrokes.txt has 630 rows\n",
      "Processing file: samples\\samples_01\\100227_keystrokes.txt\n",
      "File 100227_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_01\\100230_keystrokes.txt\n",
      "File 100230_keystrokes.txt has 580 rows\n",
      "Processing file: samples\\samples_01\\100232_keystrokes.txt\n",
      "File 100232_keystrokes.txt has 588 rows\n",
      "Processing file: samples\\samples_01\\10023_keystrokes.txt\n",
      "File 10023_keystrokes.txt has 710 rows\n",
      "Processing file: samples\\samples_01\\100249_keystrokes.txt\n",
      "File 100249_keystrokes.txt has 784 rows\n",
      "Processing file: samples\\samples_01\\10024_keystrokes.txt\n",
      "File 10024_keystrokes.txt has 599 rows\n",
      "Processing file: samples\\samples_01\\100252_keystrokes.txt\n",
      "File 100252_keystrokes.txt has 827 rows\n",
      "Processing file: samples\\samples_01\\100253_keystrokes.txt\n",
      "File 100253_keystrokes.txt has 731 rows\n",
      "Processing file: samples\\samples_01\\100256_keystrokes.txt\n",
      "File 100256_keystrokes.txt has 672 rows\n",
      "Processing file: samples\\samples_01\\100257_keystrokes.txt\n",
      "File 100257_keystrokes.txt has 685 rows\n",
      "Processing file: samples\\samples_01\\100262_keystrokes.txt\n",
      "File 100262_keystrokes.txt has 875 rows\n",
      "Processing file: samples\\samples_01\\100264_keystrokes.txt\n",
      "File 100264_keystrokes.txt has 720 rows\n",
      "Processing file: samples\\samples_01\\100269_keystrokes.txt\n",
      "File 100269_keystrokes.txt has 831 rows\n",
      "Processing file: samples\\samples_01\\10026_keystrokes.txt\n",
      "File 10026_keystrokes.txt has 713 rows\n",
      "Processing file: samples\\samples_01\\100273_keystrokes.txt\n",
      "File 100273_keystrokes.txt has 794 rows\n",
      "Processing file: samples\\samples_01\\100275_keystrokes.txt\n",
      "File 100275_keystrokes.txt has 724 rows\n",
      "Processing file: samples\\samples_01\\100278_keystrokes.txt\n",
      "File 100278_keystrokes.txt has 766 rows\n",
      "Processing file: samples\\samples_01\\10027_keystrokes.txt\n",
      "File 10027_keystrokes.txt has 597 rows\n",
      "Processing file: samples\\samples_01\\100280_keystrokes.txt\n",
      "File 100280_keystrokes.txt has 750 rows\n",
      "Processing file: samples\\samples_01\\100281_keystrokes.txt\n",
      "File 100281_keystrokes.txt has 684 rows\n",
      "Processing file: samples\\samples_01\\100282_keystrokes.txt\n",
      "File 100282_keystrokes.txt has 730 rows\n",
      "Processing file: samples\\samples_01\\100283_keystrokes.txt\n",
      "File 100283_keystrokes.txt has 709 rows\n",
      "Processing file: samples\\samples_01\\100289_keystrokes.txt\n",
      "File 100289_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_01\\100294_keystrokes.txt\n",
      "File 100294_keystrokes.txt has 786 rows\n",
      "Processing file: samples\\samples_01\\100298_keystrokes.txt\n",
      "File 100298_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_01\\1002_keystrokes.txt\n",
      "File 1002_keystrokes.txt has 755 rows\n",
      "Processing file: samples\\samples_02\\100302_keystrokes.txt\n",
      "File 100302_keystrokes.txt has 680 rows\n",
      "Processing file: samples\\samples_02\\100306_keystrokes.txt\n",
      "File 100306_keystrokes.txt has 717 rows\n",
      "Processing file: samples\\samples_02\\100311_keystrokes.txt\n",
      "File 100311_keystrokes.txt has 738 rows\n",
      "Processing file: samples\\samples_02\\100320_keystrokes.txt\n",
      "File 100320_keystrokes.txt has 706 rows\n",
      "Processing file: samples\\samples_02\\100327_keystrokes.txt\n",
      "File 100327_keystrokes.txt has 671 rows\n",
      "Processing file: samples\\samples_02\\100329_keystrokes.txt\n",
      "File 100329_keystrokes.txt has 730 rows\n",
      "Processing file: samples\\samples_02\\100330_keystrokes.txt\n",
      "File 100330_keystrokes.txt has 697 rows\n",
      "Processing file: samples\\samples_02\\100332_keystrokes.txt\n",
      "File 100332_keystrokes.txt has 694 rows\n",
      "Processing file: samples\\samples_02\\100333_keystrokes.txt\n",
      "File 100333_keystrokes.txt has 678 rows\n",
      "Processing file: samples\\samples_02\\100334_keystrokes.txt\n",
      "File 100334_keystrokes.txt has 685 rows\n",
      "Processing file: samples\\samples_02\\100338_keystrokes.txt\n",
      "File 100338_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_02\\100339_keystrokes.txt\n",
      "File 100339_keystrokes.txt has 854 rows\n",
      "Processing file: samples\\samples_02\\100341_keystrokes.txt\n",
      "File 100341_keystrokes.txt has 577 rows\n",
      "Processing file: samples\\samples_02\\100348_keystrokes.txt\n",
      "File 100348_keystrokes.txt has 613 rows\n",
      "Processing file: samples\\samples_02\\10034_keystrokes.txt\n",
      "File 10034_keystrokes.txt has 824 rows\n",
      "Processing file: samples\\samples_02\\100350_keystrokes.txt\n",
      "File 100350_keystrokes.txt has 626 rows\n",
      "Processing file: samples\\samples_02\\100353_keystrokes.txt\n",
      "File 100353_keystrokes.txt has 652 rows\n",
      "Processing file: samples\\samples_02\\100356_keystrokes.txt\n",
      "File 100356_keystrokes.txt has 759 rows\n",
      "Processing file: samples\\samples_02\\100365_keystrokes.txt\n",
      "File 100365_keystrokes.txt has 677 rows\n",
      "Processing file: samples\\samples_02\\100367_keystrokes.txt\n",
      "File 100367_keystrokes.txt has 695 rows\n",
      "Processing file: samples\\samples_02\\10036_keystrokes.txt\n",
      "File 10036_keystrokes.txt has 797 rows\n",
      "Processing file: samples\\samples_02\\100375_keystrokes.txt\n",
      "File 100375_keystrokes.txt has 681 rows\n",
      "Processing file: samples\\samples_02\\100379_keystrokes.txt\n",
      "File 100379_keystrokes.txt has 765 rows\n",
      "Processing file: samples\\samples_02\\100380_keystrokes.txt\n",
      "File 100380_keystrokes.txt has 748 rows\n",
      "Processing file: samples\\samples_02\\100381_keystrokes.txt\n",
      "File 100381_keystrokes.txt has 672 rows\n",
      "Processing file: samples\\samples_02\\100382_keystrokes.txt\n",
      "File 100382_keystrokes.txt has 621 rows\n",
      "Processing file: samples\\samples_02\\100383_keystrokes.txt\n",
      "File 100383_keystrokes.txt has 707 rows\n",
      "Processing file: samples\\samples_02\\100386_keystrokes.txt\n",
      "File 100386_keystrokes.txt has 742 rows\n",
      "Processing file: samples\\samples_02\\100390_keystrokes.txt\n",
      "File 100390_keystrokes.txt has 686 rows\n",
      "Processing file: samples\\samples_02\\100395_keystrokes.txt\n",
      "File 100395_keystrokes.txt has 619 rows\n",
      "Processing file: samples\\samples_02\\100396_keystrokes.txt\n",
      "File 100396_keystrokes.txt has 855 rows\n",
      "Processing file: samples\\samples_02\\100397_keystrokes.txt\n",
      "File 100397_keystrokes.txt has 626 rows\n",
      "Processing file: samples\\samples_02\\100410_keystrokes.txt\n",
      "File 100410_keystrokes.txt has 798 rows\n",
      "Processing file: samples\\samples_02\\100416_keystrokes.txt\n",
      "File 100416_keystrokes.txt has 672 rows\n",
      "Processing file: samples\\samples_02\\100417_keystrokes.txt\n",
      "File 100417_keystrokes.txt has 678 rows\n",
      "Processing file: samples\\samples_02\\100419_keystrokes.txt\n",
      "File 100419_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_02\\100420_keystrokes.txt\n",
      "File 100420_keystrokes.txt has 748 rows\n",
      "Processing file: samples\\samples_02\\100421_keystrokes.txt\n",
      "File 100421_keystrokes.txt has 745 rows\n",
      "Processing file: samples\\samples_02\\100422_keystrokes.txt\n",
      "File 100422_keystrokes.txt has 828 rows\n",
      "Processing file: samples\\samples_02\\100423_keystrokes.txt\n",
      "File 100423_keystrokes.txt has 878 rows\n",
      "Processing file: samples\\samples_02\\100425_keystrokes.txt\n",
      "File 100425_keystrokes.txt has 753 rows\n",
      "Processing file: samples\\samples_02\\100426_keystrokes.txt\n",
      "File 100426_keystrokes.txt has 773 rows\n",
      "Processing file: samples\\samples_02\\100431_keystrokes.txt\n",
      "File 100431_keystrokes.txt has 655 rows\n",
      "Processing file: samples\\samples_02\\100432_keystrokes.txt\n",
      "File 100432_keystrokes.txt has 815 rows\n",
      "Processing file: samples\\samples_02\\100434_keystrokes.txt\n",
      "File 100434_keystrokes.txt has 638 rows\n",
      "Processing file: samples\\samples_02\\100438_keystrokes.txt\n",
      "File 100438_keystrokes.txt has 662 rows\n",
      "Processing file: samples\\samples_02\\100439_keystrokes.txt\n",
      "File 100439_keystrokes.txt has 733 rows\n",
      "Processing file: samples\\samples_02\\100444_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100444_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100445_keystrokes.txt\n",
      "File 100445_keystrokes.txt has 635 rows\n",
      "Processing file: samples\\samples_02\\100446_keystrokes.txt\n",
      "File 100446_keystrokes.txt has 839 rows\n",
      "Processing file: samples\\samples_02\\100447_keystrokes.txt\n",
      "File 100447_keystrokes.txt has 641 rows\n",
      "Processing file: samples\\samples_02\\100450_keystrokes.txt\n",
      "File 100450_keystrokes.txt has 734 rows\n",
      "Processing file: samples\\samples_02\\100452_keystrokes.txt\n",
      "File 100452_keystrokes.txt has 683 rows\n",
      "Processing file: samples\\samples_02\\100456_keystrokes.txt\n",
      "File 100456_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_02\\100458_keystrokes.txt\n",
      "File 100458_keystrokes.txt has 868 rows\n",
      "Processing file: samples\\samples_02\\100460_keystrokes.txt\n",
      "File 100460_keystrokes.txt has 690 rows\n",
      "Processing file: samples\\samples_02\\100461_keystrokes.txt\n",
      "File 100461_keystrokes.txt has 634 rows\n",
      "Processing file: samples\\samples_02\\100462_keystrokes.txt\n",
      "File 100462_keystrokes.txt has 693 rows\n",
      "Processing file: samples\\samples_02\\100463_keystrokes.txt\n",
      "File 100463_keystrokes.txt has 789 rows\n",
      "Processing file: samples\\samples_02\\100466_keystrokes.txt\n",
      "File 100466_keystrokes.txt has 768 rows\n",
      "Processing file: samples\\samples_02\\100467_keystrokes.txt\n",
      "File 100467_keystrokes.txt has 638 rows\n",
      "Processing file: samples\\samples_02\\100470_keystrokes.txt\n",
      "File 100470_keystrokes.txt has 704 rows\n",
      "Processing file: samples\\samples_02\\100471_keystrokes.txt\n",
      "File 100471_keystrokes.txt has 735 rows\n",
      "Processing file: samples\\samples_02\\100473_keystrokes.txt\n",
      "File 100473_keystrokes.txt has 718 rows\n",
      "Processing file: samples\\samples_02\\100474_keystrokes.txt\n",
      "File 100474_keystrokes.txt has 684 rows\n",
      "Processing file: samples\\samples_02\\100475_keystrokes.txt\n",
      "File 100475_keystrokes.txt has 810 rows\n",
      "Processing file: samples\\samples_02\\100476_keystrokes.txt\n",
      "File 100476_keystrokes.txt has 874 rows\n",
      "Processing file: samples\\samples_02\\100477_keystrokes.txt\n",
      "File 100477_keystrokes.txt has 676 rows\n",
      "Processing file: samples\\samples_02\\100480_keystrokes.txt\n",
      "File 100480_keystrokes.txt has 659 rows\n",
      "Processing file: samples\\samples_02\\100485_keystrokes.txt\n",
      "File 100485_keystrokes.txt has 802 rows\n",
      "Processing file: samples\\samples_02\\100487_keystrokes.txt\n",
      "File 100487_keystrokes.txt has 715 rows\n",
      "Processing file: samples\\samples_02\\100488_keystrokes.txt\n",
      "File 100488_keystrokes.txt has 828 rows\n",
      "Processing file: samples\\samples_02\\100491_keystrokes.txt\n",
      "File 100491_keystrokes.txt has 731 rows\n",
      "Processing file: samples\\samples_02\\100492_keystrokes.txt\n",
      "File 100492_keystrokes.txt has 676 rows\n",
      "Processing file: samples\\samples_02\\100495_keystrokes.txt\n",
      "File 100495_keystrokes.txt has 751 rows\n",
      "Processing file: samples\\samples_02\\100496_keystrokes.txt\n",
      "File 100496_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_02\\100498_keystrokes.txt\n",
      "File 100498_keystrokes.txt has 745 rows\n",
      "Processing file: samples\\samples_02\\100499_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100499_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100500_keystrokes.txt\n",
      "File 100500_keystrokes.txt has 681 rows\n",
      "Processing file: samples\\samples_02\\100501_keystrokes.txt\n",
      "File 100501_keystrokes.txt has 769 rows\n",
      "Processing file: samples\\samples_02\\100502_keystrokes.txt\n",
      "File 100502_keystrokes.txt has 825 rows\n",
      "Processing file: samples\\samples_02\\100503_keystrokes.txt\n",
      "File 100503_keystrokes.txt has 770 rows\n",
      "Processing file: samples\\samples_02\\100504_keystrokes.txt\n",
      "File 100504_keystrokes.txt has 781 rows\n",
      "Processing file: samples\\samples_02\\100510_keystrokes.txt\n",
      "File 100510_keystrokes.txt has 841 rows\n",
      "Processing file: samples\\samples_02\\100512_keystrokes.txt\n",
      "File 100512_keystrokes.txt has 621 rows\n",
      "Processing file: samples\\samples_02\\100514_keystrokes.txt\n",
      "File 100514_keystrokes.txt has 678 rows\n",
      "Processing file: samples\\samples_02\\100517_keystrokes.txt\n",
      "File 100517_keystrokes.txt has 825 rows\n",
      "Processing file: samples\\samples_02\\100519_keystrokes.txt\n",
      "File 100519_keystrokes.txt has 732 rows\n",
      "Processing file: samples\\samples_02\\10051_keystrokes.txt\n",
      "File 10051_keystrokes.txt has 621 rows\n",
      "Processing file: samples\\samples_02\\100521_keystrokes.txt\n",
      "File 100521_keystrokes.txt has 749 rows\n",
      "Processing file: samples\\samples_02\\100522_keystrokes.txt\n",
      "File 100522_keystrokes.txt has 654 rows\n",
      "Processing file: samples\\samples_02\\100524_keystrokes.txt\n",
      "File 100524_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_02\\100527_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100527_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100528_keystrokes.txt\n",
      "File 100528_keystrokes.txt has 707 rows\n",
      "Processing file: samples\\samples_02\\100532_keystrokes.txt\n",
      "File 100532_keystrokes.txt has 674 rows\n",
      "Processing file: samples\\samples_02\\10053_keystrokes.txt\n",
      "File 10053_keystrokes.txt has 591 rows\n",
      "Processing file: samples\\samples_02\\100542_keystrokes.txt\n",
      "File 100542_keystrokes.txt has 659 rows\n",
      "Processing file: samples\\samples_02\\100545_keystrokes.txt\n",
      "File 100545_keystrokes.txt has 697 rows\n",
      "Processing file: samples\\samples_02\\100549_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100549_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100550_keystrokes.txt\n",
      "File 100550_keystrokes.txt has 789 rows\n",
      "Total rows in concatenated DataFrame: 140441\n",
      "Total malformed rows removed: 0\n",
      "Valid data saved to demographics_csv/uncleaned_all.csv\n",
      "Total number of .txt files processed: 200\n",
      "Total number of rows processed: 140441\n",
      "Total number of skipped files: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "       PARTICIPANT_ID  TEST_SECTION_ID  \\\n0              100001          1090979   \n1              100001          1090979   \n2              100001          1090979   \n3              100001          1090979   \n4              100001          1090979   \n...               ...              ...   \n140436         100550          1097261   \n140437         100550          1097261   \n140438         100550          1097261   \n140439         100550          1097261   \n140440         100550          1097261   \n\n                                                 SENTENCE  \\\n0             Was wondering if you and Natalie connected?   \n1             Was wondering if you and Natalie connected?   \n2             Was wondering if you and Natalie connected?   \n3             Was wondering if you and Natalie connected?   \n4             Was wondering if you and Natalie connected?   \n...                                                   ...   \n140436  You must therefore take full responsibility fo...   \n140437  You must therefore take full responsibility fo...   \n140438  You must therefore take full responsibility fo...   \n140439  You must therefore take full responsibility fo...   \n140440  You must therefore take full responsibility fo...   \n\n                                               USER_INPUT  KEYSTROKE_ID  \\\n0             Was wondering if you and Natalie connected?    51891207.0   \n1             Was wondering if you and Natalie connected?    51891214.0   \n2             Was wondering if you and Natalie connected?    51891219.0   \n3             Was wondering if you and Natalie connected?    51891226.0   \n4             Was wondering if you and Natalie connected?    51891231.0   \n...                                                   ...           ...   \n140436  You must therefore take full responsibility fo...    52190490.0   \n140437  You must therefore take full responsibility fo...    52190496.0   \n140438  You must therefore take full responsibility fo...    52190503.0   \n140439  You must therefore take full responsibility fo...    52190720.0   \n140440  You must therefore take full responsibility fo...    52190726.0   \n\n          PRESS_TIME   RELEASE_TIME LETTER  KEYCODE  \n0       1.473275e+12  1473275372663  SHIFT       16  \n1       1.473275e+12  1473275372703      W       87  \n2       1.473275e+12  1473275372903      a       65  \n3       1.473275e+12  1473275372975      s       83  \n4       1.473275e+12  1473275373079              32  \n...              ...            ...    ...      ...  \n140436  1.473280e+12  1473280428505      i       73  \n140437  1.473280e+12  1473280428527      n       78  \n140438  1.473280e+12  1473280428607      g       71  \n140439  1.473280e+12  1473280428786      .      190  \n140440  1.473280e+12  1473280428837              32  \n\n[140441 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PARTICIPANT_ID</th>\n      <th>TEST_SECTION_ID</th>\n      <th>SENTENCE</th>\n      <th>USER_INPUT</th>\n      <th>KEYSTROKE_ID</th>\n      <th>PRESS_TIME</th>\n      <th>RELEASE_TIME</th>\n      <th>LETTER</th>\n      <th>KEYCODE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891207.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372663</td>\n      <td>SHIFT</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891214.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372703</td>\n      <td>W</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891219.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372903</td>\n      <td>a</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891226.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372975</td>\n      <td>s</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891231.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275373079</td>\n      <td></td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140436</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190490.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428505</td>\n      <td>i</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>140437</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190496.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428527</td>\n      <td>n</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>140438</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190503.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428607</td>\n      <td>g</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>140439</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190720.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428786</td>\n      <td>.</td>\n      <td>190</td>\n    </tr>\n    <tr>\n      <th>140440</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190726.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428837</td>\n      <td></td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n<p>140441 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# clean the csv file by removing rows with more than one column with NaN or <unset> values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_csv(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Cleans a CSV file by removing rows where more than one column has NaN or <unset> values.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save cleaned data.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv, sep=',')\n",
    "\n",
    "    # Define a function to check for NaN or <unset> values\n",
    "    def is_unset_or_nan(value):\n",
    "        return pd.isna(value) or value == '<unset>'\n",
    "\n",
    "    # Identify and filter out rows where more than one column has NaN or <unset> values\n",
    "    malformed_rows = df[df.apply(lambda row: sum(is_unset_or_nan(val) for val in row) > 1, axis=1)]\n",
    "    cleaned_df = df[df.apply(lambda row: sum(is_unset_or_nan(val) for val in row) <= 1, axis=1)]\n",
    "\n",
    "    # Log the number of malformed rows removed\n",
    "    print(f\"Total malformed rows removed: {len(malformed_rows)}\")\n",
    "    print(\"Malformed rows:\")\n",
    "    print(malformed_rows)\n",
    "\n",
    "    # Save the cleaned DataFrame to a new CSV file\n",
    "    cleaned_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Cleaned data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/uncleaned_all.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/cleaned_samples_combined.csv'  # Replace with your desired output CSV path\n",
    "clean_csv(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:16:06.305879Z",
     "start_time": "2024-09-11T22:16:03.229353Z"
    }
   },
   "id": "4abe0a38f109b65b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total malformed rows removed: 0\n",
      "Malformed rows:\n",
      "Empty DataFrame\n",
      "Columns: [PARTICIPANT_ID, TEST_SECTION_ID, SENTENCE, USER_INPUT, KEYSTROKE_ID, PRESS_TIME, RELEASE_TIME, LETTER, KEYCODE]\n",
      "Index: []\n",
      "Cleaned data saved to demographics_csv/cleaned_samples_combined.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 adding keystroke values\n",
    "### generate D1U2, D1U3, D1D2, D1D3, D1U1_MEAN, D1U2_MEAN, D1U3_MEAN, D1D2_MEAN, D1D3_MEAN, U1D2, U1D2_MEAN, Z_SCORE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5cc67700d630cc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T22:16:12.820199Z",
     "start_time": "2024-09-11T22:16:09.567204Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to demographics_csv/added_key_values.csv\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_new_columns(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Adds new columns to a CSV file for each participant and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Define a function to calculate new columns for each participant\n",
    "    def calculate_new_columns(group):\n",
    "        group['D1U1'] = group['KEYSTROKE_ID'] * 1.1\n",
    "        group['D1U2'] = group['KEYSTROKE_ID'] * 1.2\n",
    "        group['D1U3'] = group['KEYSTROKE_ID'] * 1.3\n",
    "        group['D1D2'] = group['KEYSTROKE_ID'] * 1.4\n",
    "        group['D1D3'] = group['KEYSTROKE_ID'] * 1.5\n",
    "        group['D1U1_MEAN'] = group['D1U1'].mean()\n",
    "        group['D1U2_MEAN'] = group['D1U2'].mean()\n",
    "        group['D1U3_MEAN'] = group['D1U3'].mean()\n",
    "        group['D1D2_MEAN'] = group['D1D2'].mean()\n",
    "        group['D1D3_MEAN'] = group['D1D3'].mean()\n",
    "        group['U1D2'] = group['D1U1'] - group['D1D2']\n",
    "        group['U1D2_MEAN'] = group['U1D2'].mean()\n",
    "        group['Z_SCORE'] = (group['KEYSTROKE_ID'] - group['KEYSTROKE_ID'].mean()) / group['KEYSTROKE_ID'].std()\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each participant group\n",
    "    df = df.groupby('PARTICIPANT_ID').apply(calculate_new_columns)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/cleaned_samples_combined.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/added_key_values.csv'  # Replace with your desired output CSV path\n",
    "add_new_columns(input_csv_path, output_csv_path)"
   ],
   "id": "c19c57d86e6f7146"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to demographics_csv/enhanced_keystroke_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_keystroke_features(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Adds new keystroke features to the CSV file and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # # Define left and right hand keys using their numeric KEYCODE values\n",
    "    # left_hand_keys = {16, 65, 68, 69, 70, 71, 83, 87, 88, 90}  # Example keycodes for left hand\n",
    "    # right_hand_keys = {32, 73, 78, 79, 82, 85, 89}  # Example keycodes for right hand\n",
    "\n",
    "    left_hand_keys = {16, 65, 66, 67, 68, 69, 70, 71, 81, 82, 83, 84, 86, 87, 88, 90, 49, 50, 51, 52, 53, 9, 20,\n",
    "                      190}  # Keycodes for left hand\n",
    "    right_hand_keys = {16, 32, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 89, 48, 54, 55, 56, 57, 8, 13, 189, 191, 188,\n",
    "                       191}  # Keycodes for right hand\n",
    "\n",
    "    # left = 0, right = 1\n",
    "    # Function to determine hand\n",
    "    def determine_hand(key):\n",
    "        if key in left_hand_keys:\n",
    "            return '0'\n",
    "        elif key in right_hand_keys:\n",
    "            return '1'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "\n",
    "    # Apply the function to determine hand for each key\n",
    "    df['HAND'] = df['KEYCODE'].apply(determine_hand)\n",
    "\n",
    "    # Calculate mean hold time for each hand\n",
    "    hand_hold_time = df.groupby('HAND')['D1U1'].mean().to_dict()\n",
    "\n",
    "    # Map the mean hold time back to the DataFrame\n",
    "    df['HAND_HOLD_TIME'] = df['HAND'].map(hand_hold_time)\n",
    "\n",
    "    # Calculate Keystroke Duration Variability\n",
    "    df['KEY_HOLD_TIME_STD'] = df.groupby('KEYCODE')['D1U1'].transform('std')\n",
    "\n",
    "    # Calculate Error Rate and Correction Features\n",
    "    df['ERROR_RATE'] = df['KEYCODE'].apply(lambda x: 1 if x == 8 else 0)  # Assuming 8 is the keycode for backspace\n",
    "    df['ERROR_RATE'] = df['ERROR_RATE'].cumsum() / (df.index + 1)\n",
    "\n",
    "    # Calculate Consecutive Key Patterns\n",
    "    df['CONSECUTIVE_KEYS'] = df['KEYCODE'].astype(str) + df['KEYCODE'].shift(-1).astype(str)\n",
    "    df['CONSECUTIVE_KEYS_TIME'] = df['D1U1'].shift(-1) - df['D1D2']\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/added_key_values.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with your desired output CSV path\n",
    "add_keystroke_features(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:16:24.172716Z",
     "start_time": "2024-09-11T22:16:19.659713Z"
    }
   },
   "id": "902c00b760b4679c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_12636\\3017895694.py:24: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  drop_unknown_hand(input_csv_path, output_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to demographics_csv/enhanced_keystroke_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def drop_unknown_hand(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Drops all rows where the value in the HAND column is 'unknown' and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Drop rows where the HAND column value is 'unknown'\n",
    "    df = df[df['HAND'] != 'unknown']\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with your desired output CSV path\n",
    "drop_unknown_hand(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:16:30.570296Z",
     "start_time": "2024-09-11T22:16:26.374298Z"
    }
   },
   "id": "a6c2d0d3f0b8adbc",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_16820\\1028204779.py:30: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  add_speed_classification(input_file, output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed classification added. File saved to: demographics_csv/enhanced_keystroke_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_speed_classification(file_path, output_file):\n",
    "    \"\"\"\n",
    "    This function adds a new feature 'Speed_Class' to classify participants based on their D1D2_MEAN values.\n",
    "    Participants are classified from 1 (slowest) to 10 (fastest) based on percentiles of D1D2_MEAN.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the input CSV file.\n",
    "    - output_file (str): The path where the output CSV file with the new feature will be saved.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Calculate the deciles based on D1D2_MEAN\n",
    "    df['SPEED_CLASS'] = pd.qcut(df['D1D2_MEAN'], q=10, labels=False, duplicates='drop') + 1\n",
    "\n",
    "    # Invert the speed class to assign 10 to the fastest (lowest D1D2_MEAN values) and 1 to the slowest\n",
    "    df['SPEED_CLASS'] = 11 - df['SPEED_CLASS']\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Speed classification added. File saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with the path to your CSV file\n",
    "output_file = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with the path where you want to save the new CSV file\n",
    "\n",
    "add_speed_classification(input_file, output_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:17:49.077591Z",
     "start_time": "2024-09-11T22:17:44.911081Z"
    }
   },
   "id": "5d9eee820951ffaf",
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T17:22:47.344986Z",
     "start_time": "2024-09-10T17:22:47.316024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # this might erase outliers where values are too slow or fast\n",
    "# # to be altered and used if needed, TODO: columns to check must be altered\n",
    "# import pandas as pd\n",
    "# \n",
    "# def remove_outliers(input_csv, output_csv):\n",
    "#     \"\"\"\n",
    "#     Removes rows with extreme outliers (2.5% fastest and 2.5% slowest) from a CSV file and saves the updated data to a new CSV file.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - input_csv (str): The path to the input CSV file.\n",
    "#     - output_csv (str): The path to the output CSV file to save updated data.\n",
    "#     \"\"\"\n",
    "#     # Read the cleaned CSV file into a DataFrame\n",
    "#     df = pd.read_csv(input_csv)\n",
    "# \n",
    "#     # Define the columns to check for outliers\n",
    "#     columns_to_check = ['KEYSTROKE_ID']  # Add other relevant columns if needed\n",
    "# \n",
    "#     # Calculate the 2.5% and 97.5% percentiles for each column\n",
    "#     lower_bound = df[columns_to_check].quantile(0.025)\n",
    "#     upper_bound = df[columns_to_check].quantile(0.975)\n",
    "# \n",
    "#     # Filter out rows with values outside the 2.5% to 97.5% range\n",
    "#     df_filtered = df[(df[columns_to_check] >= lower_bound) & (df[columns_to_check] <= upper_bound)].dropna()\n",
    "# \n",
    "#     # Save the updated DataFrame to a new CSV file\n",
    "#     df_filtered.to_csv(output_csv, index=False)\n",
    "#     print(f\"Updated data saved to {output_csv}\")\n",
    "# \n",
    "# # Example usage:\n",
    "# input_csv_path = 'added_key_values.csv'  # Replace with your actual input CSV path\n",
    "# output_csv_path = 'filtered_key_values.csv'  # Replace with your desired output CSV path\n",
    "# remove_outliers(input_csv_path, output_csv_path)"
   ],
   "id": "dcefb048bc4a264d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T23:59:34.976574Z",
     "start_time": "2024-09-10T23:59:34.964860Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [],
   "id": "394d31c3b97894ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.0 Clean metadata_participants.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "127343d0580a1819"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to demographics_csv/cleaned_metadata_participants.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_metadata_participants(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Cleans the metadata_participants CSV file by removing rows with <null> or <unset> values,\n",
    "    rows where AGE is less than 10, and rows where GENDER is 'none'.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save cleaned data.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Drop rows with <null> or <unset> values\n",
    "    df = df.replace(['<null>', '<unset>'], pd.NA).dropna()\n",
    "\n",
    "    # Drop rows where AGE is less than 10\n",
    "    df = df[df['AGE'] >= 10]\n",
    "\n",
    "    # Drop rows where GENDER is 'none'\n",
    "    df = df[df['GENDER'] != 'none']\n",
    "\n",
    "    # Save the cleaned DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Cleaned data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'big_keystroke_data/metadata_participants.csv'  # Path to the input CSV file\n",
    "output_csv_path = 'demographics_csv/cleaned_metadata_participants.csv'  # Path to the output CSV file\n",
    "clean_metadata_participants(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:17:59.001399Z",
     "start_time": "2024-09-11T22:17:56.009876Z"
    }
   },
   "id": "6cc28a4ed472d11d",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.0 Merge keystroke data with metadata"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69111791452c7b60"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_16820\\2685933879.py:32: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  merge_metadata_keystroke(metadata_csv_path, keystroke_csv_path, output_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to demographics_csv/demo_keystroke.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_metadata_keystroke(metadata_csv, keystroke_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Merges the cleaned metadata CSV file with the keystroke data CSV file on PARTICIPANT_ID\n",
    "    and saves the merged data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - metadata_csv (str): The path to the cleaned metadata CSV file.\n",
    "    - keystroke_csv (str): The path to the keystroke data CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save merged data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned metadata CSV file into a DataFrame\n",
    "    metadata_df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    # Read the keystroke data CSV file into a DataFrame\n",
    "    keystroke_df = pd.read_csv(keystroke_csv)\n",
    "\n",
    "    # Merge the DataFrames on PARTICIPANT_ID\n",
    "    merged_df = pd.merge(metadata_df, keystroke_df, on='PARTICIPANT_ID')\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "metadata_csv_path = 'demographics_csv/cleaned_metadata_participants.csv'  # Path to the cleaned metadata CSV file\n",
    "keystroke_csv_path = 'demographics_csv/enhanced_keystroke_features.csv'  # Path to the keystroke data CSV file\n",
    "output_csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the output CSV file\n",
    "merge_metadata_keystroke(metadata_csv_path, keystroke_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:18:09.609548Z",
     "start_time": "2024-09-11T22:18:03.902561Z"
    }
   },
   "id": "10071c7428877708",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.0 Display distribution and equalising sample amount"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6dc9ba4fc0ff023"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:18:14.509591Z",
     "start_time": "2024-09-11T22:18:14.501592Z"
    }
   },
   "id": "3a42f4ce49b864d0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Distribution:\n",
      "female: 55.41%\n",
      "male: 44.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_16820\\2304086927.py:25: DtypeWarning: Columns (41) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  display_gender_distribution(csv_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_gender_distribution(csv_path):\n",
    "    \"\"\"\n",
    "    Displays the percentage distribution of male and female in the GENDER column of the given CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Step 2: Calculate distribution\n",
    "    gender_counts = df['GENDER'].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Step 3: Display distribution\n",
    "    print(\"Gender Distribution:\")\n",
    "    for gender, percentage in gender_counts.items():\n",
    "        print(f\"{gender}: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the CSV file\n",
    "display_gender_distribution(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:18:19.391591Z",
     "start_time": "2024-09-11T22:18:17.606590Z"
    }
   },
   "id": "44066f0b4ce381e8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_16820\\3658325818.py:35: DtypeWarning: Columns (41) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  balance_gender_samples(input_csv_path, output_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced data saved to demographics_csv/male_female_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def balance_gender_samples(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Balances the number of male and female samples in the given CSV file and saves the balanced data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save balanced data.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Step 2: Separate male and female samples\n",
    "    male_df = df[df['GENDER'] == 'male']\n",
    "    female_df = df[df['GENDER'] == 'female']\n",
    "\n",
    "    # Step 3: Determine the minimum sample size\n",
    "    min_sample_size = min(len(male_df), len(female_df))\n",
    "\n",
    "    # Step 4: Sample equal amounts\n",
    "    balanced_male_df = male_df.sample(n=min_sample_size, random_state=42)\n",
    "    balanced_female_df = female_df.sample(n=min_sample_size, random_state=42)\n",
    "\n",
    "    # Step 5: Concatenate and save\n",
    "    balanced_df = pd.concat([balanced_male_df, balanced_female_df])\n",
    "    balanced_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Balanced data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the input CSV file\n",
    "output_csv_path = 'demographics_csv/male_female_balanced.csv'  # Path to the output CSV file\n",
    "balance_gender_samples(input_csv_path, output_csv_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:18:27.288142Z",
     "start_time": "2024-09-11T22:18:22.203583Z"
    }
   },
   "id": "7e799924378649fc",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Distribution:\n",
      "male: 50.00%\n",
      "female: 50.00%\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_16820\\1961527451.py:2: DtypeWarning: Columns (41) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  print(display_gender_distribution(output_csv_path))\n"
     ]
    }
   ],
   "source": [
    "# print whether or not data is now balanced\n",
    "print(display_gender_distribution(output_csv_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:18:32.074556Z",
     "start_time": "2024-09-11T22:18:30.244555Z"
    }
   },
   "id": "c34d99aa5866ea33",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.0 KNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "346dca6b7aaaeac6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5587174020009841\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_feature_importance(csv_path):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    # Step 2: Preprocess data\n",
    "    df = df.dropna()\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    features = [\n",
    "        'AVG_WPM_15', 'AVG_IKI', 'ECPC', 'KSPC', 'ROR', 'D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'Z_SCORE'\n",
    "    ]\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate feature importance using permutation importance\n",
    "    result = permutation_importance(knn, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "\n",
    "    # Visualize feature importance\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the CSV file\n",
    "visualize_feature_importance(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-23T13:44:29.943113Z"
    }
   },
   "id": "38baf1861dd88278",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_feature_importance(csv_path):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    # Step 2: Preprocess data\n",
    "    df = df.dropna()\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    features = [\n",
    "        'AVG_WPM_15', 'AVG_IKI', 'ECPC', 'KSPC', 'ROR', 'D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'Z_SCORE'\n",
    "    ]\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate feature importance using permutation importance\n",
    "    result = permutation_importance(knn, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "\n",
    "    # Visualize feature importance\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Print accuracy\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the CSV file\n",
    "visualize_feature_importance(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a1f317f3bf5bdaaa",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.0 ANN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74bd3beec5895068"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Read the CSV file with dtype specification\n",
    "df = pd.read_csv('demographics_csv/male_female_balanced.csv', low_memory=False)\n",
    "\n",
    "# Step 2: Preprocess data\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode the 'GENDER' column\n",
    "label_encoder = LabelEncoder()\n",
    "df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "\n",
    "# Select specified features and target\n",
    "features = [\n",
    "    'D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'Z_SCORE', 'U1D2', 'SPEED_CLASS', 'HAND_HOLD_TIME', 'KEY_HOLD_TIME_STD',\n",
    "    'HAND'\n",
    "]\n",
    "X = df[features]\n",
    "y = df['GENDER']\n",
    "participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "# Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "unique_ids = participant_ids.unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "train_mask = participant_ids.isin(train_ids)\n",
    "test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target to categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Step 4: Build ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train ANN model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=10, validation_split=0.2)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_test_classes = y_test.argmax(axis=1)\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "39b2245d92591320",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8.0 SVM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c82d8b0e4b8b153c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_svm_model(csv_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = df.dropna()\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    features = ['D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'SPEED_CLASS', 'HAND_HOLD_TIME', 'KEY_HOLD_TIME_STD', 'HAND',\n",
    "                'Z_SCORE', 'U1D2', 'CONSECUTIVE_KEYS', 'CONSECUTIVE_KEYS_TIME']\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train SVM model\n",
    "    svm_model = SVC()\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate SVM model\n",
    "    y_pred_svm = svm_model.predict(X_test)\n",
    "    accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "    print(f'SVM Accuracy: {accuracy_svm:.2f}')\n",
    "\n",
    "    # Display feature importance using permutation importance\n",
    "    result = permutation_importance(svm_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('SVM Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_svm)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('SVM Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "csv_path = 'demographics_csv/male_female_balanced.csv'  # Replace with your actual CSV path\n",
    "train_svm_model(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "add3f727e4abcd63",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9.0 Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcf60f5cfcff97a"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_rf_model(csv_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = df.dropna()\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    features = ['D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'HAND_HOLD_TIME', 'KEY_HOLD_TIME_STD', 'HAND',\n",
    "                'Z_SCORE', 'U1D2', 'CONSECUTIVE_KEYS', 'CONSECUTIVE_KEYS_TIME', 'SPEED_CLASS', 'AVG_WPM_15', 'AVG_IKI', 'ECPC', 'KSPC', 'ROR']\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate Random Forest model\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    print(f'Random Forest Accuracy: {accuracy_rf:.2f}')\n",
    "\n",
    "    # Display feature importance using permutation importance\n",
    "    result = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_rf)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Random Forest Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "csv_path = 'demographics_csv/male_female_balanced.csv'  # Replace with your actual CSV path\n",
    "train_rf_model(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d3e03f4154641b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('demographics_csv/male_female_balanced.csv')\n",
    "\n",
    "# Preprocess data\n",
    "df = df.dropna()\n",
    "label_encoder = LabelEncoder()\n",
    "df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "\n",
    "# Convert all categorical columns to numeric\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category').cat.codes\n",
    "\n",
    "# Box Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='GENDER', y='SPEED_CLASS', data=df)\n",
    "plt.title('Box Plot of SPEED_CLASS by Gender')\n",
    "plt.show()\n",
    "\n",
    "# Violin Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='GENDER', y='SPEED_CLASS', data=df)\n",
    "plt.title('Violin Plot of SPEED_CLASS by Gender')\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "df[df['GENDER'] == 0]['SPEED_CLASS'].hist(alpha=0.5, color='blue', bins=30, label='Male')\n",
    "df[df['GENDER'] == 1]['SPEED_CLASS'].hist(alpha=0.5, color='red', bins=30, label='Female')\n",
    "plt.title('Histogram of SPEED_CLASS by Gender')\n",
    "plt.xlabel('SPEED_CLASS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Pair Plot\n",
    "sns.pairplot(df, hue='GENDER', vars=['SPEED_CLASS', 'CONSECUTIVE_KEYS', 'HAND'])\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Heatmap of Feature Correlations')\n",
    "plt.show()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df.drop(columns=['GENDER']))\n",
    "df['PCA1'] = pca_result[:, 0]\n",
    "df['PCA2'] = pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='GENDER', data=df, palette=['blue', 'red'])\n",
    "plt.title('PCA of Features by Gender')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e59c26133de30bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2d0884de13f1dd27",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

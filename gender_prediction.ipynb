{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.0 Read local folder, make subfolders\n",
    "### Read all txt files from local folder on pc and divide into folders containing 100 files each"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a003ca9073776d93"
  },
  {
   "cell_type": "code",
   "source": [
    "# from a local folder read 10k files and put them in project here\n",
    "# can adjust file amount \n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def organize_samples(source_dir, project_dir, total_files=10000, files_per_folder=100):\n",
    "    \"\"\"\n",
    "    Organizes .txt files from the source directory into multiple folders\n",
    "    in the 'samples' folder in the project directory, each containing a specified number of files.\n",
    "\n",
    "    Parameters:\n",
    "    - source_dir (str): The path to the directory containing the original .txt files.\n",
    "    - project_dir (str): The root path to your project directory (which contains the 'samples' folder).\n",
    "    - total_files (int): The total number of files to process (default is 10,000).\n",
    "    - files_per_folder (int): The number of files per folder (default is 100).\n",
    "    \"\"\"\n",
    "    # Path to the 'samples' folder in the project directory\n",
    "    samples_dir = os.path.join(project_dir, 'samples')\n",
    "\n",
    "    # Ensure the 'samples' directory exists in the project directory\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "\n",
    "    # Get a list of all .txt files in the source directory\n",
    "    all_files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n",
    "\n",
    "    # Sort the files (optional, depending on whether you want them in a specific order)\n",
    "    all_files.sort()\n",
    "\n",
    "    # Limit to the total number of files specified\n",
    "    files_to_copy = all_files[:total_files]\n",
    "\n",
    "    folder_number = 1\n",
    "    file_count = 0\n",
    "    total_copied = 0\n",
    "\n",
    "    # Create the first folder (samples_01)\n",
    "    current_folder_name = f'samples_{folder_number:02d}'\n",
    "    current_folder_path = os.path.join(samples_dir, current_folder_name)\n",
    "    os.makedirs(current_folder_path, exist_ok=True)\n",
    "\n",
    "    for file_name in files_to_copy:\n",
    "        source_file = os.path.join(source_dir, file_name)\n",
    "        target_file = os.path.join(current_folder_path, file_name)\n",
    "\n",
    "        # Copy the file\n",
    "        shutil.copy2(source_file, target_file)\n",
    "        file_count += 1\n",
    "        total_copied += 1\n",
    "\n",
    "        # Check if the current folder has reached the desired number of files\n",
    "        if file_count >= files_per_folder:\n",
    "            folder_number += 1\n",
    "            if total_copied >= total_files:\n",
    "                break  # Stop if we've copied the total desired number of files\n",
    "            # Reset file count and create a new folder\n",
    "            file_count = 0\n",
    "            current_folder_name = f'samples_{folder_number:02d}'\n",
    "            current_folder_path = os.path.join(samples_dir, current_folder_name)\n",
    "            os.makedirs(current_folder_path, exist_ok=True)\n",
    "\n",
    "    print(f'Total files copied: {total_copied}')\n",
    "    print(f'Files organized into {folder_number} folders.')\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "source_directory = r\"C:\\Users\\laras\\Downloads\\Keystrokes\\Keystrokes\\files\"\n",
    "project_directory = 'samples'  # Replace with the root path to your PyCharm project\n",
    "\n",
    "organize_samples(source_directory, project_directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:53:35.820596Z",
     "start_time": "2024-09-10T13:51:28.770410Z"
    }
   },
   "id": "c17144531c9b37a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files copied: 10000\n",
      "Files organized into 101 folders.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.0 Read txt files, make csv file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb4f0ef7f4cff0c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:15:59.765666Z",
     "start_time": "2024-09-11T22:15:49.923026Z"
    }
   },
   "source": [
    "# generate a csv file with all the data from the files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def try_multiple_delimiters(file_path):\n",
    "    \"\"\"\n",
    "    Attempts to read a file using different delimiters.\n",
    "    \"\"\"\n",
    "    delimiters = ['\\t', ',', ';']  # Common delimiters to try\n",
    "    for delimiter in delimiters:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=delimiter, encoding='utf-8', on_bad_lines='skip',\n",
    "                             dtype={'PARTICIPANT_ID': str})\n",
    "            # If we get more than one column, we assume we have the right delimiter\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            pass  # Try the next delimiter\n",
    "    raise ValueError(\"Could not determine delimiter\")\n",
    "\n",
    "\n",
    "def read_keystroke_data(samples_dir, output_csv='all_samples_combined1.csv'):\n",
    "    \"\"\"\n",
    "    Reads keystroke data from multiple subfolders, handles errors, and saves valid data to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - samples_dir (str): The directory where the 'samples' folder resides, which contains multiple subfolders with .txt files.\n",
    "    - output_csv (str): The output path for the CSV file to save valid data.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing valid keystroke data.\n",
    "    \"\"\"\n",
    "    all_data = []  # To hold data from all valid files\n",
    "    total_files_processed = 0  # Counter to keep track of the total number of files processed\n",
    "    total_rows_processed = 0  # Counter for the total number of rows across all files\n",
    "    skipped_files = 0  # Counter for the number of skipped files\n",
    "    malformed_rows = 0  # Counter for the number of malformed rows\n",
    "\n",
    "    # Expected columns in the file\n",
    "    required_columns = ['PARTICIPANT_ID', 'TEST_SECTION_ID', 'SENTENCE', 'USER_INPUT',\n",
    "                        'KEYSTROKE_ID', 'PRESS_TIME', 'RELEASE_TIME', 'LETTER', 'KEYCODE']\n",
    "\n",
    "    # Traverse each subfolder (e.g., samples_01, samples_02, ..., samples_100)\n",
    "    for subdir in os.listdir(samples_dir):\n",
    "        subdir_path = os.path.join(samples_dir, subdir)\n",
    "\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Traverse each .txt file in the subfolder\n",
    "            for file_name in os.listdir(subdir_path):\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = os.path.join(subdir_path, file_name)\n",
    "                    total_files_processed += 1  # Increment the counter for each file\n",
    "\n",
    "                    print(f\"Processing file: {file_path}\")  # Debugging statement\n",
    "\n",
    "                    try:\n",
    "                        # Try reading the file using multiple delimiters\n",
    "                        df = try_multiple_delimiters(file_path)\n",
    "\n",
    "                        # Log the number of rows in the current file\n",
    "                        print(f\"File {file_name} has {df.shape[0]} rows\")\n",
    "\n",
    "                        # Ensure required columns exist\n",
    "                        if not all(col in df.columns for col in required_columns):\n",
    "                            print(f\"Missing columns in {file_path}\")\n",
    "                            skipped_files += 1\n",
    "                            continue  # Skip this file if it doesn't have the required columns\n",
    "\n",
    "                        # If the number of columns is 1, it means that the file might be incorrectly formatted\n",
    "                        if len(df.columns) == 1:\n",
    "                            print(f\"Malformed data in {file_path}, skipping.\")\n",
    "                            skipped_files += 1\n",
    "                            continue  # Skip files with misformatted rows\n",
    "\n",
    "                        # Filter out rows where all values are under one column\n",
    "                        malformed_df = df[df.apply(lambda row: row.count() == 1, axis=1)]\n",
    "                        malformed_rows += len(malformed_df)\n",
    "                        df = df[df.apply(lambda row: row.count() > 1, axis=1)]\n",
    "\n",
    "                        # Append the number of rows to the total rows processed\n",
    "                        total_rows_processed += df.shape[0]\n",
    "\n",
    "                        # Append the valid DataFrame to the list\n",
    "                        all_data.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")\n",
    "                        skipped_files += 1\n",
    "                        continue  # Skip files with errors like encoding issues or missing data\n",
    "\n",
    "    # Concatenate all valid data into one DataFrame\n",
    "    if not all_data:\n",
    "        print(\"No valid data found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no valid data was found\n",
    "\n",
    "    full_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Verify the number of rows before saving to CSV\n",
    "    print(f\"Total rows in concatenated DataFrame: {full_data.shape[0]}\")\n",
    "    print(f\"Total malformed rows removed: {malformed_rows}\")\n",
    "\n",
    "    # Save the data to CSV\n",
    "    full_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Valid data saved to {output_csv}\")\n",
    "\n",
    "    # Print the total number of files processed and total rows processed\n",
    "    print(f\"Total number of .txt files processed: {total_files_processed}\")\n",
    "    print(f\"Total number of rows processed: {total_rows_processed}\")\n",
    "    print(f\"Total number of skipped files: {skipped_files}\")\n",
    "\n",
    "    return full_data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "samples_directory = \"samples\"  # Replace with your actual path\n",
    "read_keystroke_data(samples_directory, output_csv='demographics_csv/uncleaned_all.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: samples\\samples_01\\100001_keystrokes.txt\n",
      "File 100001_keystrokes.txt has 658 rows\n",
      "Processing file: samples\\samples_01\\100003_keystrokes.txt\n",
      "File 100003_keystrokes.txt has 806 rows\n",
      "Processing file: samples\\samples_01\\100007_keystrokes.txt\n",
      "File 100007_keystrokes.txt has 801 rows\n",
      "Processing file: samples\\samples_01\\100008_keystrokes.txt\n",
      "File 100008_keystrokes.txt has 687 rows\n",
      "Processing file: samples\\samples_01\\100013_keystrokes.txt\n",
      "File 100013_keystrokes.txt has 744 rows\n",
      "Processing file: samples\\samples_01\\100016_keystrokes.txt\n",
      "File 100016_keystrokes.txt has 776 rows\n",
      "Processing file: samples\\samples_01\\10001_keystrokes.txt\n",
      "File 10001_keystrokes.txt has 763 rows\n",
      "Processing file: samples\\samples_01\\100020_keystrokes.txt\n",
      "File 100020_keystrokes.txt has 649 rows\n",
      "Processing file: samples\\samples_01\\100030_keystrokes.txt\n",
      "File 100030_keystrokes.txt has 654 rows\n",
      "Processing file: samples\\samples_01\\100031_keystrokes.txt\n",
      "File 100031_keystrokes.txt has 545 rows\n",
      "Processing file: samples\\samples_01\\100032_keystrokes.txt\n",
      "File 100032_keystrokes.txt has 924 rows\n",
      "Processing file: samples\\samples_01\\100033_keystrokes.txt\n",
      "File 100033_keystrokes.txt has 829 rows\n",
      "Processing file: samples\\samples_01\\100045_keystrokes.txt\n",
      "File 100045_keystrokes.txt has 738 rows\n",
      "Processing file: samples\\samples_01\\10004_keystrokes.txt\n",
      "File 10004_keystrokes.txt has 769 rows\n",
      "Processing file: samples\\samples_01\\100050_keystrokes.txt\n",
      "File 100050_keystrokes.txt has 756 rows\n",
      "Processing file: samples\\samples_01\\100056_keystrokes.txt\n",
      "File 100056_keystrokes.txt has 760 rows\n",
      "Processing file: samples\\samples_01\\100059_keystrokes.txt\n",
      "File 100059_keystrokes.txt has 968 rows\n",
      "Processing file: samples\\samples_01\\100060_keystrokes.txt\n",
      "File 100060_keystrokes.txt has 795 rows\n",
      "Processing file: samples\\samples_01\\100068_keystrokes.txt\n",
      "File 100068_keystrokes.txt has 721 rows\n",
      "Processing file: samples\\samples_01\\100072_keystrokes.txt\n",
      "File 100072_keystrokes.txt has 752 rows\n",
      "Processing file: samples\\samples_01\\100076_keystrokes.txt\n",
      "File 100076_keystrokes.txt has 610 rows\n",
      "Processing file: samples\\samples_01\\100077_keystrokes.txt\n",
      "File 100077_keystrokes.txt has 737 rows\n",
      "Processing file: samples\\samples_01\\100081_keystrokes.txt\n",
      "File 100081_keystrokes.txt has 847 rows\n",
      "Processing file: samples\\samples_01\\100084_keystrokes.txt\n",
      "File 100084_keystrokes.txt has 892 rows\n",
      "Processing file: samples\\samples_01\\100085_keystrokes.txt\n",
      "File 100085_keystrokes.txt has 723 rows\n",
      "Processing file: samples\\samples_01\\100088_keystrokes.txt\n",
      "File 100088_keystrokes.txt has 782 rows\n",
      "Processing file: samples\\samples_01\\100091_keystrokes.txt\n",
      "File 100091_keystrokes.txt has 648 rows\n",
      "Processing file: samples\\samples_01\\100094_keystrokes.txt\n",
      "File 100094_keystrokes.txt has 725 rows\n",
      "Processing file: samples\\samples_01\\100097_keystrokes.txt\n",
      "File 100097_keystrokes.txt has 687 rows\n",
      "Processing file: samples\\samples_01\\10009_keystrokes.txt\n",
      "File 10009_keystrokes.txt has 862 rows\n",
      "Processing file: samples\\samples_01\\100102_keystrokes.txt\n",
      "File 100102_keystrokes.txt has 743 rows\n",
      "Processing file: samples\\samples_01\\100104_keystrokes.txt\n",
      "File 100104_keystrokes.txt has 618 rows\n",
      "Processing file: samples\\samples_01\\100106_keystrokes.txt\n",
      "File 100106_keystrokes.txt has 703 rows\n",
      "Processing file: samples\\samples_01\\100108_keystrokes.txt\n",
      "File 100108_keystrokes.txt has 695 rows\n",
      "Processing file: samples\\samples_01\\100116_keystrokes.txt\n",
      "File 100116_keystrokes.txt has 644 rows\n",
      "Processing file: samples\\samples_01\\100120_keystrokes.txt\n",
      "File 100120_keystrokes.txt has 669 rows\n",
      "Processing file: samples\\samples_01\\100122_keystrokes.txt\n",
      "Error processing file samples\\samples_01\\100122_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_01\\100123_keystrokes.txt\n",
      "File 100123_keystrokes.txt has 726 rows\n",
      "Processing file: samples\\samples_01\\100124_keystrokes.txt\n",
      "File 100124_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_01\\100125_keystrokes.txt\n",
      "File 100125_keystrokes.txt has 765 rows\n",
      "Processing file: samples\\samples_01\\100132_keystrokes.txt\n",
      "File 100132_keystrokes.txt has 783 rows\n",
      "Processing file: samples\\samples_01\\100134_keystrokes.txt\n",
      "File 100134_keystrokes.txt has 1143 rows\n",
      "Processing file: samples\\samples_01\\100135_keystrokes.txt\n",
      "File 100135_keystrokes.txt has 975 rows\n",
      "Processing file: samples\\samples_01\\100136_keystrokes.txt\n",
      "File 100136_keystrokes.txt has 582 rows\n",
      "Processing file: samples\\samples_01\\100138_keystrokes.txt\n",
      "File 100138_keystrokes.txt has 701 rows\n",
      "Processing file: samples\\samples_01\\100145_keystrokes.txt\n",
      "File 100145_keystrokes.txt has 788 rows\n",
      "Processing file: samples\\samples_01\\100151_keystrokes.txt\n",
      "File 100151_keystrokes.txt has 618 rows\n",
      "Processing file: samples\\samples_01\\100156_keystrokes.txt\n",
      "File 100156_keystrokes.txt has 644 rows\n",
      "Processing file: samples\\samples_01\\100157_keystrokes.txt\n",
      "File 100157_keystrokes.txt has 767 rows\n",
      "Processing file: samples\\samples_01\\100159_keystrokes.txt\n",
      "File 100159_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_01\\100162_keystrokes.txt\n",
      "File 100162_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_01\\100163_keystrokes.txt\n",
      "File 100163_keystrokes.txt has 816 rows\n",
      "Processing file: samples\\samples_01\\100165_keystrokes.txt\n",
      "File 100165_keystrokes.txt has 648 rows\n",
      "Processing file: samples\\samples_01\\100166_keystrokes.txt\n",
      "File 100166_keystrokes.txt has 737 rows\n",
      "Processing file: samples\\samples_01\\100167_keystrokes.txt\n",
      "File 100167_keystrokes.txt has 747 rows\n",
      "Processing file: samples\\samples_01\\100168_keystrokes.txt\n",
      "File 100168_keystrokes.txt has 880 rows\n",
      "Processing file: samples\\samples_01\\100169_keystrokes.txt\n",
      "File 100169_keystrokes.txt has 723 rows\n",
      "Processing file: samples\\samples_01\\100180_keystrokes.txt\n",
      "File 100180_keystrokes.txt has 636 rows\n",
      "Processing file: samples\\samples_01\\100182_keystrokes.txt\n",
      "File 100182_keystrokes.txt has 825 rows\n",
      "Processing file: samples\\samples_01\\100183_keystrokes.txt\n",
      "Error processing file samples\\samples_01\\100183_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_01\\100184_keystrokes.txt\n",
      "File 100184_keystrokes.txt has 693 rows\n",
      "Processing file: samples\\samples_01\\100185_keystrokes.txt\n",
      "File 100185_keystrokes.txt has 735 rows\n",
      "Processing file: samples\\samples_01\\100186_keystrokes.txt\n",
      "File 100186_keystrokes.txt has 596 rows\n",
      "Processing file: samples\\samples_01\\100188_keystrokes.txt\n",
      "File 100188_keystrokes.txt has 605 rows\n",
      "Processing file: samples\\samples_01\\100190_keystrokes.txt\n",
      "File 100190_keystrokes.txt has 734 rows\n",
      "Processing file: samples\\samples_01\\100191_keystrokes.txt\n",
      "File 100191_keystrokes.txt has 809 rows\n",
      "Processing file: samples\\samples_01\\100193_keystrokes.txt\n",
      "File 100193_keystrokes.txt has 728 rows\n",
      "Processing file: samples\\samples_01\\100197_keystrokes.txt\n",
      "File 100197_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_01\\10019_keystrokes.txt\n",
      "File 10019_keystrokes.txt has 615 rows\n",
      "Processing file: samples\\samples_01\\100206_keystrokes.txt\n",
      "File 100206_keystrokes.txt has 776 rows\n",
      "Processing file: samples\\samples_01\\100210_keystrokes.txt\n",
      "File 100210_keystrokes.txt has 620 rows\n",
      "Processing file: samples\\samples_01\\100211_keystrokes.txt\n",
      "File 100211_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_01\\100215_keystrokes.txt\n",
      "File 100215_keystrokes.txt has 686 rows\n",
      "Processing file: samples\\samples_01\\100224_keystrokes.txt\n",
      "File 100224_keystrokes.txt has 630 rows\n",
      "Processing file: samples\\samples_01\\100227_keystrokes.txt\n",
      "File 100227_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_01\\100230_keystrokes.txt\n",
      "File 100230_keystrokes.txt has 580 rows\n",
      "Processing file: samples\\samples_01\\100232_keystrokes.txt\n",
      "File 100232_keystrokes.txt has 588 rows\n",
      "Processing file: samples\\samples_01\\10023_keystrokes.txt\n",
      "File 10023_keystrokes.txt has 710 rows\n",
      "Processing file: samples\\samples_01\\100249_keystrokes.txt\n",
      "File 100249_keystrokes.txt has 784 rows\n",
      "Processing file: samples\\samples_01\\10024_keystrokes.txt\n",
      "File 10024_keystrokes.txt has 599 rows\n",
      "Processing file: samples\\samples_01\\100252_keystrokes.txt\n",
      "File 100252_keystrokes.txt has 827 rows\n",
      "Processing file: samples\\samples_01\\100253_keystrokes.txt\n",
      "File 100253_keystrokes.txt has 731 rows\n",
      "Processing file: samples\\samples_01\\100256_keystrokes.txt\n",
      "File 100256_keystrokes.txt has 672 rows\n",
      "Processing file: samples\\samples_01\\100257_keystrokes.txt\n",
      "File 100257_keystrokes.txt has 685 rows\n",
      "Processing file: samples\\samples_01\\100262_keystrokes.txt\n",
      "File 100262_keystrokes.txt has 875 rows\n",
      "Processing file: samples\\samples_01\\100264_keystrokes.txt\n",
      "File 100264_keystrokes.txt has 720 rows\n",
      "Processing file: samples\\samples_01\\100269_keystrokes.txt\n",
      "File 100269_keystrokes.txt has 831 rows\n",
      "Processing file: samples\\samples_01\\10026_keystrokes.txt\n",
      "File 10026_keystrokes.txt has 713 rows\n",
      "Processing file: samples\\samples_01\\100273_keystrokes.txt\n",
      "File 100273_keystrokes.txt has 794 rows\n",
      "Processing file: samples\\samples_01\\100275_keystrokes.txt\n",
      "File 100275_keystrokes.txt has 724 rows\n",
      "Processing file: samples\\samples_01\\100278_keystrokes.txt\n",
      "File 100278_keystrokes.txt has 766 rows\n",
      "Processing file: samples\\samples_01\\10027_keystrokes.txt\n",
      "File 10027_keystrokes.txt has 597 rows\n",
      "Processing file: samples\\samples_01\\100280_keystrokes.txt\n",
      "File 100280_keystrokes.txt has 750 rows\n",
      "Processing file: samples\\samples_01\\100281_keystrokes.txt\n",
      "File 100281_keystrokes.txt has 684 rows\n",
      "Processing file: samples\\samples_01\\100282_keystrokes.txt\n",
      "File 100282_keystrokes.txt has 730 rows\n",
      "Processing file: samples\\samples_01\\100283_keystrokes.txt\n",
      "File 100283_keystrokes.txt has 709 rows\n",
      "Processing file: samples\\samples_01\\100289_keystrokes.txt\n",
      "File 100289_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_01\\100294_keystrokes.txt\n",
      "File 100294_keystrokes.txt has 786 rows\n",
      "Processing file: samples\\samples_01\\100298_keystrokes.txt\n",
      "File 100298_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_01\\1002_keystrokes.txt\n",
      "File 1002_keystrokes.txt has 755 rows\n",
      "Processing file: samples\\samples_02\\100302_keystrokes.txt\n",
      "File 100302_keystrokes.txt has 680 rows\n",
      "Processing file: samples\\samples_02\\100306_keystrokes.txt\n",
      "File 100306_keystrokes.txt has 717 rows\n",
      "Processing file: samples\\samples_02\\100311_keystrokes.txt\n",
      "File 100311_keystrokes.txt has 738 rows\n",
      "Processing file: samples\\samples_02\\100320_keystrokes.txt\n",
      "File 100320_keystrokes.txt has 706 rows\n",
      "Processing file: samples\\samples_02\\100327_keystrokes.txt\n",
      "File 100327_keystrokes.txt has 671 rows\n",
      "Processing file: samples\\samples_02\\100329_keystrokes.txt\n",
      "File 100329_keystrokes.txt has 730 rows\n",
      "Processing file: samples\\samples_02\\100330_keystrokes.txt\n",
      "File 100330_keystrokes.txt has 697 rows\n",
      "Processing file: samples\\samples_02\\100332_keystrokes.txt\n",
      "File 100332_keystrokes.txt has 694 rows\n",
      "Processing file: samples\\samples_02\\100333_keystrokes.txt\n",
      "File 100333_keystrokes.txt has 678 rows\n",
      "Processing file: samples\\samples_02\\100334_keystrokes.txt\n",
      "File 100334_keystrokes.txt has 685 rows\n",
      "Processing file: samples\\samples_02\\100338_keystrokes.txt\n",
      "File 100338_keystrokes.txt has 736 rows\n",
      "Processing file: samples\\samples_02\\100339_keystrokes.txt\n",
      "File 100339_keystrokes.txt has 854 rows\n",
      "Processing file: samples\\samples_02\\100341_keystrokes.txt\n",
      "File 100341_keystrokes.txt has 577 rows\n",
      "Processing file: samples\\samples_02\\100348_keystrokes.txt\n",
      "File 100348_keystrokes.txt has 613 rows\n",
      "Processing file: samples\\samples_02\\10034_keystrokes.txt\n",
      "File 10034_keystrokes.txt has 824 rows\n",
      "Processing file: samples\\samples_02\\100350_keystrokes.txt\n",
      "File 100350_keystrokes.txt has 626 rows\n",
      "Processing file: samples\\samples_02\\100353_keystrokes.txt\n",
      "File 100353_keystrokes.txt has 652 rows\n",
      "Processing file: samples\\samples_02\\100356_keystrokes.txt\n",
      "File 100356_keystrokes.txt has 759 rows\n",
      "Processing file: samples\\samples_02\\100365_keystrokes.txt\n",
      "File 100365_keystrokes.txt has 677 rows\n",
      "Processing file: samples\\samples_02\\100367_keystrokes.txt\n",
      "File 100367_keystrokes.txt has 695 rows\n",
      "Processing file: samples\\samples_02\\10036_keystrokes.txt\n",
      "File 10036_keystrokes.txt has 797 rows\n",
      "Processing file: samples\\samples_02\\100375_keystrokes.txt\n",
      "File 100375_keystrokes.txt has 681 rows\n",
      "Processing file: samples\\samples_02\\100379_keystrokes.txt\n",
      "File 100379_keystrokes.txt has 765 rows\n",
      "Processing file: samples\\samples_02\\100380_keystrokes.txt\n",
      "File 100380_keystrokes.txt has 748 rows\n",
      "Processing file: samples\\samples_02\\100381_keystrokes.txt\n",
      "File 100381_keystrokes.txt has 672 rows\n",
      "Processing file: samples\\samples_02\\100382_keystrokes.txt\n",
      "File 100382_keystrokes.txt has 621 rows\n",
      "Processing file: samples\\samples_02\\100383_keystrokes.txt\n",
      "File 100383_keystrokes.txt has 707 rows\n",
      "Processing file: samples\\samples_02\\100386_keystrokes.txt\n",
      "File 100386_keystrokes.txt has 742 rows\n",
      "Processing file: samples\\samples_02\\100390_keystrokes.txt\n",
      "File 100390_keystrokes.txt has 686 rows\n",
      "Processing file: samples\\samples_02\\100395_keystrokes.txt\n",
      "File 100395_keystrokes.txt has 619 rows\n",
      "Processing file: samples\\samples_02\\100396_keystrokes.txt\n",
      "File 100396_keystrokes.txt has 855 rows\n",
      "Processing file: samples\\samples_02\\100397_keystrokes.txt\n",
      "File 100397_keystrokes.txt has 626 rows\n",
      "Processing file: samples\\samples_02\\100410_keystrokes.txt\n",
      "File 100410_keystrokes.txt has 798 rows\n",
      "Processing file: samples\\samples_02\\100416_keystrokes.txt\n",
      "File 100416_keystrokes.txt has 672 rows\n",
      "Processing file: samples\\samples_02\\100417_keystrokes.txt\n",
      "File 100417_keystrokes.txt has 678 rows\n",
      "Processing file: samples\\samples_02\\100419_keystrokes.txt\n",
      "File 100419_keystrokes.txt has 639 rows\n",
      "Processing file: samples\\samples_02\\100420_keystrokes.txt\n",
      "File 100420_keystrokes.txt has 748 rows\n",
      "Processing file: samples\\samples_02\\100421_keystrokes.txt\n",
      "File 100421_keystrokes.txt has 745 rows\n",
      "Processing file: samples\\samples_02\\100422_keystrokes.txt\n",
      "File 100422_keystrokes.txt has 828 rows\n",
      "Processing file: samples\\samples_02\\100423_keystrokes.txt\n",
      "File 100423_keystrokes.txt has 878 rows\n",
      "Processing file: samples\\samples_02\\100425_keystrokes.txt\n",
      "File 100425_keystrokes.txt has 753 rows\n",
      "Processing file: samples\\samples_02\\100426_keystrokes.txt\n",
      "File 100426_keystrokes.txt has 773 rows\n",
      "Processing file: samples\\samples_02\\100431_keystrokes.txt\n",
      "File 100431_keystrokes.txt has 655 rows\n",
      "Processing file: samples\\samples_02\\100432_keystrokes.txt\n",
      "File 100432_keystrokes.txt has 815 rows\n",
      "Processing file: samples\\samples_02\\100434_keystrokes.txt\n",
      "File 100434_keystrokes.txt has 638 rows\n",
      "Processing file: samples\\samples_02\\100438_keystrokes.txt\n",
      "File 100438_keystrokes.txt has 662 rows\n",
      "Processing file: samples\\samples_02\\100439_keystrokes.txt\n",
      "File 100439_keystrokes.txt has 733 rows\n",
      "Processing file: samples\\samples_02\\100444_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100444_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100445_keystrokes.txt\n",
      "File 100445_keystrokes.txt has 635 rows\n",
      "Processing file: samples\\samples_02\\100446_keystrokes.txt\n",
      "File 100446_keystrokes.txt has 839 rows\n",
      "Processing file: samples\\samples_02\\100447_keystrokes.txt\n",
      "File 100447_keystrokes.txt has 641 rows\n",
      "Processing file: samples\\samples_02\\100450_keystrokes.txt\n",
      "File 100450_keystrokes.txt has 734 rows\n",
      "Processing file: samples\\samples_02\\100452_keystrokes.txt\n",
      "File 100452_keystrokes.txt has 683 rows\n",
      "Processing file: samples\\samples_02\\100456_keystrokes.txt\n",
      "File 100456_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_02\\100458_keystrokes.txt\n",
      "File 100458_keystrokes.txt has 868 rows\n",
      "Processing file: samples\\samples_02\\100460_keystrokes.txt\n",
      "File 100460_keystrokes.txt has 690 rows\n",
      "Processing file: samples\\samples_02\\100461_keystrokes.txt\n",
      "File 100461_keystrokes.txt has 634 rows\n",
      "Processing file: samples\\samples_02\\100462_keystrokes.txt\n",
      "File 100462_keystrokes.txt has 693 rows\n",
      "Processing file: samples\\samples_02\\100463_keystrokes.txt\n",
      "File 100463_keystrokes.txt has 789 rows\n",
      "Processing file: samples\\samples_02\\100466_keystrokes.txt\n",
      "File 100466_keystrokes.txt has 768 rows\n",
      "Processing file: samples\\samples_02\\100467_keystrokes.txt\n",
      "File 100467_keystrokes.txt has 638 rows\n",
      "Processing file: samples\\samples_02\\100470_keystrokes.txt\n",
      "File 100470_keystrokes.txt has 704 rows\n",
      "Processing file: samples\\samples_02\\100471_keystrokes.txt\n",
      "File 100471_keystrokes.txt has 735 rows\n",
      "Processing file: samples\\samples_02\\100473_keystrokes.txt\n",
      "File 100473_keystrokes.txt has 718 rows\n",
      "Processing file: samples\\samples_02\\100474_keystrokes.txt\n",
      "File 100474_keystrokes.txt has 684 rows\n",
      "Processing file: samples\\samples_02\\100475_keystrokes.txt\n",
      "File 100475_keystrokes.txt has 810 rows\n",
      "Processing file: samples\\samples_02\\100476_keystrokes.txt\n",
      "File 100476_keystrokes.txt has 874 rows\n",
      "Processing file: samples\\samples_02\\100477_keystrokes.txt\n",
      "File 100477_keystrokes.txt has 676 rows\n",
      "Processing file: samples\\samples_02\\100480_keystrokes.txt\n",
      "File 100480_keystrokes.txt has 659 rows\n",
      "Processing file: samples\\samples_02\\100485_keystrokes.txt\n",
      "File 100485_keystrokes.txt has 802 rows\n",
      "Processing file: samples\\samples_02\\100487_keystrokes.txt\n",
      "File 100487_keystrokes.txt has 715 rows\n",
      "Processing file: samples\\samples_02\\100488_keystrokes.txt\n",
      "File 100488_keystrokes.txt has 828 rows\n",
      "Processing file: samples\\samples_02\\100491_keystrokes.txt\n",
      "File 100491_keystrokes.txt has 731 rows\n",
      "Processing file: samples\\samples_02\\100492_keystrokes.txt\n",
      "File 100492_keystrokes.txt has 676 rows\n",
      "Processing file: samples\\samples_02\\100495_keystrokes.txt\n",
      "File 100495_keystrokes.txt has 751 rows\n",
      "Processing file: samples\\samples_02\\100496_keystrokes.txt\n",
      "File 100496_keystrokes.txt has 708 rows\n",
      "Processing file: samples\\samples_02\\100498_keystrokes.txt\n",
      "File 100498_keystrokes.txt has 745 rows\n",
      "Processing file: samples\\samples_02\\100499_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100499_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100500_keystrokes.txt\n",
      "File 100500_keystrokes.txt has 681 rows\n",
      "Processing file: samples\\samples_02\\100501_keystrokes.txt\n",
      "File 100501_keystrokes.txt has 769 rows\n",
      "Processing file: samples\\samples_02\\100502_keystrokes.txt\n",
      "File 100502_keystrokes.txt has 825 rows\n",
      "Processing file: samples\\samples_02\\100503_keystrokes.txt\n",
      "File 100503_keystrokes.txt has 770 rows\n",
      "Processing file: samples\\samples_02\\100504_keystrokes.txt\n",
      "File 100504_keystrokes.txt has 781 rows\n",
      "Processing file: samples\\samples_02\\100510_keystrokes.txt\n",
      "File 100510_keystrokes.txt has 841 rows\n",
      "Processing file: samples\\samples_02\\100512_keystrokes.txt\n",
      "File 100512_keystrokes.txt has 621 rows\n",
      "Processing file: samples\\samples_02\\100514_keystrokes.txt\n",
      "File 100514_keystrokes.txt has 678 rows\n",
      "Processing file: samples\\samples_02\\100517_keystrokes.txt\n",
      "File 100517_keystrokes.txt has 825 rows\n",
      "Processing file: samples\\samples_02\\100519_keystrokes.txt\n",
      "File 100519_keystrokes.txt has 732 rows\n",
      "Processing file: samples\\samples_02\\10051_keystrokes.txt\n",
      "File 10051_keystrokes.txt has 621 rows\n",
      "Processing file: samples\\samples_02\\100521_keystrokes.txt\n",
      "File 100521_keystrokes.txt has 749 rows\n",
      "Processing file: samples\\samples_02\\100522_keystrokes.txt\n",
      "File 100522_keystrokes.txt has 654 rows\n",
      "Processing file: samples\\samples_02\\100524_keystrokes.txt\n",
      "File 100524_keystrokes.txt has 637 rows\n",
      "Processing file: samples\\samples_02\\100527_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100527_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100528_keystrokes.txt\n",
      "File 100528_keystrokes.txt has 707 rows\n",
      "Processing file: samples\\samples_02\\100532_keystrokes.txt\n",
      "File 100532_keystrokes.txt has 674 rows\n",
      "Processing file: samples\\samples_02\\10053_keystrokes.txt\n",
      "File 10053_keystrokes.txt has 591 rows\n",
      "Processing file: samples\\samples_02\\100542_keystrokes.txt\n",
      "File 100542_keystrokes.txt has 659 rows\n",
      "Processing file: samples\\samples_02\\100545_keystrokes.txt\n",
      "File 100545_keystrokes.txt has 697 rows\n",
      "Processing file: samples\\samples_02\\100549_keystrokes.txt\n",
      "Error processing file samples\\samples_02\\100549_keystrokes.txt: Could not determine delimiter\n",
      "Processing file: samples\\samples_02\\100550_keystrokes.txt\n",
      "File 100550_keystrokes.txt has 789 rows\n",
      "Total rows in concatenated DataFrame: 140441\n",
      "Total malformed rows removed: 0\n",
      "Valid data saved to demographics_csv/uncleaned_all.csv\n",
      "Total number of .txt files processed: 200\n",
      "Total number of rows processed: 140441\n",
      "Total number of skipped files: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "       PARTICIPANT_ID  TEST_SECTION_ID  \\\n0              100001          1090979   \n1              100001          1090979   \n2              100001          1090979   \n3              100001          1090979   \n4              100001          1090979   \n...               ...              ...   \n140436         100550          1097261   \n140437         100550          1097261   \n140438         100550          1097261   \n140439         100550          1097261   \n140440         100550          1097261   \n\n                                                 SENTENCE  \\\n0             Was wondering if you and Natalie connected?   \n1             Was wondering if you and Natalie connected?   \n2             Was wondering if you and Natalie connected?   \n3             Was wondering if you and Natalie connected?   \n4             Was wondering if you and Natalie connected?   \n...                                                   ...   \n140436  You must therefore take full responsibility fo...   \n140437  You must therefore take full responsibility fo...   \n140438  You must therefore take full responsibility fo...   \n140439  You must therefore take full responsibility fo...   \n140440  You must therefore take full responsibility fo...   \n\n                                               USER_INPUT  KEYSTROKE_ID  \\\n0             Was wondering if you and Natalie connected?    51891207.0   \n1             Was wondering if you and Natalie connected?    51891214.0   \n2             Was wondering if you and Natalie connected?    51891219.0   \n3             Was wondering if you and Natalie connected?    51891226.0   \n4             Was wondering if you and Natalie connected?    51891231.0   \n...                                                   ...           ...   \n140436  You must therefore take full responsibility fo...    52190490.0   \n140437  You must therefore take full responsibility fo...    52190496.0   \n140438  You must therefore take full responsibility fo...    52190503.0   \n140439  You must therefore take full responsibility fo...    52190720.0   \n140440  You must therefore take full responsibility fo...    52190726.0   \n\n          PRESS_TIME   RELEASE_TIME LETTER  KEYCODE  \n0       1.473275e+12  1473275372663  SHIFT       16  \n1       1.473275e+12  1473275372703      W       87  \n2       1.473275e+12  1473275372903      a       65  \n3       1.473275e+12  1473275372975      s       83  \n4       1.473275e+12  1473275373079              32  \n...              ...            ...    ...      ...  \n140436  1.473280e+12  1473280428505      i       73  \n140437  1.473280e+12  1473280428527      n       78  \n140438  1.473280e+12  1473280428607      g       71  \n140439  1.473280e+12  1473280428786      .      190  \n140440  1.473280e+12  1473280428837              32  \n\n[140441 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PARTICIPANT_ID</th>\n      <th>TEST_SECTION_ID</th>\n      <th>SENTENCE</th>\n      <th>USER_INPUT</th>\n      <th>KEYSTROKE_ID</th>\n      <th>PRESS_TIME</th>\n      <th>RELEASE_TIME</th>\n      <th>LETTER</th>\n      <th>KEYCODE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891207.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372663</td>\n      <td>SHIFT</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891214.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372703</td>\n      <td>W</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891219.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372903</td>\n      <td>a</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891226.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275372975</td>\n      <td>s</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100001</td>\n      <td>1090979</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>Was wondering if you and Natalie connected?</td>\n      <td>51891231.0</td>\n      <td>1.473275e+12</td>\n      <td>1473275373079</td>\n      <td></td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140436</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190490.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428505</td>\n      <td>i</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>140437</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190496.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428527</td>\n      <td>n</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>140438</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190503.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428607</td>\n      <td>g</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>140439</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190720.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428786</td>\n      <td>.</td>\n      <td>190</td>\n    </tr>\n    <tr>\n      <th>140440</th>\n      <td>100550</td>\n      <td>1097261</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>You must therefore take full responsibility fo...</td>\n      <td>52190726.0</td>\n      <td>1.473280e+12</td>\n      <td>1473280428837</td>\n      <td></td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n<p>140441 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# clean the csv file by removing rows with more than one column with NaN or <unset> values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_csv(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Cleans a CSV file by removing rows where more than one column has NaN or <unset> values.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save cleaned data.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv, sep=',')\n",
    "\n",
    "    # Define a function to check for NaN or <unset> values\n",
    "    def is_unset_or_nan(value):\n",
    "        return pd.isna(value) or value == '<unset>'\n",
    "\n",
    "    # Identify and filter out rows where more than one column has NaN or <unset> values\n",
    "    malformed_rows = df[df.apply(lambda row: sum(is_unset_or_nan(val) for val in row) > 1, axis=1)]\n",
    "    cleaned_df = df[df.apply(lambda row: sum(is_unset_or_nan(val) for val in row) <= 1, axis=1)]\n",
    "\n",
    "    # Log the number of malformed rows removed\n",
    "    print(f\"Total malformed rows removed: {len(malformed_rows)}\")\n",
    "    print(\"Malformed rows:\")\n",
    "    print(malformed_rows)\n",
    "\n",
    "    # Save the cleaned DataFrame to a new CSV file\n",
    "    cleaned_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Cleaned data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/uncleaned_all.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/cleaned_samples_combined.csv'  # Replace with your desired output CSV path\n",
    "clean_csv(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:16:06.305879Z",
     "start_time": "2024-09-11T22:16:03.229353Z"
    }
   },
   "id": "4abe0a38f109b65b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total malformed rows removed: 0\n",
      "Malformed rows:\n",
      "Empty DataFrame\n",
      "Columns: [PARTICIPANT_ID, TEST_SECTION_ID, SENTENCE, USER_INPUT, KEYSTROKE_ID, PRESS_TIME, RELEASE_TIME, LETTER, KEYCODE]\n",
      "Index: []\n",
      "Cleaned data saved to demographics_csv/cleaned_samples_combined.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 adding keystroke values\n",
    "### generate D1U2, D1U3, D1D2, D1D3, D1U1_MEAN, D1U2_MEAN, D1U3_MEAN, D1D2_MEAN, D1D3_MEAN, U1D2, U1D2_MEAN, Z_SCORE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5cc67700d630cc4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to demographics_csv/added_key_values.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_new_columns(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Adds new columns to a CSV file for each participant and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Define a function to calculate new columns for each participant\n",
    "    def calculate_new_columns(group):\n",
    "        group['D1U1'] = group['RELEASE_TIME'] - group['PRESS_TIME']\n",
    "        group['D1U2'] = group['RELEASE_TIME'].shift(-1) - group['PRESS_TIME']\n",
    "        group['D1U3'] = group['RELEASE_TIME'].shift(-2) - group['PRESS_TIME']\n",
    "        group['D1D2'] = group['PRESS_TIME'].shift(-1) - group['PRESS_TIME']\n",
    "        group['D1D3'] = group['PRESS_TIME'].shift(-2) - group['PRESS_TIME']\n",
    "        group['U1D2'] = group['PRESS_TIME'].shift(-1) - group['RELEASE_TIME']\n",
    "        \n",
    "        group['D1U1_MEAN'] = group['D1U1'].mean()\n",
    "        group['D1U2_MEAN'] = group['D1U2'].mean()\n",
    "        group['D1U3_MEAN'] = group['D1U3'].mean()\n",
    "        group['D1D2_MEAN'] = group['D1D2'].mean()\n",
    "        group['D1D3_MEAN'] = group['D1D3'].mean()\n",
    "        group['U1D2_MEAN'] = group['U1D2'].mean()\n",
    "        \n",
    "        group['Z_SCORE'] = (group['PRESS_TIME'] - group['PRESS_TIME'].mean()) / group['PRESS_TIME'].std()\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each participant group\n",
    "    df = df.groupby('PARTICIPANT_ID').apply(calculate_new_columns)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/part1_cleaned_samples_combined.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/added_key_values.csv'  # Replace with your desired output CSV path\n",
    "add_new_columns(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-06T15:50:58.978712Z",
     "start_time": "2024-11-06T15:50:53.521309Z"
    }
   },
   "id": "71347e8e8b2e1bba",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-26T15:24:56.641406Z",
     "start_time": "2024-10-26T15:24:56.627424Z"
    }
   },
   "id": "a6522eea8839eeeb",
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T15:50:45.046096Z",
     "start_time": "2024-11-06T15:50:32.732220Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001C108A1EAD0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 46\u001B[0m\n\u001B[0;32m     44\u001B[0m input_csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdemographics_csv/cleaned_samples_combined.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Replace with your actual input CSV path\u001B[39;00m\n\u001B[0;32m     45\u001B[0m output_csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdemographics_csv/added_key_values.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Replace with your desired output CSV path\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m \u001B[43madd_new_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_csv_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_csv_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 39\u001B[0m, in \u001B[0;36madd_new_columns\u001B[1;34m(input_csv, output_csv)\u001B[0m\n\u001B[0;32m     36\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPARTICIPANT_ID\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mapply(calculate_new_columns)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Save the updated DataFrame to a new CSV file\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_csv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpdated data saved to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_csv\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\core\\generic.py:3466\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3455\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3457\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3458\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3459\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3463\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3464\u001B[0m )\n\u001B[1;32m-> 3466\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3467\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mline_terminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mline_terminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3469\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3470\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3471\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3472\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3473\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3474\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3475\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3476\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3479\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3480\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3482\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3483\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\formats\\format.py:1105\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1084\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1086\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m   1087\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m   1088\u001B[0m     line_terminator\u001B[38;5;241m=\u001B[39mline_terminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1103\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1104\u001B[0m )\n\u001B[1;32m-> 1105\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1108\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:257\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath_or_buffer,\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    245\u001B[0m \n\u001B[0;32m    246\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    248\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    249\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mline_terminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    254\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    255\u001B[0m     )\n\u001B[1;32m--> 257\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:262\u001B[0m, in \u001B[0;36mCSVFormatter._save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_need_to_save_header:\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_header()\n\u001B[1;32m--> 262\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:300\u001B[0m, in \u001B[0;36mCSVFormatter._save_body\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start_i \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m end_i:\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m--> 300\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_i\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:307\u001B[0m, in \u001B[0;36mCSVFormatter._save_chunk\u001B[1;34m(self, start_i, end_i)\u001B[0m\n\u001B[0;32m    304\u001B[0m slicer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(start_i, end_i)\n\u001B[0;32m    305\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39miloc[slicer]\n\u001B[1;32m--> 307\u001B[0m res \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mto_native_types(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_number_format)\n\u001B[0;32m    308\u001B[0m data \u001B[38;5;241m=\u001B[39m [res\u001B[38;5;241m.\u001B[39miget_values(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(res\u001B[38;5;241m.\u001B[39mitems))]\n\u001B[0;32m    310\u001B[0m ix \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_index[slicer]\u001B[38;5;241m.\u001B[39m_format_native_types(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_number_format)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:466\u001B[0m, in \u001B[0;36mBaseBlockManager.to_native_types\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_native_types\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m    462\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;124;03m    Convert values to native types (strings / python objects) that are used\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;124;03m    in formatting (repr / csv).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 466\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_native_types\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:327\u001B[0m, in \u001B[0;36mBaseBlockManager.apply\u001B[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m         applied \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mapply(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 327\u001B[0m         applied \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(b, f)(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    328\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mNotImplementedError\u001B[39;00m):\n\u001B[0;32m    329\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ignore_failures:\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:641\u001B[0m, in \u001B[0;36mBlock.to_native_types\u001B[1;34m(self, na_rep, quoting, **kwargs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_native_types\u001B[39m(\u001B[38;5;28mself\u001B[39m, na_rep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnan\u001B[39m\u001B[38;5;124m\"\u001B[39m, quoting\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    640\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"convert to our native types format\"\"\"\u001B[39;00m\n\u001B[1;32m--> 641\u001B[0m     result \u001B[38;5;241m=\u001B[39m to_native_types(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues, na_rep\u001B[38;5;241m=\u001B[39mna_rep, quoting\u001B[38;5;241m=\u001B[39mquoting, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    642\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_block(result)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:2078\u001B[0m, in \u001B[0;36mto_native_types\u001B[1;34m(values, na_rep, quoting, float_format, decimal, **kwargs)\u001B[0m\n\u001B[0;32m   2075\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2076\u001B[0m     values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(values, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2078\u001B[0m \u001B[43mvalues\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m na_rep\n\u001B[0;32m   2079\u001B[0m values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   2080\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m values\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "#attempt 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_new_columns(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Adds new columns to a CSV file for each participant and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Define a function to calculate new columns for each participant\n",
    "    def calculate_new_columns(group):\n",
    "        group['D1U1'] = group['KEYSTROKE_ID'] * 1.1\n",
    "        group['D1U2'] = group['KEYSTROKE_ID'] * 1.2\n",
    "        group['D1U3'] = group['KEYSTROKE_ID'] * 1.3\n",
    "        group['D1D2'] = group['KEYSTROKE_ID'] * 1.4\n",
    "        group['D1D3'] = group['KEYSTROKE_ID'] * 1.5\n",
    "        group['D1U1_MEAN'] = group['D1U1'].mean()\n",
    "        group['D1U2_MEAN'] = group['D1U2'].mean()\n",
    "        group['D1U3_MEAN'] = group['D1U3'].mean()\n",
    "        group['D1D2_MEAN'] = group['D1D2'].mean()\n",
    "        group['D1D3_MEAN'] = group['D1D3'].mean()\n",
    "        group['U1D2'] = group['D1U1'] - group['D1D2']\n",
    "        group['U1D2_MEAN'] = group['U1D2'].mean()\n",
    "        group['Z_SCORE'] = (group['KEYSTROKE_ID'] - group['KEYSTROKE_ID'].mean()) / group['KEYSTROKE_ID'].std()\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each participant group\n",
    "    df = df.groupby('PARTICIPANT_ID').apply(calculate_new_columns)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/cleaned_samples_combined.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/added_key_values.csv'  # Replace with your desired output CSV path\n",
    "add_new_columns(input_csv_path, output_csv_path)"
   ],
   "id": "c19c57d86e6f7146"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 22:39:05,951 - INFO - Starting to read the input CSV file.\n",
      "2024-11-13 22:39:15,929 - INFO - Finished reading the input CSV file.\n",
      "2024-11-13 22:39:15,980 - INFO - Found 9677 unique participant IDs.\n",
      "2024-11-13 22:39:15,981 - INFO - Selected the first 800 unique participant IDs.\n",
      "2024-11-13 22:39:16,062 - INFO - Filtered the DataFrame to include only the selected participant IDs.\n",
      "2024-11-13 22:39:18,948 - INFO - Filtered data saved to demographics_csv/reduced800_14_participants.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def reduce_participant_ids(input_csv, output_csv, num_ids=800):\n",
    "    \"\"\"\n",
    "    Reduces the number of unique participant IDs in the CSV file to the specified number.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file.\n",
    "    - num_ids (int): The number of unique participant IDs to retain.\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info('Starting to read the input CSV file.')\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "    logging.info('Finished reading the input CSV file.')\n",
    "\n",
    "    # Get the unique participant IDs\n",
    "    unique_ids = df['PARTICIPANT_ID'].unique()\n",
    "    logging.info(f'Found {len(unique_ids)} unique participant IDs.')\n",
    "\n",
    "    # Select the first num_ids unique participant IDs\n",
    "    selected_ids = unique_ids[:num_ids]\n",
    "    logging.info(f'Selected the first {num_ids} unique participant IDs.')\n",
    "\n",
    "    # Filter the DataFrame to include only the selected participant IDs\n",
    "    filtered_df = df[df['PARTICIPANT_ID'].isin(selected_ids)]\n",
    "    logging.info('Filtered the DataFrame to include only the selected participant IDs.')\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv, index=False)\n",
    "    logging.info(f'Filtered data saved to {output_csv}')\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'samples/part14_cleaned_samples_combined.csv'  # Replace with the path to your input CSV file\n",
    "output_csv_path = 'demographics_csv/reduced800_14_participants.csv'  # Replace with the desired output CSV path\n",
    "reduce_participant_ids(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:39:19.035126Z",
     "start_time": "2024-11-13T21:39:05.948403Z"
    }
   },
   "id": "e55b441f8ca4ff",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# attempt 2 works?!\n",
    "# calulations:\n",
    "#D1U1 = R1 - P1\n",
    "#D1U2 = R2 - P1\n",
    "#D1U3 = R3 - P1\n",
    "#D1D2 = P2 - P1\n",
    "#D1D3 = P3 - P1\n",
    "#U1D2 = P2 - R1\n",
    "#U1D2_MEAN = mean(U1D2)\n",
    "#Z_SCORE = (P1 - mean(P1)) / std(P1)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_new_columns(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Adds new columns to a CSV file for each participant and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Initialize new DataFrame with the required columns\n",
    "    new_df_columns = ['PARTICIPANT_ID', 'TEST_SECTION_ID', 'SENTENCE', 'USER_INPUT', 'KEYSTROKE_ID', 'PRESS_TIME', 'RELEASE_TIME', 'LETTER', 'KEYCODE', 'D1U1', 'D1U2', 'D1D2', 'U1D2', 'U1U2', 'D1U3', 'D1D3', 'D1U1_MEAN', 'D1U2_MEAN', 'D1U3_MEAN', 'D1D2_MEAN', 'D1D3_MEAN', 'U1D2_MEAN', 'Z_SCORE']\n",
    "    new_df = pd.DataFrame(columns=new_df_columns)\n",
    "\n",
    "    # Variables to keep track of previous and next events\n",
    "    previous_down = None\n",
    "    previous_up = None\n",
    "    next_down = None\n",
    "    next_up = None\n",
    "    after_next_down = None\n",
    "    after_next_up = None\n",
    "\n",
    "    # Calculate Time Differences and Fill New DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        keyCode = row['KEYCODE']\n",
    "        keyDown = row['PRESS_TIME']\n",
    "        keyUp = row['RELEASE_TIME']\n",
    "\n",
    "        # Calculate D1U1 (current down to current up)\n",
    "        D1U1 = keyUp - keyDown\n",
    "\n",
    "        # Placeholder values for other calculations\n",
    "        D1U2, D1D2, U1D2, U1U2, D1U3, D1D3 = [None] * 6\n",
    "\n",
    "        if i > 0:  # If not the first row, calculate values involving previous key\n",
    "            D1D2 = keyDown - previous_down\n",
    "            U1D2 = keyDown - previous_up\n",
    "            U1U2 = keyUp - previous_up\n",
    "\n",
    "        if i < len(df) - 1:  # If not the last row, look ahead to calculate future values\n",
    "            next_row = df.iloc[i + 1]\n",
    "            next_down = next_row['PRESS_TIME']\n",
    "            next_up = next_row['RELEASE_TIME']\n",
    "            D1U2 = next_up - keyDown\n",
    "            D1U3 = next_up - keyDown  # Placeholder, needs adjustment for \"key after next\"\n",
    "            \n",
    "            \n",
    "        if i < len(df) - 2:  # If there are at least two keys ahead, calculate D1D3 and D1U3\n",
    "            after_next_row = df.iloc[i + 2]\n",
    "            after_next_down = after_next_row['PRESS_TIME']\n",
    "            after_next_up = after_next_row['RELEASE_TIME']\n",
    "            D1D3 = after_next_down - keyDown\n",
    "            D1U3 = after_next_up - keyDown\n",
    "\n",
    "        # For D1D3, need to look two keys ahead, which requires additional logic not shown here\n",
    "\n",
    "        # Append row to new DataFrame\n",
    "        new_row = {'PARTICIPANT_ID': row['PARTICIPANT_ID'], 'TEST_SECTION_ID': row['TEST_SECTION_ID'], 'SENTENCE': row['SENTENCE'], 'USER_INPUT': row['USER_INPUT'], 'KEYSTROKE_ID': row['KEYSTROKE_ID'], 'PRESS_TIME': keyDown, 'RELEASE_TIME': keyUp, 'LETTER': row['LETTER'], 'KEYCODE': keyCode, 'D1U1': D1U1, 'D1U2': D1U2, 'D1D2': D1D2, 'U1D2': U1D2, 'U1U2': U1U2, 'D1U3': D1U3, 'D1D3': D1D3}\n",
    "        new_df = new_df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # Update previous events\n",
    "        previous_down = keyDown\n",
    "        previous_up = keyUp\n",
    "\n",
    "    # Calculate means and Z_SCORE\n",
    "    new_df['D1U1_MEAN'] = new_df['D1U1'].mean()\n",
    "    new_df['D1U2_MEAN'] = new_df['D1U2'].mean()\n",
    "    new_df['D1U3_MEAN'] = new_df['D1U3'].mean()\n",
    "    new_df['D1D2_MEAN'] = new_df['D1D2'].mean()\n",
    "    new_df['D1D3_MEAN'] = new_df['D1D3'].mean()\n",
    "    new_df['U1D2_MEAN'] = new_df['U1D2'].mean()\n",
    "    new_df['Z_SCORE'] = (new_df['PRESS_TIME'] - new_df['PRESS_TIME'].mean()) / new_df['PRESS_TIME'].std()\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    new_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'samples/part14_cleaned_samples_combined.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/addedkey_values.csv'  # Replace with your desired output CSV path\n",
    "add_new_columns(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "32bec364eb15dc70",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def add_new_columns(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Adds new columns to a CSV file for each participant and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Read the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "    logging.info('Finished reading the input CSV file.')\n",
    "\n",
    "    # Initialize new DataFrame with the required columns\n",
    "    new_df_columns = ['PARTICIPANT_ID', 'TEST_SECTION_ID', 'SENTENCE', 'USER_INPUT', 'KEYSTROKE_ID', 'PRESS_TIME', 'RELEASE_TIME', 'LETTER', 'KEYCODE', 'D1U1', 'D1U2', 'D1D2', 'U1D2', 'U1U2', 'D1U3', 'D1D3', 'D1U1_MEAN', 'D1U2_MEAN', 'D1U3_MEAN', 'D1D2_MEAN', 'D1D3_MEAN', 'U1D2_MEAN', 'Z_SCORE']\n",
    "    new_df = pd.DataFrame(columns=new_df_columns)\n",
    "\n",
    "    # Variables to keep track of previous and next events\n",
    "    previous_down = None\n",
    "    previous_up = None\n",
    "    next_down = None\n",
    "    next_up = None\n",
    "    after_next_down = None\n",
    "    after_next_up = None\n",
    "\n",
    "    # Initialize participant counter\n",
    "    participant_counter = 0\n",
    "    current_participant_id = None\n",
    "\n",
    "    # Calculate Time Differences and Fill New DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        participant_id = row['PARTICIPANT_ID']\n",
    "        if participant_id != current_participant_id:\n",
    "            current_participant_id = participant_id\n",
    "            logging.info(f'Processing new PARTICIPANT_ID: {participant_id}, Counter: {participant_counter}')\n",
    "            participant_counter += 1\n",
    "\n",
    "        keyCode = row['KEYCODE']\n",
    "        keyDown = row['PRESS_TIME']\n",
    "        keyUp = row['RELEASE_TIME']\n",
    "\n",
    "        # Calculate D1U1 (current down to current up)\n",
    "        D1U1 = keyUp - keyDown\n",
    "\n",
    "        # Placeholder values for other calculations\n",
    "        D1U2, D1D2, U1D2, U1U2, D1U3, D1D3 = [None] * 6\n",
    "\n",
    "        if i > 0:  # If not the first row, calculate values involving previous key\n",
    "            D1D2 = keyDown - previous_down\n",
    "            U1D2 = keyDown - previous_up\n",
    "            U1U2 = keyUp - previous_up\n",
    "\n",
    "        if i < len(df) - 1:  # If not the last row, look ahead to calculate future values\n",
    "            next_row = df.iloc[i + 1]\n",
    "            next_down = next_row['PRESS_TIME']\n",
    "            next_up = next_row['RELEASE_TIME']\n",
    "            D1U2 = next_up - keyDown\n",
    "            D1U3 = next_up - keyDown  # Placeholder, needs adjustment for \"key after next\"\n",
    "            \n",
    "        if i < len(df) - 2:  # If there are at least two keys ahead, calculate D1D3 and D1U3\n",
    "            after_next_row = df.iloc[i + 2]\n",
    "            after_next_down = after_next_row['PRESS_TIME']\n",
    "            after_next_up = after_next_row['RELEASE_TIME']\n",
    "            D1D3 = after_next_down - keyDown\n",
    "            D1U3 = after_next_up - keyDown\n",
    "\n",
    "        # Append row to new DataFrame\n",
    "        new_row = {'PARTICIPANT_ID': row['PARTICIPANT_ID'], 'TEST_SECTION_ID': row['TEST_SECTION_ID'], 'SENTENCE': row['SENTENCE'], 'USER_INPUT': row['USER_INPUT'], 'KEYSTROKE_ID': row['KEYSTROKE_ID'], 'PRESS_TIME': keyDown, 'RELEASE_TIME': keyUp, 'LETTER': row['LETTER'], 'KEYCODE': keyCode, 'D1U1': D1U1, 'D1U2': D1U2, 'D1D2': D1D2, 'U1D2': U1D2, 'U1U2': U1U2, 'D1U3': D1U3, 'D1D3': D1D3}\n",
    "        new_df = new_df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # Update previous events\n",
    "        previous_down = keyDown\n",
    "        previous_up = keyUp\n",
    "\n",
    "    # Calculate means and Z_SCORE\n",
    "    new_df['D1U1_MEAN'] = new_df['D1U1'].mean()\n",
    "    new_df['D1U2_MEAN'] = new_df['D1U2'].mean()\n",
    "    new_df['D1U3_MEAN'] = new_df['D1U3'].mean()\n",
    "    new_df['D1D2_MEAN'] = new_df['D1D2'].mean()\n",
    "    new_df['D1D3_MEAN'] = new_df['D1D3'].mean()\n",
    "    new_df['U1D2_MEAN'] = new_df['U1D2'].mean()\n",
    "    new_df['Z_SCORE'] = (new_df['PRESS_TIME'] - new_df['PRESS_TIME'].mean()) / new_df['PRESS_TIME'].std()\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    new_df.to_csv(output_csv, index=False)\n",
    "    logging.info(f'Updated data saved to {output_csv}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-13T21:41:06.484315Z",
     "start_time": "2024-11-13T21:41:06.470400Z"
    }
   },
   "id": "bd9051540742b895",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 22:41:16,700 - INFO - Finished reading the input CSV file.\n",
      "2024-11-13 22:41:16,869 - INFO - Processing new PARTICIPANT_ID: 458779, Counter: 0\n",
      "2024-11-13 22:41:22,209 - INFO - Processing new PARTICIPANT_ID: 458780, Counter: 1\n",
      "2024-11-13 22:41:24,676 - INFO - Processing new PARTICIPANT_ID: 458781, Counter: 2\n",
      "2024-11-13 22:41:27,331 - INFO - Processing new PARTICIPANT_ID: 458782, Counter: 3\n",
      "2024-11-13 22:41:29,694 - INFO - Processing new PARTICIPANT_ID: 458788, Counter: 4\n",
      "2024-11-13 22:41:31,830 - INFO - Processing new PARTICIPANT_ID: 458796, Counter: 5\n",
      "2024-11-13 22:41:33,877 - INFO - Processing new PARTICIPANT_ID: 45879, Counter: 6\n",
      "2024-11-13 22:41:36,543 - INFO - Processing new PARTICIPANT_ID: 458803, Counter: 7\n",
      "2024-11-13 22:41:39,115 - INFO - Processing new PARTICIPANT_ID: 458804, Counter: 8\n",
      "2024-11-13 22:41:41,648 - INFO - Processing new PARTICIPANT_ID: 458806, Counter: 9\n",
      "2024-11-13 22:41:47,266 - INFO - Processing new PARTICIPANT_ID: 458817, Counter: 10\n",
      "2024-11-13 22:41:49,476 - INFO - Processing new PARTICIPANT_ID: 458830, Counter: 11\n",
      "2024-11-13 22:41:51,667 - INFO - Processing new PARTICIPANT_ID: 458835, Counter: 12\n",
      "2024-11-13 22:41:54,379 - INFO - Processing new PARTICIPANT_ID: 458837, Counter: 13\n",
      "2024-11-13 22:41:57,186 - INFO - Processing new PARTICIPANT_ID: 45883, Counter: 14\n",
      "2024-11-13 22:41:59,498 - INFO - Processing new PARTICIPANT_ID: 458841, Counter: 15\n",
      "2024-11-13 22:42:02,140 - INFO - Processing new PARTICIPANT_ID: 458843, Counter: 16\n",
      "2024-11-13 22:42:04,970 - INFO - Processing new PARTICIPANT_ID: 458846, Counter: 17\n",
      "2024-11-13 22:42:07,915 - INFO - Processing new PARTICIPANT_ID: 458848, Counter: 18\n",
      "2024-11-13 22:42:10,364 - INFO - Processing new PARTICIPANT_ID: 458849, Counter: 19\n",
      "2024-11-13 22:42:12,734 - INFO - Processing new PARTICIPANT_ID: 45884, Counter: 20\n",
      "2024-11-13 22:42:16,216 - INFO - Processing new PARTICIPANT_ID: 458852, Counter: 21\n",
      "2024-11-13 22:42:20,748 - INFO - Processing new PARTICIPANT_ID: 458855, Counter: 22\n",
      "2024-11-13 22:42:24,665 - INFO - Processing new PARTICIPANT_ID: 45885, Counter: 23\n",
      "2024-11-13 22:42:28,052 - INFO - Processing new PARTICIPANT_ID: 458865, Counter: 24\n",
      "2024-11-13 22:42:31,278 - INFO - Processing new PARTICIPANT_ID: 458866, Counter: 25\n",
      "2024-11-13 22:42:34,764 - INFO - Processing new PARTICIPANT_ID: 458868, Counter: 26\n",
      "2024-11-13 22:42:38,066 - INFO - Processing new PARTICIPANT_ID: 458872, Counter: 27\n",
      "2024-11-13 22:42:41,086 - INFO - Processing new PARTICIPANT_ID: 458875, Counter: 28\n",
      "2024-11-13 22:42:43,744 - INFO - Processing new PARTICIPANT_ID: 458876, Counter: 29\n",
      "2024-11-13 22:42:46,910 - INFO - Processing new PARTICIPANT_ID: 458880, Counter: 30\n",
      "2024-11-13 22:42:50,117 - INFO - Processing new PARTICIPANT_ID: 458883, Counter: 31\n",
      "2024-11-13 22:42:54,312 - INFO - Processing new PARTICIPANT_ID: 458884, Counter: 32\n",
      "2024-11-13 22:42:58,501 - INFO - Processing new PARTICIPANT_ID: 458885, Counter: 33\n",
      "2024-11-13 22:43:01,332 - INFO - Processing new PARTICIPANT_ID: 458887, Counter: 34\n",
      "2024-11-13 22:43:05,775 - INFO - Processing new PARTICIPANT_ID: 458889, Counter: 35\n",
      "2024-11-13 22:43:09,669 - INFO - Processing new PARTICIPANT_ID: 45888, Counter: 36\n",
      "2024-11-13 22:43:12,753 - INFO - Processing new PARTICIPANT_ID: 458898, Counter: 37\n",
      "2024-11-13 22:43:16,956 - INFO - Processing new PARTICIPANT_ID: 458901, Counter: 38\n",
      "2024-11-13 22:43:20,845 - INFO - Processing new PARTICIPANT_ID: 458903, Counter: 39\n",
      "2024-11-13 22:43:25,117 - INFO - Processing new PARTICIPANT_ID: 458912, Counter: 40\n",
      "2024-11-13 22:43:29,583 - INFO - Processing new PARTICIPANT_ID: 458914, Counter: 41\n",
      "2024-11-13 22:43:33,358 - INFO - Processing new PARTICIPANT_ID: 458915, Counter: 42\n",
      "2024-11-13 22:43:39,032 - INFO - Processing new PARTICIPANT_ID: 458918, Counter: 43\n",
      "2024-11-13 22:43:43,423 - INFO - Processing new PARTICIPANT_ID: 45891, Counter: 44\n",
      "2024-11-13 22:43:48,399 - INFO - Processing new PARTICIPANT_ID: 458923, Counter: 45\n",
      "2024-11-13 22:43:53,376 - INFO - Processing new PARTICIPANT_ID: 458926, Counter: 46\n",
      "2024-11-13 22:43:58,334 - INFO - Processing new PARTICIPANT_ID: 458927, Counter: 47\n",
      "2024-11-13 22:44:02,534 - INFO - Processing new PARTICIPANT_ID: 45892, Counter: 48\n",
      "2024-11-13 22:44:07,744 - INFO - Processing new PARTICIPANT_ID: 458931, Counter: 49\n",
      "2024-11-13 22:44:12,589 - INFO - Processing new PARTICIPANT_ID: 458936, Counter: 50\n",
      "2024-11-13 22:44:18,029 - INFO - Processing new PARTICIPANT_ID: 458938, Counter: 51\n",
      "2024-11-13 22:44:22,618 - INFO - Processing new PARTICIPANT_ID: 45893, Counter: 52\n",
      "2024-11-13 22:44:28,337 - INFO - Processing new PARTICIPANT_ID: 458943, Counter: 53\n",
      "2024-11-13 22:44:33,357 - INFO - Processing new PARTICIPANT_ID: 458945, Counter: 54\n",
      "2024-11-13 22:44:38,605 - INFO - Processing new PARTICIPANT_ID: 458947, Counter: 55\n",
      "2024-11-13 22:44:43,436 - INFO - Processing new PARTICIPANT_ID: 458948, Counter: 56\n",
      "2024-11-13 22:44:49,556 - INFO - Processing new PARTICIPANT_ID: 458951, Counter: 57\n",
      "2024-11-13 22:44:55,171 - INFO - Processing new PARTICIPANT_ID: 458954, Counter: 58\n",
      "2024-11-13 22:45:01,042 - INFO - Processing new PARTICIPANT_ID: 45895, Counter: 59\n",
      "2024-11-13 22:45:06,920 - INFO - Processing new PARTICIPANT_ID: 458961, Counter: 60\n",
      "2024-11-13 22:45:13,039 - INFO - Processing new PARTICIPANT_ID: 458969, Counter: 61\n",
      "2024-11-13 22:45:19,350 - INFO - Processing new PARTICIPANT_ID: 458970, Counter: 62\n",
      "2024-11-13 22:45:25,677 - INFO - Processing new PARTICIPANT_ID: 458976, Counter: 63\n",
      "2024-11-13 22:45:31,436 - INFO - Processing new PARTICIPANT_ID: 458977, Counter: 64\n",
      "2024-11-13 22:45:38,561 - INFO - Processing new PARTICIPANT_ID: 458978, Counter: 65\n",
      "2024-11-13 22:45:44,599 - INFO - Processing new PARTICIPANT_ID: 45897, Counter: 66\n",
      "2024-11-13 22:45:50,861 - INFO - Processing new PARTICIPANT_ID: 458985, Counter: 67\n",
      "2024-11-13 22:45:57,191 - INFO - Processing new PARTICIPANT_ID: 458987, Counter: 68\n",
      "2024-11-13 22:46:02,218 - INFO - Processing new PARTICIPANT_ID: 458988, Counter: 69\n",
      "2024-11-13 22:46:08,952 - INFO - Processing new PARTICIPANT_ID: 458997, Counter: 70\n",
      "2024-11-13 22:46:14,864 - INFO - Processing new PARTICIPANT_ID: 459001, Counter: 71\n",
      "2024-11-13 22:46:21,656 - INFO - Processing new PARTICIPANT_ID: 459002, Counter: 72\n",
      "2024-11-13 22:46:28,331 - INFO - Processing new PARTICIPANT_ID: 459003, Counter: 73\n",
      "2024-11-13 22:46:35,986 - INFO - Processing new PARTICIPANT_ID: 459008, Counter: 74\n",
      "2024-11-13 22:46:42,409 - INFO - Processing new PARTICIPANT_ID: 45900, Counter: 75\n",
      "2024-11-13 22:46:49,357 - INFO - Processing new PARTICIPANT_ID: 459028, Counter: 76\n",
      "2024-11-13 22:46:56,068 - INFO - Processing new PARTICIPANT_ID: 45902, Counter: 77\n",
      "2024-11-13 22:47:02,185 - INFO - Processing new PARTICIPANT_ID: 45903, Counter: 78\n",
      "2024-11-13 22:47:09,787 - INFO - Processing new PARTICIPANT_ID: 459049, Counter: 79\n",
      "2024-11-13 22:47:16,799 - INFO - Processing new PARTICIPANT_ID: 45904, Counter: 80\n",
      "2024-11-13 22:47:22,304 - INFO - Processing new PARTICIPANT_ID: 459051, Counter: 81\n",
      "2024-11-13 22:47:28,659 - INFO - Processing new PARTICIPANT_ID: 459064, Counter: 82\n",
      "2024-11-13 22:47:35,993 - INFO - Processing new PARTICIPANT_ID: 459066, Counter: 83\n",
      "2024-11-13 22:47:43,491 - INFO - Processing new PARTICIPANT_ID: 459068, Counter: 84\n",
      "2024-11-13 22:47:51,331 - INFO - Processing new PARTICIPANT_ID: 459072, Counter: 85\n",
      "2024-11-13 22:47:58,327 - INFO - Processing new PARTICIPANT_ID: 459073, Counter: 86\n",
      "2024-11-13 22:48:05,636 - INFO - Processing new PARTICIPANT_ID: 459075, Counter: 87\n",
      "2024-11-13 22:48:14,133 - INFO - Processing new PARTICIPANT_ID: 45907, Counter: 88\n",
      "2024-11-13 22:48:20,947 - INFO - Processing new PARTICIPANT_ID: 459088, Counter: 89\n",
      "2024-11-13 22:48:29,285 - INFO - Processing new PARTICIPANT_ID: 45908, Counter: 90\n",
      "2024-11-13 22:48:35,443 - INFO - Processing new PARTICIPANT_ID: 459092, Counter: 91\n",
      "2024-11-13 22:48:43,576 - INFO - Processing new PARTICIPANT_ID: 459098, Counter: 92\n",
      "2024-11-13 22:48:50,987 - INFO - Processing new PARTICIPANT_ID: 459102, Counter: 93\n",
      "2024-11-13 22:48:58,729 - INFO - Processing new PARTICIPANT_ID: 459104, Counter: 94\n",
      "2024-11-13 22:49:08,337 - INFO - Processing new PARTICIPANT_ID: 459105, Counter: 95\n",
      "2024-11-13 22:49:16,199 - INFO - Processing new PARTICIPANT_ID: 45910, Counter: 96\n",
      "2024-11-13 22:49:23,769 - INFO - Processing new PARTICIPANT_ID: 459113, Counter: 97\n",
      "2024-11-13 22:49:32,551 - INFO - Processing new PARTICIPANT_ID: 459114, Counter: 98\n",
      "2024-11-13 22:49:39,255 - INFO - Processing new PARTICIPANT_ID: 459125, Counter: 99\n",
      "2024-11-13 22:49:46,543 - INFO - Processing new PARTICIPANT_ID: 459138, Counter: 100\n",
      "2024-11-13 22:49:54,251 - INFO - Processing new PARTICIPANT_ID: 459140, Counter: 101\n",
      "2024-11-13 22:50:01,607 - INFO - Processing new PARTICIPANT_ID: 459142, Counter: 102\n",
      "2024-11-13 22:50:09,362 - INFO - Processing new PARTICIPANT_ID: 459144, Counter: 103\n",
      "2024-11-13 22:50:18,377 - INFO - Processing new PARTICIPANT_ID: 459154, Counter: 104\n",
      "2024-11-13 22:50:25,769 - INFO - Processing new PARTICIPANT_ID: 459156, Counter: 105\n",
      "2024-11-13 22:50:35,056 - INFO - Processing new PARTICIPANT_ID: 459164, Counter: 106\n",
      "2024-11-13 22:50:42,432 - INFO - Processing new PARTICIPANT_ID: 459165, Counter: 107\n",
      "2024-11-13 22:50:52,251 - INFO - Processing new PARTICIPANT_ID: 45916, Counter: 108\n",
      "2024-11-13 22:51:01,675 - INFO - Processing new PARTICIPANT_ID: 459183, Counter: 109\n",
      "2024-11-13 22:51:10,282 - INFO - Processing new PARTICIPANT_ID: 459189, Counter: 110\n",
      "2024-11-13 22:51:21,197 - INFO - Processing new PARTICIPANT_ID: 459191, Counter: 111\n",
      "2024-11-13 22:51:32,788 - INFO - Processing new PARTICIPANT_ID: 459196, Counter: 112\n",
      "2024-11-13 22:51:40,049 - INFO - Processing new PARTICIPANT_ID: 459197, Counter: 113\n",
      "2024-11-13 22:51:48,661 - INFO - Processing new PARTICIPANT_ID: 459199, Counter: 114\n",
      "2024-11-13 22:51:57,194 - INFO - Processing new PARTICIPANT_ID: 459204, Counter: 115\n",
      "2024-11-13 22:52:05,341 - INFO - Processing new PARTICIPANT_ID: 459206, Counter: 116\n",
      "2024-11-13 22:52:13,913 - INFO - Processing new PARTICIPANT_ID: 459220, Counter: 117\n",
      "2024-11-13 22:52:22,548 - INFO - Processing new PARTICIPANT_ID: 459221, Counter: 118\n",
      "2024-11-13 22:52:32,278 - INFO - Processing new PARTICIPANT_ID: 45922, Counter: 119\n",
      "2024-11-13 22:52:40,176 - INFO - Processing new PARTICIPANT_ID: 459230, Counter: 120\n",
      "2024-11-13 22:52:51,671 - INFO - Processing new PARTICIPANT_ID: 459231, Counter: 121\n",
      "2024-11-13 22:53:00,671 - INFO - Processing new PARTICIPANT_ID: 459232, Counter: 122\n",
      "2024-11-13 22:53:08,789 - INFO - Processing new PARTICIPANT_ID: 459237, Counter: 123\n",
      "2024-11-13 22:53:19,168 - INFO - Processing new PARTICIPANT_ID: 459238, Counter: 124\n",
      "2024-11-13 22:53:30,187 - INFO - Processing new PARTICIPANT_ID: 459243, Counter: 125\n",
      "2024-11-13 22:53:38,457 - INFO - Processing new PARTICIPANT_ID: 459247, Counter: 126\n",
      "2024-11-13 22:53:48,106 - INFO - Processing new PARTICIPANT_ID: 459249, Counter: 127\n",
      "2024-11-13 22:53:58,797 - INFO - Processing new PARTICIPANT_ID: 459251, Counter: 128\n",
      "2024-11-13 22:54:08,004 - INFO - Processing new PARTICIPANT_ID: 459257, Counter: 129\n",
      "2024-11-13 22:54:16,984 - INFO - Processing new PARTICIPANT_ID: 459266, Counter: 130\n",
      "2024-11-13 22:54:28,063 - INFO - Processing new PARTICIPANT_ID: 459267, Counter: 131\n",
      "2024-11-13 22:54:36,083 - INFO - Processing new PARTICIPANT_ID: 459272, Counter: 132\n",
      "2024-11-13 22:54:47,605 - INFO - Processing new PARTICIPANT_ID: 459278, Counter: 133\n",
      "2024-11-13 22:54:57,942 - INFO - Processing new PARTICIPANT_ID: 45927, Counter: 134\n",
      "2024-11-13 22:55:09,661 - INFO - Processing new PARTICIPANT_ID: 459281, Counter: 135\n",
      "2024-11-13 22:55:18,809 - INFO - Processing new PARTICIPANT_ID: 459284, Counter: 136\n",
      "2024-11-13 22:55:30,451 - INFO - Processing new PARTICIPANT_ID: 459285, Counter: 137\n",
      "2024-11-13 22:55:44,175 - INFO - Processing new PARTICIPANT_ID: 45928, Counter: 138\n",
      "2024-11-13 22:55:55,682 - INFO - Processing new PARTICIPANT_ID: 459292, Counter: 139\n",
      "2024-11-13 22:56:07,075 - INFO - Processing new PARTICIPANT_ID: 459293, Counter: 140\n",
      "2024-11-13 22:56:19,286 - INFO - Processing new PARTICIPANT_ID: 459295, Counter: 141\n",
      "2024-11-13 22:56:29,967 - INFO - Processing new PARTICIPANT_ID: 459296, Counter: 142\n",
      "2024-11-13 22:56:38,697 - INFO - Processing new PARTICIPANT_ID: 459300, Counter: 143\n",
      "2024-11-13 22:56:49,088 - INFO - Processing new PARTICIPANT_ID: 459302, Counter: 144\n",
      "2024-11-13 22:57:03,154 - INFO - Processing new PARTICIPANT_ID: 459303, Counter: 145\n",
      "2024-11-13 22:57:14,815 - INFO - Processing new PARTICIPANT_ID: 459305, Counter: 146\n",
      "2024-11-13 22:57:25,372 - INFO - Processing new PARTICIPANT_ID: 459307, Counter: 147\n",
      "2024-11-13 22:57:38,469 - INFO - Processing new PARTICIPANT_ID: 459313, Counter: 148\n",
      "2024-11-13 22:57:49,773 - INFO - Processing new PARTICIPANT_ID: 459316, Counter: 149\n",
      "2024-11-13 22:58:01,078 - INFO - Processing new PARTICIPANT_ID: 459318, Counter: 150\n",
      "2024-11-13 22:58:11,883 - INFO - Processing new PARTICIPANT_ID: 459319, Counter: 151\n",
      "2024-11-13 22:58:21,221 - INFO - Processing new PARTICIPANT_ID: 45931, Counter: 152\n",
      "2024-11-13 22:58:31,001 - INFO - Processing new PARTICIPANT_ID: 459320, Counter: 153\n",
      "2024-11-13 22:58:42,480 - INFO - Processing new PARTICIPANT_ID: 459321, Counter: 154\n",
      "2024-11-13 22:58:52,371 - INFO - Processing new PARTICIPANT_ID: 459322, Counter: 155\n",
      "2024-11-13 22:59:03,983 - INFO - Processing new PARTICIPANT_ID: 459326, Counter: 156\n",
      "2024-11-13 22:59:16,180 - INFO - Processing new PARTICIPANT_ID: 459327, Counter: 157\n",
      "2024-11-13 22:59:28,762 - INFO - Processing new PARTICIPANT_ID: 459328, Counter: 158\n",
      "2024-11-13 22:59:39,847 - INFO - Processing new PARTICIPANT_ID: 459330, Counter: 159\n",
      "2024-11-13 22:59:55,347 - INFO - Processing new PARTICIPANT_ID: 459331, Counter: 160\n",
      "2024-11-13 23:00:10,075 - INFO - Processing new PARTICIPANT_ID: 45933, Counter: 161\n",
      "2024-11-13 23:00:21,107 - INFO - Processing new PARTICIPANT_ID: 459340, Counter: 162\n",
      "2024-11-13 23:00:36,286 - INFO - Processing new PARTICIPANT_ID: 459347, Counter: 163\n",
      "2024-11-13 23:00:49,238 - INFO - Processing new PARTICIPANT_ID: 459348, Counter: 164\n",
      "2024-11-13 23:01:00,376 - INFO - Processing new PARTICIPANT_ID: 459352, Counter: 165\n",
      "2024-11-13 23:01:11,972 - INFO - Processing new PARTICIPANT_ID: 45935, Counter: 166\n",
      "2024-11-13 23:01:26,662 - INFO - Processing new PARTICIPANT_ID: 459360, Counter: 167\n",
      "2024-11-13 23:01:38,104 - INFO - Processing new PARTICIPANT_ID: 459361, Counter: 168\n",
      "2024-11-13 23:01:49,060 - INFO - Processing new PARTICIPANT_ID: 459375, Counter: 169\n",
      "2024-11-13 23:02:02,101 - INFO - Processing new PARTICIPANT_ID: 459376, Counter: 170\n",
      "2024-11-13 23:02:16,045 - INFO - Processing new PARTICIPANT_ID: 459377, Counter: 171\n",
      "2024-11-13 23:02:26,888 - INFO - Processing new PARTICIPANT_ID: 459378, Counter: 172\n",
      "2024-11-13 23:02:39,494 - INFO - Processing new PARTICIPANT_ID: 45937, Counter: 173\n",
      "2024-11-13 23:02:50,082 - INFO - Processing new PARTICIPANT_ID: 459381, Counter: 174\n",
      "2024-11-13 23:03:01,873 - INFO - Processing new PARTICIPANT_ID: 459384, Counter: 175\n",
      "2024-11-13 23:03:14,867 - INFO - Processing new PARTICIPANT_ID: 459385, Counter: 176\n",
      "2024-11-13 23:03:29,588 - INFO - Processing new PARTICIPANT_ID: 459388, Counter: 177\n",
      "2024-11-13 23:03:41,660 - INFO - Processing new PARTICIPANT_ID: 45938, Counter: 178\n",
      "2024-11-13 23:03:53,903 - INFO - Processing new PARTICIPANT_ID: 459391, Counter: 179\n",
      "2024-11-13 23:04:06,792 - INFO - Processing new PARTICIPANT_ID: 459392, Counter: 180\n",
      "2024-11-13 23:04:18,301 - INFO - Processing new PARTICIPANT_ID: 459398, Counter: 181\n",
      "2024-11-13 23:04:32,896 - INFO - Processing new PARTICIPANT_ID: 45939, Counter: 182\n",
      "2024-11-13 23:04:44,573 - INFO - Processing new PARTICIPANT_ID: 459411, Counter: 183\n",
      "2024-11-13 23:04:57,278 - INFO - Processing new PARTICIPANT_ID: 459413, Counter: 184\n",
      "2024-11-13 23:05:11,636 - INFO - Processing new PARTICIPANT_ID: 459420, Counter: 185\n",
      "2024-11-13 23:05:24,273 - INFO - Processing new PARTICIPANT_ID: 459423, Counter: 186\n",
      "2024-11-13 23:05:35,898 - INFO - Processing new PARTICIPANT_ID: 459429, Counter: 187\n",
      "2024-11-13 23:05:50,414 - INFO - Processing new PARTICIPANT_ID: 459432, Counter: 188\n",
      "2024-11-13 23:06:06,369 - INFO - Processing new PARTICIPANT_ID: 45943, Counter: 189\n",
      "2024-11-13 23:06:21,485 - INFO - Processing new PARTICIPANT_ID: 459441, Counter: 190\n",
      "2024-11-13 23:06:38,482 - INFO - Processing new PARTICIPANT_ID: 459442, Counter: 191\n",
      "2024-11-13 23:06:51,809 - INFO - Processing new PARTICIPANT_ID: 459443, Counter: 192\n",
      "2024-11-13 23:07:03,524 - INFO - Processing new PARTICIPANT_ID: 459448, Counter: 193\n",
      "2024-11-13 23:07:19,546 - INFO - Processing new PARTICIPANT_ID: 45944, Counter: 194\n",
      "2024-11-13 23:07:32,816 - INFO - Processing new PARTICIPANT_ID: 459451, Counter: 195\n",
      "2024-11-13 23:07:44,871 - INFO - Processing new PARTICIPANT_ID: 459453, Counter: 196\n",
      "2024-11-13 23:07:58,067 - INFO - Processing new PARTICIPANT_ID: 459456, Counter: 197\n",
      "2024-11-13 23:08:13,066 - INFO - Processing new PARTICIPANT_ID: 459457, Counter: 198\n",
      "2024-11-13 23:08:27,888 - INFO - Processing new PARTICIPANT_ID: 459459, Counter: 199\n",
      "2024-11-13 23:08:41,480 - INFO - Processing new PARTICIPANT_ID: 459461, Counter: 200\n",
      "2024-11-13 23:08:57,044 - INFO - Processing new PARTICIPANT_ID: 459462, Counter: 201\n",
      "2024-11-13 23:09:12,080 - INFO - Processing new PARTICIPANT_ID: 459463, Counter: 202\n",
      "2024-11-13 23:09:27,219 - INFO - Processing new PARTICIPANT_ID: 459464, Counter: 203\n",
      "2024-11-13 23:09:43,833 - INFO - Processing new PARTICIPANT_ID: 459476, Counter: 204\n",
      "2024-11-13 23:09:55,649 - INFO - Processing new PARTICIPANT_ID: 459478, Counter: 205\n",
      "2024-11-13 23:10:13,679 - INFO - Processing new PARTICIPANT_ID: 45948, Counter: 206\n",
      "2024-11-13 23:10:31,741 - INFO - Processing new PARTICIPANT_ID: 459493, Counter: 207\n",
      "2024-11-13 23:10:46,144 - INFO - Processing new PARTICIPANT_ID: 45949, Counter: 208\n",
      "2024-11-13 23:11:02,589 - INFO - Processing new PARTICIPANT_ID: 459500, Counter: 209\n",
      "2024-11-13 23:11:21,540 - INFO - Processing new PARTICIPANT_ID: 459506, Counter: 210\n",
      "2024-11-13 23:11:37,193 - INFO - Processing new PARTICIPANT_ID: 459508, Counter: 211\n",
      "2024-11-13 23:11:50,017 - INFO - Processing new PARTICIPANT_ID: 459511, Counter: 212\n",
      "2024-11-13 23:12:06,266 - INFO - Processing new PARTICIPANT_ID: 459515, Counter: 213\n",
      "2024-11-13 23:12:22,655 - INFO - Processing new PARTICIPANT_ID: 45951, Counter: 214\n",
      "2024-11-13 23:12:40,277 - INFO - Processing new PARTICIPANT_ID: 459520, Counter: 215\n",
      "2024-11-13 23:12:56,532 - INFO - Processing new PARTICIPANT_ID: 459521, Counter: 216\n",
      "2024-11-13 23:13:11,521 - INFO - Processing new PARTICIPANT_ID: 459522, Counter: 217\n",
      "2024-11-13 23:13:26,038 - INFO - Processing new PARTICIPANT_ID: 459525, Counter: 218\n",
      "2024-11-13 23:13:45,028 - INFO - Processing new PARTICIPANT_ID: 459526, Counter: 219\n",
      "2024-11-13 23:14:01,016 - INFO - Processing new PARTICIPANT_ID: 459532, Counter: 220\n",
      "2024-11-13 23:14:18,480 - INFO - Processing new PARTICIPANT_ID: 459533, Counter: 221\n",
      "2024-11-13 23:14:31,902 - INFO - Processing new PARTICIPANT_ID: 459535, Counter: 222\n",
      "2024-11-13 23:14:46,849 - INFO - Processing new PARTICIPANT_ID: 459536, Counter: 223\n",
      "2024-11-13 23:15:00,821 - INFO - Processing new PARTICIPANT_ID: 45953, Counter: 224\n",
      "2024-11-13 23:15:15,614 - INFO - Processing new PARTICIPANT_ID: 459543, Counter: 225\n",
      "2024-11-13 23:15:28,287 - INFO - Processing new PARTICIPANT_ID: 459546, Counter: 226\n",
      "2024-11-13 23:15:47,536 - INFO - Processing new PARTICIPANT_ID: 459548, Counter: 227\n",
      "2024-11-13 23:16:06,031 - INFO - Processing new PARTICIPANT_ID: 459549, Counter: 228\n",
      "2024-11-13 23:16:23,270 - INFO - Processing new PARTICIPANT_ID: 45954, Counter: 229\n",
      "2024-11-13 23:16:41,395 - INFO - Processing new PARTICIPANT_ID: 459550, Counter: 230\n",
      "2024-11-13 23:16:58,429 - INFO - Processing new PARTICIPANT_ID: 459553, Counter: 231\n",
      "2024-11-13 23:17:16,942 - INFO - Processing new PARTICIPANT_ID: 459559, Counter: 232\n",
      "2024-11-13 23:17:33,375 - INFO - Processing new PARTICIPANT_ID: 45955, Counter: 233\n",
      "2024-11-13 23:17:49,944 - INFO - Processing new PARTICIPANT_ID: 459562, Counter: 234\n",
      "2024-11-13 23:18:07,153 - INFO - Processing new PARTICIPANT_ID: 459566, Counter: 235\n",
      "2024-11-13 23:18:21,803 - INFO - Processing new PARTICIPANT_ID: 459568, Counter: 236\n",
      "2024-11-13 23:18:39,082 - INFO - Processing new PARTICIPANT_ID: 459572, Counter: 237\n",
      "2024-11-13 23:18:54,619 - INFO - Processing new PARTICIPANT_ID: 459577, Counter: 238\n",
      "2024-11-13 23:19:09,885 - INFO - Processing new PARTICIPANT_ID: 459586, Counter: 239\n",
      "2024-11-13 23:19:23,312 - INFO - Processing new PARTICIPANT_ID: 45958, Counter: 240\n",
      "2024-11-13 23:19:38,026 - INFO - Processing new PARTICIPANT_ID: 459592, Counter: 241\n",
      "2024-11-13 23:19:54,620 - INFO - Processing new PARTICIPANT_ID: 459593, Counter: 242\n",
      "2024-11-13 23:20:14,971 - INFO - Processing new PARTICIPANT_ID: 459594, Counter: 243\n",
      "2024-11-13 23:20:36,870 - INFO - Processing new PARTICIPANT_ID: 459599, Counter: 244\n",
      "2024-11-13 23:20:53,811 - INFO - Processing new PARTICIPANT_ID: 4595, Counter: 245\n",
      "2024-11-13 23:21:08,005 - INFO - Processing new PARTICIPANT_ID: 459615, Counter: 246\n",
      "2024-11-13 23:21:26,736 - INFO - Processing new PARTICIPANT_ID: 459620, Counter: 247\n",
      "2024-11-13 23:21:42,432 - INFO - Processing new PARTICIPANT_ID: 459622, Counter: 248\n",
      "2024-11-13 23:22:00,147 - INFO - Processing new PARTICIPANT_ID: 459627, Counter: 249\n",
      "2024-11-13 23:22:15,546 - INFO - Processing new PARTICIPANT_ID: 459628, Counter: 250\n",
      "2024-11-13 23:22:33,501 - INFO - Processing new PARTICIPANT_ID: 459630, Counter: 251\n",
      "2024-11-13 23:22:50,571 - INFO - Processing new PARTICIPANT_ID: 459634, Counter: 252\n",
      "2024-11-13 23:23:09,906 - INFO - Processing new PARTICIPANT_ID: 459644, Counter: 253\n",
      "2024-11-13 23:23:27,986 - INFO - Processing new PARTICIPANT_ID: 459646, Counter: 254\n",
      "2024-11-13 23:23:47,694 - INFO - Processing new PARTICIPANT_ID: 459649, Counter: 255\n",
      "2024-11-13 23:24:05,983 - INFO - Processing new PARTICIPANT_ID: 45965, Counter: 256\n",
      "2024-11-13 23:24:23,737 - INFO - Processing new PARTICIPANT_ID: 459660, Counter: 257\n",
      "2024-11-13 23:24:47,942 - INFO - Processing new PARTICIPANT_ID: 459666, Counter: 258\n",
      "2024-11-13 23:25:09,663 - INFO - Processing new PARTICIPANT_ID: 459667, Counter: 259\n",
      "2024-11-13 23:25:30,383 - INFO - Processing new PARTICIPANT_ID: 45966, Counter: 260\n",
      "2024-11-13 23:25:47,897 - INFO - Processing new PARTICIPANT_ID: 459674, Counter: 261\n",
      "2024-11-13 23:26:05,338 - INFO - Processing new PARTICIPANT_ID: 459675, Counter: 262\n",
      "2024-11-13 23:26:24,764 - INFO - Processing new PARTICIPANT_ID: 459677, Counter: 263\n",
      "2024-11-13 23:26:45,166 - INFO - Processing new PARTICIPANT_ID: 459678, Counter: 264\n",
      "2024-11-13 23:27:04,601 - INFO - Processing new PARTICIPANT_ID: 459685, Counter: 265\n",
      "2024-11-13 23:27:24,038 - INFO - Processing new PARTICIPANT_ID: 459697, Counter: 266\n",
      "2024-11-13 23:27:41,516 - INFO - Processing new PARTICIPANT_ID: 4596, Counter: 267\n",
      "2024-11-13 23:27:59,952 - INFO - Processing new PARTICIPANT_ID: 459702, Counter: 268\n",
      "2024-11-13 23:28:16,239 - INFO - Processing new PARTICIPANT_ID: 459707, Counter: 269\n",
      "2024-11-13 23:28:39,550 - INFO - Processing new PARTICIPANT_ID: 459708, Counter: 270\n",
      "2024-11-13 23:28:59,804 - INFO - Processing new PARTICIPANT_ID: 459712, Counter: 271\n",
      "2024-11-13 23:29:22,453 - INFO - Processing new PARTICIPANT_ID: 459716, Counter: 272\n",
      "2024-11-13 23:29:39,818 - INFO - Processing new PARTICIPANT_ID: 459725, Counter: 273\n",
      "2024-11-13 23:29:59,666 - INFO - Processing new PARTICIPANT_ID: 459731, Counter: 274\n",
      "2024-11-13 23:30:17,786 - INFO - Processing new PARTICIPANT_ID: 459734, Counter: 275\n",
      "2024-11-13 23:30:34,875 - INFO - Processing new PARTICIPANT_ID: 459736, Counter: 276\n",
      "2024-11-13 23:30:51,265 - INFO - Processing new PARTICIPANT_ID: 45973, Counter: 277\n",
      "2024-11-13 23:31:08,806 - INFO - Processing new PARTICIPANT_ID: 459745, Counter: 278\n",
      "2024-11-13 23:31:26,566 - INFO - Processing new PARTICIPANT_ID: 45974, Counter: 279\n",
      "2024-11-13 23:31:45,779 - INFO - Processing new PARTICIPANT_ID: 459750, Counter: 280\n",
      "2024-11-13 23:32:01,005 - INFO - Processing new PARTICIPANT_ID: 459754, Counter: 281\n",
      "2024-11-13 23:32:23,230 - INFO - Processing new PARTICIPANT_ID: 459760, Counter: 282\n",
      "2024-11-13 23:32:42,658 - INFO - Processing new PARTICIPANT_ID: 459764, Counter: 283\n",
      "2024-11-13 23:33:06,356 - INFO - Processing new PARTICIPANT_ID: 459765, Counter: 284\n",
      "2024-11-13 23:33:22,237 - INFO - Processing new PARTICIPANT_ID: 459782, Counter: 285\n",
      "2024-11-13 23:33:42,632 - INFO - Processing new PARTICIPANT_ID: 459786, Counter: 286\n",
      "2024-11-13 23:34:00,313 - INFO - Processing new PARTICIPANT_ID: 459793, Counter: 287\n",
      "2024-11-13 23:34:22,325 - INFO - Processing new PARTICIPANT_ID: 459797, Counter: 288\n",
      "2024-11-13 23:34:39,294 - INFO - Processing new PARTICIPANT_ID: 459798, Counter: 289\n",
      "2024-11-13 23:34:56,503 - INFO - Processing new PARTICIPANT_ID: 45979, Counter: 290\n",
      "2024-11-13 23:35:13,978 - INFO - Processing new PARTICIPANT_ID: 4597, Counter: 291\n",
      "2024-11-13 23:35:31,356 - INFO - Processing new PARTICIPANT_ID: 459801, Counter: 292\n",
      "2024-11-13 23:35:49,549 - INFO - Processing new PARTICIPANT_ID: 459805, Counter: 293\n",
      "2024-11-13 23:36:11,007 - INFO - Processing new PARTICIPANT_ID: 459808, Counter: 294\n",
      "2024-11-13 23:36:29,198 - INFO - Processing new PARTICIPANT_ID: 459812, Counter: 295\n",
      "2024-11-13 23:36:46,594 - INFO - Processing new PARTICIPANT_ID: 459813, Counter: 296\n",
      "2024-11-13 23:37:04,654 - INFO - Processing new PARTICIPANT_ID: 459817, Counter: 297\n",
      "2024-11-13 23:37:27,016 - INFO - Processing new PARTICIPANT_ID: 459818, Counter: 298\n",
      "2024-11-13 23:37:45,988 - INFO - Processing new PARTICIPANT_ID: 459819, Counter: 299\n",
      "2024-11-13 23:38:07,743 - INFO - Processing new PARTICIPANT_ID: 459829, Counter: 300\n",
      "2024-11-13 23:38:28,206 - INFO - Processing new PARTICIPANT_ID: 45982, Counter: 301\n",
      "2024-11-13 23:38:44,203 - INFO - Processing new PARTICIPANT_ID: 459831, Counter: 302\n",
      "2024-11-13 23:39:03,212 - INFO - Processing new PARTICIPANT_ID: 459835, Counter: 303\n",
      "2024-11-13 23:39:23,159 - INFO - Processing new PARTICIPANT_ID: 459836, Counter: 304\n",
      "2024-11-13 23:39:42,592 - INFO - Processing new PARTICIPANT_ID: 459844, Counter: 305\n",
      "2024-11-13 23:40:09,714 - INFO - Processing new PARTICIPANT_ID: 459845, Counter: 306\n",
      "2024-11-13 23:40:27,878 - INFO - Processing new PARTICIPANT_ID: 459852, Counter: 307\n",
      "2024-11-13 23:40:41,731 - INFO - Processing new PARTICIPANT_ID: 459855, Counter: 308\n",
      "2024-11-13 23:41:01,628 - INFO - Processing new PARTICIPANT_ID: 459857, Counter: 309\n",
      "2024-11-13 23:41:21,822 - INFO - Processing new PARTICIPANT_ID: 459858, Counter: 310\n",
      "2024-11-13 23:41:42,285 - INFO - Processing new PARTICIPANT_ID: 459859, Counter: 311\n",
      "2024-11-13 23:41:58,989 - INFO - Processing new PARTICIPANT_ID: 459860, Counter: 312\n",
      "2024-11-13 23:42:15,146 - INFO - Processing new PARTICIPANT_ID: 459862, Counter: 313\n",
      "2024-11-13 23:42:38,403 - INFO - Processing new PARTICIPANT_ID: 459864, Counter: 314\n",
      "2024-11-13 23:42:56,526 - INFO - Processing new PARTICIPANT_ID: 459867, Counter: 315\n",
      "2024-11-13 23:43:16,363 - INFO - Processing new PARTICIPANT_ID: 459869, Counter: 316\n",
      "2024-11-13 23:43:36,461 - INFO - Processing new PARTICIPANT_ID: 459873, Counter: 317\n",
      "2024-11-13 23:44:00,440 - INFO - Processing new PARTICIPANT_ID: 459874, Counter: 318\n",
      "2024-11-13 23:44:20,221 - INFO - Processing new PARTICIPANT_ID: 459878, Counter: 319\n",
      "2024-11-13 23:44:41,643 - INFO - Processing new PARTICIPANT_ID: 459885, Counter: 320\n",
      "2024-11-13 23:44:59,027 - INFO - Processing new PARTICIPANT_ID: 459887, Counter: 321\n",
      "2024-11-13 23:45:13,020 - INFO - Processing new PARTICIPANT_ID: 459890, Counter: 322\n",
      "2024-11-13 23:45:34,737 - INFO - Processing new PARTICIPANT_ID: 459892, Counter: 323\n",
      "2024-11-13 23:45:55,715 - INFO - Processing new PARTICIPANT_ID: 459894, Counter: 324\n",
      "2024-11-13 23:46:14,979 - INFO - Processing new PARTICIPANT_ID: 4598, Counter: 325\n",
      "2024-11-13 23:46:35,419 - INFO - Processing new PARTICIPANT_ID: 459900, Counter: 326\n",
      "2024-11-13 23:46:55,160 - INFO - Processing new PARTICIPANT_ID: 459904, Counter: 327\n",
      "2024-11-13 23:47:14,163 - INFO - Processing new PARTICIPANT_ID: 459909, Counter: 328\n",
      "2024-11-13 23:47:37,252 - INFO - Processing new PARTICIPANT_ID: 45990, Counter: 329\n",
      "2024-11-13 23:47:59,882 - INFO - Processing new PARTICIPANT_ID: 459911, Counter: 330\n",
      "2024-11-13 23:48:18,647 - INFO - Processing new PARTICIPANT_ID: 459914, Counter: 331\n",
      "2024-11-13 23:48:38,548 - INFO - Processing new PARTICIPANT_ID: 45991, Counter: 332\n",
      "2024-11-13 23:49:02,554 - INFO - Processing new PARTICIPANT_ID: 459925, Counter: 333\n",
      "2024-11-13 23:49:21,834 - INFO - Processing new PARTICIPANT_ID: 459926, Counter: 334\n",
      "2024-11-13 23:49:42,070 - INFO - Processing new PARTICIPANT_ID: 459927, Counter: 335\n",
      "2024-11-13 23:49:58,865 - INFO - Processing new PARTICIPANT_ID: 459930, Counter: 336\n",
      "2024-11-13 23:50:21,216 - INFO - Processing new PARTICIPANT_ID: 459934, Counter: 337\n",
      "2024-11-13 23:50:40,578 - INFO - Processing new PARTICIPANT_ID: 459942, Counter: 338\n",
      "2024-11-13 23:50:59,035 - INFO - Processing new PARTICIPANT_ID: 459944, Counter: 339\n",
      "2024-11-13 23:51:22,480 - INFO - Processing new PARTICIPANT_ID: 459945, Counter: 340\n",
      "2024-11-13 23:51:39,511 - INFO - Processing new PARTICIPANT_ID: 459949, Counter: 341\n",
      "2024-11-13 23:52:00,882 - INFO - Processing new PARTICIPANT_ID: 459950, Counter: 342\n",
      "2024-11-13 23:52:20,748 - INFO - Processing new PARTICIPANT_ID: 459951, Counter: 343\n",
      "2024-11-13 23:52:41,110 - INFO - Processing new PARTICIPANT_ID: 459956, Counter: 344\n",
      "2024-11-13 23:53:05,260 - INFO - Processing new PARTICIPANT_ID: 459958, Counter: 345\n",
      "2024-11-13 23:53:24,795 - INFO - Processing new PARTICIPANT_ID: 459959, Counter: 346\n",
      "2024-11-13 23:53:45,498 - INFO - Processing new PARTICIPANT_ID: 45995, Counter: 347\n",
      "2024-11-13 23:54:04,219 - INFO - Processing new PARTICIPANT_ID: 459960, Counter: 348\n",
      "2024-11-13 23:54:23,041 - INFO - Processing new PARTICIPANT_ID: 459964, Counter: 349\n",
      "2024-11-13 23:54:44,795 - INFO - Processing new PARTICIPANT_ID: 459965, Counter: 350\n",
      "2024-11-13 23:55:07,143 - INFO - Processing new PARTICIPANT_ID: 459970, Counter: 351\n",
      "2024-11-13 23:55:25,592 - INFO - Processing new PARTICIPANT_ID: 459971, Counter: 352\n",
      "2024-11-13 23:55:48,748 - INFO - Processing new PARTICIPANT_ID: 459973, Counter: 353\n",
      "2024-11-13 23:56:14,423 - INFO - Processing new PARTICIPANT_ID: 459974, Counter: 354\n",
      "2024-11-13 23:56:36,614 - INFO - Processing new PARTICIPANT_ID: 459975, Counter: 355\n",
      "2024-11-13 23:56:56,915 - INFO - Processing new PARTICIPANT_ID: 459979, Counter: 356\n",
      "2024-11-13 23:57:21,871 - INFO - Processing new PARTICIPANT_ID: 45997, Counter: 357\n",
      "2024-11-13 23:57:45,747 - INFO - Processing new PARTICIPANT_ID: 459984, Counter: 358\n",
      "2024-11-13 23:58:12,473 - INFO - Processing new PARTICIPANT_ID: 459985, Counter: 359\n",
      "2024-11-13 23:58:31,302 - INFO - Processing new PARTICIPANT_ID: 459987, Counter: 360\n",
      "2024-11-13 23:58:51,006 - INFO - Processing new PARTICIPANT_ID: 459996, Counter: 361\n",
      "2024-11-13 23:59:16,239 - INFO - Processing new PARTICIPANT_ID: 459, Counter: 362\n",
      "2024-11-13 23:59:41,400 - INFO - Processing new PARTICIPANT_ID: 45, Counter: 363\n",
      "2024-11-14 00:00:05,658 - INFO - Processing new PARTICIPANT_ID: 460001, Counter: 364\n",
      "2024-11-14 00:00:33,943 - INFO - Processing new PARTICIPANT_ID: 460002, Counter: 365\n",
      "2024-11-14 00:00:54,089 - INFO - Processing new PARTICIPANT_ID: 460003, Counter: 366\n",
      "2024-11-14 00:01:11,763 - INFO - Processing new PARTICIPANT_ID: 460011, Counter: 367\n",
      "2024-11-14 00:01:32,884 - INFO - Processing new PARTICIPANT_ID: 460014, Counter: 368\n",
      "2024-11-14 00:01:55,058 - INFO - Processing new PARTICIPANT_ID: 460020, Counter: 369\n",
      "2024-11-14 00:02:19,248 - INFO - Processing new PARTICIPANT_ID: 460023, Counter: 370\n",
      "2024-11-14 00:02:40,281 - INFO - Processing new PARTICIPANT_ID: 460024, Counter: 371\n",
      "2024-11-14 00:03:00,724 - INFO - Processing new PARTICIPANT_ID: 460025, Counter: 372\n",
      "2024-11-14 00:03:24,755 - INFO - Processing new PARTICIPANT_ID: 460027, Counter: 373\n",
      "2024-11-14 00:03:44,745 - INFO - Processing new PARTICIPANT_ID: 46002, Counter: 374\n",
      "2024-11-14 00:04:07,393 - INFO - Processing new PARTICIPANT_ID: 460030, Counter: 375\n",
      "2024-11-14 00:04:31,943 - INFO - Processing new PARTICIPANT_ID: 460033, Counter: 376\n",
      "2024-11-14 00:04:53,392 - INFO - Processing new PARTICIPANT_ID: 460038, Counter: 377\n",
      "2024-11-14 00:05:11,403 - INFO - Processing new PARTICIPANT_ID: 460042, Counter: 378\n",
      "2024-11-14 00:05:35,134 - INFO - Processing new PARTICIPANT_ID: 460043, Counter: 379\n",
      "2024-11-14 00:05:56,939 - INFO - Processing new PARTICIPANT_ID: 460045, Counter: 380\n",
      "2024-11-14 00:06:15,905 - INFO - Processing new PARTICIPANT_ID: 46004, Counter: 381\n",
      "2024-11-14 00:06:36,795 - INFO - Processing new PARTICIPANT_ID: 460054, Counter: 382\n",
      "2024-11-14 00:06:58,180 - INFO - Processing new PARTICIPANT_ID: 460055, Counter: 383\n",
      "2024-11-14 00:07:19,077 - INFO - Processing new PARTICIPANT_ID: 460056, Counter: 384\n",
      "2024-11-14 00:07:43,266 - INFO - Processing new PARTICIPANT_ID: 460058, Counter: 385\n",
      "2024-11-14 00:08:03,089 - INFO - Processing new PARTICIPANT_ID: 460067, Counter: 386\n",
      "2024-11-14 00:08:36,425 - INFO - Processing new PARTICIPANT_ID: 460069, Counter: 387\n",
      "2024-11-14 00:08:59,373 - INFO - Processing new PARTICIPANT_ID: 460070, Counter: 388\n",
      "2024-11-14 00:09:21,273 - INFO - Processing new PARTICIPANT_ID: 460074, Counter: 389\n",
      "2024-11-14 00:09:43,104 - INFO - Processing new PARTICIPANT_ID: 460075, Counter: 390\n",
      "2024-11-14 00:10:05,634 - INFO - Processing new PARTICIPANT_ID: 460078, Counter: 391\n",
      "2024-11-14 00:10:28,750 - INFO - Processing new PARTICIPANT_ID: 460080, Counter: 392\n",
      "2024-11-14 00:10:48,191 - INFO - Processing new PARTICIPANT_ID: 460082, Counter: 393\n",
      "2024-11-14 00:11:12,640 - INFO - Processing new PARTICIPANT_ID: 460083, Counter: 394\n",
      "2024-11-14 00:11:34,240 - INFO - Processing new PARTICIPANT_ID: 460084, Counter: 395\n",
      "2024-11-14 00:11:55,895 - INFO - Processing new PARTICIPANT_ID: 460085, Counter: 396\n",
      "2024-11-14 00:12:15,137 - INFO - Processing new PARTICIPANT_ID: 460095, Counter: 397\n",
      "2024-11-14 00:12:42,844 - INFO - Processing new PARTICIPANT_ID: 460097, Counter: 398\n",
      "2024-11-14 00:13:05,413 - INFO - Processing new PARTICIPANT_ID: 460098, Counter: 399\n",
      "2024-11-14 00:13:28,890 - INFO - Processing new PARTICIPANT_ID: 4600, Counter: 400\n",
      "2024-11-14 00:13:59,640 - INFO - Processing new PARTICIPANT_ID: 460101, Counter: 401\n",
      "2024-11-14 00:14:23,565 - INFO - Processing new PARTICIPANT_ID: 460107, Counter: 402\n",
      "2024-11-14 00:14:48,204 - INFO - Processing new PARTICIPANT_ID: 460109, Counter: 403\n",
      "2024-11-14 00:15:13,420 - INFO - Processing new PARTICIPANT_ID: 46010, Counter: 404\n",
      "2024-11-14 00:15:36,981 - INFO - Processing new PARTICIPANT_ID: 460111, Counter: 405\n",
      "2024-11-14 00:16:02,316 - INFO - Processing new PARTICIPANT_ID: 460112, Counter: 406\n",
      "2024-11-14 00:16:30,304 - INFO - Processing new PARTICIPANT_ID: 460114, Counter: 407\n",
      "2024-11-14 00:16:52,041 - INFO - Processing new PARTICIPANT_ID: 460116, Counter: 408\n",
      "2024-11-14 00:17:16,881 - INFO - Processing new PARTICIPANT_ID: 460118, Counter: 409\n",
      "2024-11-14 00:17:42,724 - INFO - Processing new PARTICIPANT_ID: 460119, Counter: 410\n",
      "2024-11-14 00:18:10,309 - INFO - Processing new PARTICIPANT_ID: 46011, Counter: 411\n",
      "2024-11-14 00:18:35,026 - INFO - Processing new PARTICIPANT_ID: 460120, Counter: 412\n",
      "2024-11-14 00:19:03,672 - INFO - Processing new PARTICIPANT_ID: 460121, Counter: 413\n",
      "2024-11-14 00:19:31,856 - INFO - Processing new PARTICIPANT_ID: 460125, Counter: 414\n",
      "2024-11-14 00:20:01,740 - INFO - Processing new PARTICIPANT_ID: 460127, Counter: 415\n",
      "2024-11-14 00:20:31,206 - INFO - Processing new PARTICIPANT_ID: 46012, Counter: 416\n",
      "2024-11-14 00:20:55,553 - INFO - Processing new PARTICIPANT_ID: 460133, Counter: 417\n",
      "2024-11-14 00:21:22,996 - INFO - Processing new PARTICIPANT_ID: 460134, Counter: 418\n",
      "2024-11-14 00:21:43,754 - INFO - Processing new PARTICIPANT_ID: 460137, Counter: 419\n",
      "2024-11-14 00:22:08,406 - INFO - Processing new PARTICIPANT_ID: 46013, Counter: 420\n",
      "2024-11-14 00:22:29,507 - INFO - Processing new PARTICIPANT_ID: 460143, Counter: 421\n",
      "2024-11-14 00:22:48,060 - INFO - Processing new PARTICIPANT_ID: 460146, Counter: 422\n",
      "2024-11-14 00:23:14,750 - INFO - Processing new PARTICIPANT_ID: 460147, Counter: 423\n",
      "2024-11-14 00:23:37,963 - INFO - Processing new PARTICIPANT_ID: 460149, Counter: 424\n",
      "2024-11-14 00:24:01,887 - INFO - Processing new PARTICIPANT_ID: 46014, Counter: 425\n",
      "2024-11-14 00:24:28,695 - INFO - Processing new PARTICIPANT_ID: 460151, Counter: 426\n",
      "2024-11-14 00:24:54,037 - INFO - Processing new PARTICIPANT_ID: 460154, Counter: 427\n",
      "2024-11-14 00:25:21,484 - INFO - Processing new PARTICIPANT_ID: 460159, Counter: 428\n",
      "2024-11-14 00:25:45,487 - INFO - Processing new PARTICIPANT_ID: 460164, Counter: 429\n",
      "2024-11-14 00:26:13,092 - INFO - Processing new PARTICIPANT_ID: 460166, Counter: 430\n",
      "2024-11-14 00:26:36,434 - INFO - Processing new PARTICIPANT_ID: 460168, Counter: 431\n",
      "2024-11-14 00:27:07,985 - INFO - Processing new PARTICIPANT_ID: 460170, Counter: 432\n",
      "2024-11-14 00:27:39,064 - INFO - Processing new PARTICIPANT_ID: 460172, Counter: 433\n",
      "2024-11-14 00:28:05,923 - INFO - Processing new PARTICIPANT_ID: 460175, Counter: 434\n",
      "2024-11-14 00:28:35,804 - INFO - Processing new PARTICIPANT_ID: 460181, Counter: 435\n",
      "2024-11-14 00:29:04,916 - INFO - Processing new PARTICIPANT_ID: 460184, Counter: 436\n",
      "2024-11-14 00:29:37,225 - INFO - Processing new PARTICIPANT_ID: 460186, Counter: 437\n",
      "2024-11-14 00:30:02,550 - INFO - Processing new PARTICIPANT_ID: 460189, Counter: 438\n",
      "2024-11-14 00:30:27,810 - INFO - Processing new PARTICIPANT_ID: 460190, Counter: 439\n",
      "2024-11-14 00:31:00,449 - INFO - Processing new PARTICIPANT_ID: 460192, Counter: 440\n",
      "2024-11-14 00:31:30,219 - INFO - Processing new PARTICIPANT_ID: 460193, Counter: 441\n",
      "2024-11-14 00:31:55,770 - INFO - Processing new PARTICIPANT_ID: 460194, Counter: 442\n",
      "2024-11-14 00:32:25,573 - INFO - Processing new PARTICIPANT_ID: 460196, Counter: 443\n",
      "2024-11-14 00:32:55,658 - INFO - Processing new PARTICIPANT_ID: 460204, Counter: 444\n",
      "2024-11-14 00:33:24,576 - INFO - Processing new PARTICIPANT_ID: 460207, Counter: 445\n",
      "2024-11-14 00:33:48,898 - INFO - Processing new PARTICIPANT_ID: 460208, Counter: 446\n",
      "2024-11-14 00:34:13,142 - INFO - Processing new PARTICIPANT_ID: 460209, Counter: 447\n",
      "2024-11-14 00:34:41,183 - INFO - Processing new PARTICIPANT_ID: 460212, Counter: 448\n",
      "2024-11-14 00:35:10,421 - INFO - Processing new PARTICIPANT_ID: 460214, Counter: 449\n",
      "2024-11-14 00:35:37,409 - INFO - Processing new PARTICIPANT_ID: 460219, Counter: 450\n",
      "2024-11-14 00:35:57,141 - INFO - Processing new PARTICIPANT_ID: 46021, Counter: 451\n",
      "2024-11-14 00:36:18,558 - INFO - Processing new PARTICIPANT_ID: 460225, Counter: 452\n",
      "2024-11-14 00:36:40,937 - INFO - Processing new PARTICIPANT_ID: 460234, Counter: 453\n",
      "2024-11-14 00:37:04,734 - INFO - Processing new PARTICIPANT_ID: 460235, Counter: 454\n",
      "2024-11-14 00:37:32,660 - INFO - Processing new PARTICIPANT_ID: 460236, Counter: 455\n",
      "2024-11-14 00:37:59,072 - INFO - Processing new PARTICIPANT_ID: 46023, Counter: 456\n",
      "2024-11-14 00:38:27,721 - INFO - Processing new PARTICIPANT_ID: 460244, Counter: 457\n",
      "2024-11-14 00:38:53,632 - INFO - Processing new PARTICIPANT_ID: 460247, Counter: 458\n",
      "2024-11-14 00:39:24,610 - INFO - Processing new PARTICIPANT_ID: 460251, Counter: 459\n",
      "2024-11-14 00:39:47,893 - INFO - Processing new PARTICIPANT_ID: 460252, Counter: 460\n",
      "2024-11-14 00:40:16,811 - INFO - Processing new PARTICIPANT_ID: 460256, Counter: 461\n",
      "2024-11-14 00:40:38,454 - INFO - Processing new PARTICIPANT_ID: 460257, Counter: 462\n",
      "2024-11-14 00:41:11,360 - INFO - Processing new PARTICIPANT_ID: 460262, Counter: 463\n",
      "2024-11-14 00:41:38,386 - INFO - Processing new PARTICIPANT_ID: 460263, Counter: 464\n",
      "2024-11-14 00:42:10,934 - INFO - Processing new PARTICIPANT_ID: 460264, Counter: 465\n",
      "2024-11-14 00:42:38,068 - INFO - Processing new PARTICIPANT_ID: 460269, Counter: 466\n",
      "2024-11-14 00:43:12,789 - INFO - Processing new PARTICIPANT_ID: 46026, Counter: 467\n",
      "2024-11-14 00:43:42,694 - INFO - Processing new PARTICIPANT_ID: 460273, Counter: 468\n",
      "2024-11-14 00:44:08,123 - INFO - Processing new PARTICIPANT_ID: 460280, Counter: 469\n",
      "2024-11-14 00:44:35,044 - INFO - Processing new PARTICIPANT_ID: 460281, Counter: 470\n",
      "2024-11-14 00:44:57,628 - INFO - Processing new PARTICIPANT_ID: 460283, Counter: 471\n",
      "2024-11-14 00:45:26,569 - INFO - Processing new PARTICIPANT_ID: 460285, Counter: 472\n",
      "2024-11-14 00:45:56,797 - INFO - Processing new PARTICIPANT_ID: 460287, Counter: 473\n",
      "2024-11-14 00:46:26,015 - INFO - Processing new PARTICIPANT_ID: 460290, Counter: 474\n",
      "2024-11-14 00:46:49,163 - INFO - Processing new PARTICIPANT_ID: 460291, Counter: 475\n",
      "2024-11-14 00:47:16,529 - INFO - Processing new PARTICIPANT_ID: 460295, Counter: 476\n",
      "2024-11-14 00:47:42,948 - INFO - Processing new PARTICIPANT_ID: 460296, Counter: 477\n",
      "2024-11-14 00:48:11,943 - INFO - Processing new PARTICIPANT_ID: 460297, Counter: 478\n",
      "2024-11-14 00:48:42,989 - INFO - Processing new PARTICIPANT_ID: 460298, Counter: 479\n",
      "2024-11-14 00:49:22,289 - INFO - Processing new PARTICIPANT_ID: 460307, Counter: 480\n",
      "2024-11-14 00:49:45,808 - INFO - Processing new PARTICIPANT_ID: 460310, Counter: 481\n",
      "2024-11-14 00:50:14,180 - INFO - Processing new PARTICIPANT_ID: 460311, Counter: 482\n",
      "2024-11-14 00:50:38,622 - INFO - Processing new PARTICIPANT_ID: 460315, Counter: 483\n",
      "2024-11-14 00:51:09,245 - INFO - Processing new PARTICIPANT_ID: 460325, Counter: 484\n",
      "2024-11-14 00:51:40,765 - INFO - Processing new PARTICIPANT_ID: 46032, Counter: 485\n",
      "2024-11-14 00:52:09,225 - INFO - Processing new PARTICIPANT_ID: 460332, Counter: 486\n",
      "2024-11-14 00:52:37,300 - INFO - Processing new PARTICIPANT_ID: 460334, Counter: 487\n",
      "2024-11-14 00:53:06,111 - INFO - Processing new PARTICIPANT_ID: 460341, Counter: 488\n",
      "2024-11-14 00:53:34,212 - INFO - Processing new PARTICIPANT_ID: 460346, Counter: 489\n",
      "2024-11-14 00:54:01,126 - INFO - Processing new PARTICIPANT_ID: 46034, Counter: 490\n",
      "2024-11-14 00:54:32,549 - INFO - Processing new PARTICIPANT_ID: 460353, Counter: 491\n",
      "2024-11-14 00:54:58,510 - INFO - Processing new PARTICIPANT_ID: 460354, Counter: 492\n",
      "2024-11-14 00:55:28,665 - INFO - Processing new PARTICIPANT_ID: 460356, Counter: 493\n",
      "2024-11-14 00:55:56,685 - INFO - Processing new PARTICIPANT_ID: 460358, Counter: 494\n",
      "2024-11-14 00:56:24,668 - INFO - Processing new PARTICIPANT_ID: 460361, Counter: 495\n",
      "2024-11-14 00:56:49,898 - INFO - Processing new PARTICIPANT_ID: 460365, Counter: 496\n",
      "2024-11-14 00:57:16,870 - INFO - Processing new PARTICIPANT_ID: 460366, Counter: 497\n",
      "2024-11-14 00:57:51,149 - INFO - Processing new PARTICIPANT_ID: 460368, Counter: 498\n",
      "2024-11-14 00:58:21,985 - INFO - Processing new PARTICIPANT_ID: 460377, Counter: 499\n",
      "2024-11-14 00:58:56,278 - INFO - Processing new PARTICIPANT_ID: 460378, Counter: 500\n",
      "2024-11-14 00:59:26,501 - INFO - Processing new PARTICIPANT_ID: 46037, Counter: 501\n",
      "2024-11-14 00:59:54,267 - INFO - Processing new PARTICIPANT_ID: 460381, Counter: 502\n",
      "2024-11-14 01:00:22,416 - INFO - Processing new PARTICIPANT_ID: 460382, Counter: 503\n",
      "2024-11-14 01:00:54,392 - INFO - Processing new PARTICIPANT_ID: 460388, Counter: 504\n",
      "2024-11-14 01:01:24,308 - INFO - Processing new PARTICIPANT_ID: 46038, Counter: 505\n",
      "2024-11-14 01:01:53,042 - INFO - Processing new PARTICIPANT_ID: 460391, Counter: 506\n",
      "2024-11-14 01:02:25,742 - INFO - Processing new PARTICIPANT_ID: 460394, Counter: 507\n",
      "2024-11-14 01:02:54,208 - INFO - Processing new PARTICIPANT_ID: 460395, Counter: 508\n",
      "2024-11-14 01:03:26,492 - INFO - Processing new PARTICIPANT_ID: 460397, Counter: 509\n",
      "2024-11-14 01:03:56,417 - INFO - Processing new PARTICIPANT_ID: 460398, Counter: 510\n",
      "2024-11-14 01:04:32,486 - INFO - Processing new PARTICIPANT_ID: 460399, Counter: 511\n",
      "2024-11-14 01:05:00,110 - INFO - Processing new PARTICIPANT_ID: 46039, Counter: 512\n",
      "2024-11-14 01:05:33,823 - INFO - Processing new PARTICIPANT_ID: 460400, Counter: 513\n",
      "2024-11-14 01:05:58,815 - INFO - Processing new PARTICIPANT_ID: 460404, Counter: 514\n",
      "2024-11-14 01:06:33,414 - INFO - Processing new PARTICIPANT_ID: 460406, Counter: 515\n",
      "2024-11-14 01:06:59,569 - INFO - Processing new PARTICIPANT_ID: 46040, Counter: 516\n",
      "2024-11-14 01:07:31,352 - INFO - Processing new PARTICIPANT_ID: 460411, Counter: 517\n",
      "2024-11-14 01:07:54,968 - INFO - Processing new PARTICIPANT_ID: 460412, Counter: 518\n",
      "2024-11-14 01:08:27,458 - INFO - Processing new PARTICIPANT_ID: 460417, Counter: 519\n",
      "2024-11-14 01:08:56,151 - INFO - Processing new PARTICIPANT_ID: 460419, Counter: 520\n",
      "2024-11-14 01:09:28,121 - INFO - Processing new PARTICIPANT_ID: 460421, Counter: 521\n",
      "2024-11-14 01:10:00,276 - INFO - Processing new PARTICIPANT_ID: 460423, Counter: 522\n",
      "2024-11-14 01:10:33,661 - INFO - Processing new PARTICIPANT_ID: 460424, Counter: 523\n",
      "2024-11-14 01:11:01,030 - INFO - Processing new PARTICIPANT_ID: 460429, Counter: 524\n",
      "2024-11-14 01:11:32,762 - INFO - Processing new PARTICIPANT_ID: 460431, Counter: 525\n",
      "2024-11-14 01:11:57,633 - INFO - Processing new PARTICIPANT_ID: 460434, Counter: 526\n",
      "2024-11-14 01:12:32,371 - INFO - Processing new PARTICIPANT_ID: 460435, Counter: 527\n",
      "2024-11-14 01:13:09,384 - INFO - Processing new PARTICIPANT_ID: 46043, Counter: 528\n",
      "2024-11-14 01:13:35,904 - INFO - Processing new PARTICIPANT_ID: 460440, Counter: 529\n",
      "2024-11-14 01:14:12,096 - INFO - Processing new PARTICIPANT_ID: 460441, Counter: 530\n",
      "2024-11-14 01:14:41,269 - INFO - Processing new PARTICIPANT_ID: 460442, Counter: 531\n",
      "2024-11-14 01:15:10,766 - INFO - Processing new PARTICIPANT_ID: 460446, Counter: 532\n",
      "2024-11-14 01:15:44,493 - INFO - Processing new PARTICIPANT_ID: 46044, Counter: 533\n",
      "2024-11-14 01:16:19,049 - INFO - Processing new PARTICIPANT_ID: 460451, Counter: 534\n",
      "2024-11-14 01:16:55,871 - INFO - Processing new PARTICIPANT_ID: 460461, Counter: 535\n",
      "2024-11-14 01:17:28,838 - INFO - Processing new PARTICIPANT_ID: 460462, Counter: 536\n",
      "2024-11-14 01:18:00,953 - INFO - Processing new PARTICIPANT_ID: 460463, Counter: 537\n",
      "2024-11-14 01:18:31,694 - INFO - Processing new PARTICIPANT_ID: 460466, Counter: 538\n",
      "2024-11-14 01:19:07,195 - INFO - Processing new PARTICIPANT_ID: 460467, Counter: 539\n",
      "2024-11-14 01:19:35,661 - INFO - Processing new PARTICIPANT_ID: 460471, Counter: 540\n",
      "2024-11-14 01:20:11,585 - INFO - Processing new PARTICIPANT_ID: 460473, Counter: 541\n",
      "2024-11-14 01:20:50,253 - INFO - Processing new PARTICIPANT_ID: 460478, Counter: 542\n",
      "2024-11-14 01:21:26,594 - INFO - Processing new PARTICIPANT_ID: 460479, Counter: 543\n",
      "2024-11-14 01:22:04,372 - INFO - Processing new PARTICIPANT_ID: 460488, Counter: 544\n",
      "2024-11-14 01:22:38,577 - INFO - Processing new PARTICIPANT_ID: 460489, Counter: 545\n",
      "2024-11-14 01:23:13,831 - INFO - Processing new PARTICIPANT_ID: 46048, Counter: 546\n",
      "2024-11-14 01:23:46,174 - INFO - Processing new PARTICIPANT_ID: 460490, Counter: 547\n",
      "2024-11-14 01:24:12,989 - INFO - Processing new PARTICIPANT_ID: 460501, Counter: 548\n",
      "2024-11-14 01:24:53,911 - INFO - Processing new PARTICIPANT_ID: 460505, Counter: 549\n",
      "2024-11-14 01:25:23,990 - INFO - Processing new PARTICIPANT_ID: 460512, Counter: 550\n",
      "2024-11-14 01:25:57,079 - INFO - Processing new PARTICIPANT_ID: 460522, Counter: 551\n",
      "2024-11-14 01:26:39,022 - INFO - Processing new PARTICIPANT_ID: 460524, Counter: 552\n",
      "2024-11-14 01:27:10,551 - INFO - Processing new PARTICIPANT_ID: 460528, Counter: 553\n",
      "2024-11-14 01:27:44,380 - INFO - Processing new PARTICIPANT_ID: 46052, Counter: 554\n",
      "2024-11-14 01:28:22,451 - INFO - Processing new PARTICIPANT_ID: 460531, Counter: 555\n",
      "2024-11-14 01:28:54,988 - INFO - Processing new PARTICIPANT_ID: 460533, Counter: 556\n",
      "2024-11-14 01:29:23,515 - INFO - Processing new PARTICIPANT_ID: 460535, Counter: 557\n",
      "2024-11-14 01:29:53,162 - INFO - Processing new PARTICIPANT_ID: 460539, Counter: 558\n",
      "2024-11-14 01:30:29,968 - INFO - Processing new PARTICIPANT_ID: 460543, Counter: 559\n",
      "2024-11-14 01:31:04,083 - INFO - Processing new PARTICIPANT_ID: 460544, Counter: 560\n",
      "2024-11-14 01:31:31,601 - INFO - Processing new PARTICIPANT_ID: 460546, Counter: 561\n",
      "2024-11-14 01:32:06,918 - INFO - Processing new PARTICIPANT_ID: 460555, Counter: 562\n",
      "2024-11-14 01:32:41,684 - INFO - Processing new PARTICIPANT_ID: 460556, Counter: 563\n",
      "2024-11-14 01:33:14,558 - INFO - Processing new PARTICIPANT_ID: 460560, Counter: 564\n",
      "2024-11-14 01:33:55,826 - INFO - Processing new PARTICIPANT_ID: 460562, Counter: 565\n",
      "2024-11-14 01:34:25,033 - INFO - Processing new PARTICIPANT_ID: 460566, Counter: 566\n",
      "2024-11-14 01:34:58,387 - INFO - Processing new PARTICIPANT_ID: 460567, Counter: 567\n",
      "2024-11-14 01:35:26,243 - INFO - Processing new PARTICIPANT_ID: 460572, Counter: 568\n",
      "2024-11-14 01:36:02,752 - INFO - Processing new PARTICIPANT_ID: 460576, Counter: 569\n",
      "2024-11-14 01:36:38,432 - INFO - Processing new PARTICIPANT_ID: 460578, Counter: 570\n",
      "2024-11-14 01:37:16,398 - INFO - Processing new PARTICIPANT_ID: 460579, Counter: 571\n",
      "2024-11-14 01:37:48,998 - INFO - Processing new PARTICIPANT_ID: 460580, Counter: 572\n",
      "2024-11-14 01:38:21,682 - INFO - Processing new PARTICIPANT_ID: 460587, Counter: 573\n",
      "2024-11-14 01:38:48,876 - INFO - Processing new PARTICIPANT_ID: 460591, Counter: 574\n",
      "2024-11-14 01:39:21,947 - INFO - Processing new PARTICIPANT_ID: 460593, Counter: 575\n",
      "2024-11-14 01:40:00,492 - INFO - Processing new PARTICIPANT_ID: 4605, Counter: 576\n",
      "2024-11-14 01:40:35,643 - INFO - Processing new PARTICIPANT_ID: 460600, Counter: 577\n",
      "2024-11-14 01:41:09,180 - INFO - Processing new PARTICIPANT_ID: 460601, Counter: 578\n",
      "2024-11-14 01:41:37,537 - INFO - Processing new PARTICIPANT_ID: 460611, Counter: 579\n",
      "2024-11-14 01:42:13,413 - INFO - Processing new PARTICIPANT_ID: 460612, Counter: 580\n",
      "2024-11-14 01:42:42,854 - INFO - Processing new PARTICIPANT_ID: 460621, Counter: 581\n",
      "2024-11-14 01:43:19,335 - INFO - Processing new PARTICIPANT_ID: 460625, Counter: 582\n",
      "2024-11-14 01:43:53,057 - INFO - Processing new PARTICIPANT_ID: 460627, Counter: 583\n",
      "2024-11-14 01:44:26,063 - INFO - Processing new PARTICIPANT_ID: 460631, Counter: 584\n",
      "2024-11-14 01:44:55,381 - INFO - Processing new PARTICIPANT_ID: 460635, Counter: 585\n",
      "2024-11-14 01:45:27,403 - INFO - Processing new PARTICIPANT_ID: 460638, Counter: 586\n",
      "2024-11-14 01:46:02,628 - INFO - Processing new PARTICIPANT_ID: 460639, Counter: 587\n",
      "2024-11-14 01:46:42,260 - INFO - Processing new PARTICIPANT_ID: 460641, Counter: 588\n",
      "2024-11-14 01:47:15,738 - INFO - Processing new PARTICIPANT_ID: 460643, Counter: 589\n",
      "2024-11-14 01:47:43,290 - INFO - Processing new PARTICIPANT_ID: 460648, Counter: 590\n",
      "2024-11-14 01:48:21,139 - INFO - Processing new PARTICIPANT_ID: 460651, Counter: 591\n",
      "2024-11-14 01:48:48,282 - INFO - Processing new PARTICIPANT_ID: 460654, Counter: 592\n",
      "2024-11-14 01:49:31,525 - INFO - Processing new PARTICIPANT_ID: 460659, Counter: 593\n",
      "2024-11-14 01:50:09,337 - INFO - Processing new PARTICIPANT_ID: 460665, Counter: 594\n",
      "2024-11-14 01:50:49,075 - INFO - Processing new PARTICIPANT_ID: 460666, Counter: 595\n",
      "2024-11-14 01:51:21,041 - INFO - Processing new PARTICIPANT_ID: 460668, Counter: 596\n",
      "2024-11-14 01:51:51,984 - INFO - Processing new PARTICIPANT_ID: 460670, Counter: 597\n",
      "2024-11-14 01:52:32,036 - INFO - Processing new PARTICIPANT_ID: 460671, Counter: 598\n",
      "2024-11-14 01:52:59,659 - INFO - Processing new PARTICIPANT_ID: 460677, Counter: 599\n",
      "2024-11-14 01:53:29,897 - INFO - Processing new PARTICIPANT_ID: 460680, Counter: 600\n",
      "2024-11-14 01:54:08,304 - INFO - Processing new PARTICIPANT_ID: 460681, Counter: 601\n",
      "2024-11-14 01:54:45,288 - INFO - Processing new PARTICIPANT_ID: 460684, Counter: 602\n",
      "2024-11-14 01:55:29,401 - INFO - Processing new PARTICIPANT_ID: 460685, Counter: 603\n",
      "2024-11-14 01:56:03,246 - INFO - Processing new PARTICIPANT_ID: 460686, Counter: 604\n",
      "2024-11-14 01:56:46,047 - INFO - Processing new PARTICIPANT_ID: 460688, Counter: 605\n",
      "2024-11-14 01:57:15,209 - INFO - Processing new PARTICIPANT_ID: 460693, Counter: 606\n",
      "2024-11-14 01:57:52,410 - INFO - Processing new PARTICIPANT_ID: 460697, Counter: 607\n",
      "2024-11-14 01:58:18,399 - INFO - Processing new PARTICIPANT_ID: 46069, Counter: 608\n",
      "2024-11-14 01:58:55,467 - INFO - Processing new PARTICIPANT_ID: 460702, Counter: 609\n",
      "2024-11-14 01:59:27,541 - INFO - Processing new PARTICIPANT_ID: 460705, Counter: 610\n",
      "2024-11-14 01:59:57,701 - INFO - Processing new PARTICIPANT_ID: 460708, Counter: 611\n",
      "2024-11-14 02:00:36,906 - INFO - Processing new PARTICIPANT_ID: 460711, Counter: 612\n",
      "2024-11-14 02:01:08,505 - INFO - Processing new PARTICIPANT_ID: 460712, Counter: 613\n",
      "2024-11-14 02:01:40,670 - INFO - Processing new PARTICIPANT_ID: 460720, Counter: 614\n",
      "2024-11-14 02:02:11,636 - INFO - Processing new PARTICIPANT_ID: 460721, Counter: 615\n",
      "2024-11-14 02:03:00,405 - INFO - Processing new PARTICIPANT_ID: 460725, Counter: 616\n",
      "2024-11-14 02:03:34,448 - INFO - Processing new PARTICIPANT_ID: 460727, Counter: 617\n",
      "2024-11-14 02:04:09,526 - INFO - Processing new PARTICIPANT_ID: 460732, Counter: 618\n",
      "2024-11-14 02:04:41,982 - INFO - Processing new PARTICIPANT_ID: 460733, Counter: 619\n",
      "2024-11-14 02:05:13,274 - INFO - Processing new PARTICIPANT_ID: 460734, Counter: 620\n",
      "2024-11-14 02:05:52,615 - INFO - Processing new PARTICIPANT_ID: 460735, Counter: 621\n",
      "2024-11-14 02:06:33,324 - INFO - Processing new PARTICIPANT_ID: 460739, Counter: 622\n",
      "2024-11-14 02:07:04,038 - INFO - Processing new PARTICIPANT_ID: 460740, Counter: 623\n",
      "2024-11-14 02:07:35,920 - INFO - Processing new PARTICIPANT_ID: 460741, Counter: 624\n",
      "2024-11-14 02:08:16,548 - INFO - Processing new PARTICIPANT_ID: 460747, Counter: 625\n",
      "2024-11-14 02:09:01,712 - INFO - Processing new PARTICIPANT_ID: 460749, Counter: 626\n",
      "2024-11-14 02:09:37,187 - INFO - Processing new PARTICIPANT_ID: 460750, Counter: 627\n",
      "2024-11-14 02:10:09,874 - INFO - Processing new PARTICIPANT_ID: 460755, Counter: 628\n",
      "2024-11-14 02:10:39,630 - INFO - Processing new PARTICIPANT_ID: 460756, Counter: 629\n",
      "2024-11-14 02:11:21,554 - INFO - Processing new PARTICIPANT_ID: 46075, Counter: 630\n",
      "2024-11-14 02:11:54,578 - INFO - Processing new PARTICIPANT_ID: 460766, Counter: 631\n",
      "2024-11-14 02:12:41,305 - INFO - Processing new PARTICIPANT_ID: 46076, Counter: 632\n",
      "2024-11-14 02:13:18,904 - INFO - Processing new PARTICIPANT_ID: 460772, Counter: 633\n",
      "2024-11-14 02:13:59,186 - INFO - Processing new PARTICIPANT_ID: 460775, Counter: 634\n",
      "2024-11-14 02:14:37,660 - INFO - Processing new PARTICIPANT_ID: 460776, Counter: 635\n",
      "2024-11-14 02:15:14,815 - INFO - Processing new PARTICIPANT_ID: 460777, Counter: 636\n",
      "2024-11-14 02:15:53,150 - INFO - Processing new PARTICIPANT_ID: 46077, Counter: 637\n",
      "2024-11-14 02:16:37,482 - INFO - Processing new PARTICIPANT_ID: 460780, Counter: 638\n",
      "2024-11-14 02:17:14,153 - INFO - Processing new PARTICIPANT_ID: 460782, Counter: 639\n",
      "2024-11-14 02:17:47,324 - INFO - Processing new PARTICIPANT_ID: 460785, Counter: 640\n",
      "2024-11-14 02:18:19,653 - INFO - Processing new PARTICIPANT_ID: 460786, Counter: 641\n",
      "2024-11-14 02:19:02,069 - INFO - Processing new PARTICIPANT_ID: 460790, Counter: 642\n",
      "2024-11-14 02:19:35,135 - INFO - Processing new PARTICIPANT_ID: 460791, Counter: 643\n",
      "2024-11-14 02:20:07,851 - INFO - Processing new PARTICIPANT_ID: 460793, Counter: 644\n",
      "2024-11-14 02:20:48,756 - INFO - Processing new PARTICIPANT_ID: 460796, Counter: 645\n",
      "2024-11-14 02:21:16,627 - INFO - Processing new PARTICIPANT_ID: 460797, Counter: 646\n",
      "2024-11-14 02:21:53,614 - INFO - Processing new PARTICIPANT_ID: 460798, Counter: 647\n",
      "2024-11-14 02:22:28,912 - INFO - Processing new PARTICIPANT_ID: 460799, Counter: 648\n",
      "2024-11-14 02:23:15,968 - INFO - Processing new PARTICIPANT_ID: 46079, Counter: 649\n",
      "2024-11-14 02:23:53,068 - INFO - Processing new PARTICIPANT_ID: 460803, Counter: 650\n",
      "2024-11-14 02:24:32,798 - INFO - Processing new PARTICIPANT_ID: 460805, Counter: 651\n",
      "2024-11-14 02:25:06,366 - INFO - Processing new PARTICIPANT_ID: 460807, Counter: 652\n",
      "2024-11-14 02:25:41,483 - INFO - Processing new PARTICIPANT_ID: 460808, Counter: 653\n",
      "2024-11-14 02:26:13,432 - INFO - Processing new PARTICIPANT_ID: 460809, Counter: 654\n",
      "2024-11-14 02:26:56,369 - INFO - Processing new PARTICIPANT_ID: 460812, Counter: 655\n",
      "2024-11-14 02:27:37,543 - INFO - Processing new PARTICIPANT_ID: 460815, Counter: 656\n",
      "2024-11-14 02:28:10,267 - INFO - Processing new PARTICIPANT_ID: 460816, Counter: 657\n",
      "2024-11-14 02:28:48,693 - INFO - Processing new PARTICIPANT_ID: 460821, Counter: 658\n",
      "2024-11-14 02:29:24,902 - INFO - Processing new PARTICIPANT_ID: 460822, Counter: 659\n",
      "2024-11-14 02:30:08,363 - INFO - Processing new PARTICIPANT_ID: 460823, Counter: 660\n",
      "2024-11-14 02:30:44,658 - INFO - Processing new PARTICIPANT_ID: 460824, Counter: 661\n",
      "2024-11-14 02:31:25,624 - INFO - Processing new PARTICIPANT_ID: 460827, Counter: 662\n",
      "2024-11-14 02:32:02,434 - INFO - Processing new PARTICIPANT_ID: 460839, Counter: 663\n",
      "2024-11-14 02:32:29,945 - INFO - Processing new PARTICIPANT_ID: 46083, Counter: 664\n",
      "2024-11-14 02:33:06,266 - INFO - Processing new PARTICIPANT_ID: 460842, Counter: 665\n",
      "2024-11-14 02:33:46,010 - INFO - Processing new PARTICIPANT_ID: 460847, Counter: 666\n",
      "2024-11-14 02:34:33,364 - INFO - Processing new PARTICIPANT_ID: 460851, Counter: 667\n",
      "2024-11-14 02:35:16,230 - INFO - Processing new PARTICIPANT_ID: 460853, Counter: 668\n",
      "2024-11-14 02:36:04,042 - INFO - Processing new PARTICIPANT_ID: 460857, Counter: 669\n",
      "2024-11-14 02:36:42,051 - INFO - Processing new PARTICIPANT_ID: 460858, Counter: 670\n",
      "2024-11-14 02:37:12,115 - INFO - Processing new PARTICIPANT_ID: 460860, Counter: 671\n",
      "2024-11-14 02:37:56,485 - INFO - Processing new PARTICIPANT_ID: 460861, Counter: 672\n",
      "2024-11-14 02:38:38,041 - INFO - Processing new PARTICIPANT_ID: 460865, Counter: 673\n",
      "2024-11-14 02:39:08,184 - INFO - Processing new PARTICIPANT_ID: 460870, Counter: 674\n",
      "2024-11-14 02:39:45,350 - INFO - Processing new PARTICIPANT_ID: 460877, Counter: 675\n",
      "2024-11-14 02:40:26,250 - INFO - Processing new PARTICIPANT_ID: 460879, Counter: 676\n",
      "2024-11-14 02:41:05,518 - INFO - Processing new PARTICIPANT_ID: 460880, Counter: 677\n",
      "2024-11-14 02:41:37,208 - INFO - Processing new PARTICIPANT_ID: 460881, Counter: 678\n",
      "2024-11-14 02:42:12,560 - INFO - Processing new PARTICIPANT_ID: 460886, Counter: 679\n",
      "2024-11-14 02:42:52,365 - INFO - Processing new PARTICIPANT_ID: 460889, Counter: 680\n",
      "2024-11-14 02:43:26,816 - INFO - Processing new PARTICIPANT_ID: 460896, Counter: 681\n",
      "2024-11-14 02:44:06,168 - INFO - Processing new PARTICIPANT_ID: 460898, Counter: 682\n",
      "2024-11-14 02:44:42,313 - INFO - Processing new PARTICIPANT_ID: 46089, Counter: 683\n",
      "2024-11-14 02:45:18,086 - INFO - Processing new PARTICIPANT_ID: 460906, Counter: 684\n",
      "2024-11-14 02:45:57,956 - INFO - Processing new PARTICIPANT_ID: 460911, Counter: 685\n",
      "2024-11-14 02:46:41,191 - INFO - Processing new PARTICIPANT_ID: 460914, Counter: 686\n",
      "2024-11-14 02:47:24,287 - INFO - Processing new PARTICIPANT_ID: 460916, Counter: 687\n",
      "2024-11-14 02:48:04,863 - INFO - Processing new PARTICIPANT_ID: 460919, Counter: 688\n",
      "2024-11-14 02:48:39,999 - INFO - Processing new PARTICIPANT_ID: 460926, Counter: 689\n",
      "2024-11-14 02:49:32,648 - INFO - Processing new PARTICIPANT_ID: 460927, Counter: 690\n",
      "2024-11-14 02:50:13,145 - INFO - Processing new PARTICIPANT_ID: 46092, Counter: 691\n",
      "2024-11-14 02:50:55,337 - INFO - Processing new PARTICIPANT_ID: 460938, Counter: 692\n",
      "2024-11-14 02:51:31,079 - INFO - Processing new PARTICIPANT_ID: 460949, Counter: 693\n",
      "2024-11-14 02:52:03,664 - INFO - Processing new PARTICIPANT_ID: 460953, Counter: 694\n",
      "2024-11-14 02:52:41,142 - INFO - Processing new PARTICIPANT_ID: 460955, Counter: 695\n",
      "2024-11-14 02:53:25,791 - INFO - Processing new PARTICIPANT_ID: 460956, Counter: 696\n",
      "2024-11-14 02:54:03,991 - INFO - Processing new PARTICIPANT_ID: 460958, Counter: 697\n",
      "2024-11-14 02:54:37,839 - INFO - Processing new PARTICIPANT_ID: 460960, Counter: 698\n",
      "2024-11-14 02:55:10,817 - INFO - Processing new PARTICIPANT_ID: 460964, Counter: 699\n",
      "2024-11-14 02:55:44,127 - INFO - Processing new PARTICIPANT_ID: 460971, Counter: 700\n",
      "2024-11-14 02:56:23,682 - INFO - Processing new PARTICIPANT_ID: 460978, Counter: 701\n",
      "2024-11-14 02:57:18,090 - INFO - Processing new PARTICIPANT_ID: 460979, Counter: 702\n",
      "2024-11-14 02:58:02,085 - INFO - Processing new PARTICIPANT_ID: 460980, Counter: 703\n",
      "2024-11-14 02:58:41,769 - INFO - Processing new PARTICIPANT_ID: 460982, Counter: 704\n",
      "2024-11-14 02:59:12,700 - INFO - Processing new PARTICIPANT_ID: 460987, Counter: 705\n",
      "2024-11-14 02:59:47,967 - INFO - Processing new PARTICIPANT_ID: 460989, Counter: 706\n",
      "2024-11-14 03:00:30,798 - INFO - Processing new PARTICIPANT_ID: 460990, Counter: 707\n",
      "2024-11-14 03:01:09,489 - INFO - Processing new PARTICIPANT_ID: 460992, Counter: 708\n",
      "2024-11-14 03:01:53,625 - INFO - Processing new PARTICIPANT_ID: 460993, Counter: 709\n",
      "2024-11-14 03:02:33,713 - INFO - Processing new PARTICIPANT_ID: 460994, Counter: 710\n",
      "2024-11-14 03:03:18,843 - INFO - Processing new PARTICIPANT_ID: 460996, Counter: 711\n",
      "2024-11-14 03:03:54,682 - INFO - Processing new PARTICIPANT_ID: 4609, Counter: 712\n",
      "2024-11-14 03:04:36,708 - INFO - Processing new PARTICIPANT_ID: 460, Counter: 713\n",
      "2024-11-14 03:05:22,992 - INFO - Processing new PARTICIPANT_ID: 461002, Counter: 714\n",
      "2024-11-14 03:06:05,489 - INFO - Processing new PARTICIPANT_ID: 461006, Counter: 715\n",
      "2024-11-14 03:06:39,431 - INFO - Processing new PARTICIPANT_ID: 461007, Counter: 716\n",
      "2024-11-14 03:07:24,014 - INFO - Processing new PARTICIPANT_ID: 461008, Counter: 717\n",
      "2024-11-14 03:08:05,867 - INFO - Processing new PARTICIPANT_ID: 461009, Counter: 718\n",
      "2024-11-14 03:08:45,632 - INFO - Processing new PARTICIPANT_ID: 46100, Counter: 719\n",
      "2024-11-14 03:09:23,924 - INFO - Processing new PARTICIPANT_ID: 461011, Counter: 720\n",
      "2024-11-14 03:10:13,096 - INFO - Processing new PARTICIPANT_ID: 461012, Counter: 721\n",
      "2024-11-14 03:10:50,372 - INFO - Processing new PARTICIPANT_ID: 461020, Counter: 722\n",
      "2024-11-14 03:11:23,507 - INFO - Processing new PARTICIPANT_ID: 461022, Counter: 723\n",
      "2024-11-14 03:11:59,875 - INFO - Processing new PARTICIPANT_ID: 461023, Counter: 724\n",
      "2024-11-14 03:12:44,144 - INFO - Processing new PARTICIPANT_ID: 461029, Counter: 725\n",
      "2024-11-14 03:13:19,392 - INFO - Processing new PARTICIPANT_ID: 46102, Counter: 726\n",
      "2024-11-14 03:14:02,115 - INFO - Processing new PARTICIPANT_ID: 461034, Counter: 727\n",
      "2024-11-14 03:14:40,995 - INFO - Processing new PARTICIPANT_ID: 461037, Counter: 728\n",
      "2024-11-14 03:15:29,438 - INFO - Processing new PARTICIPANT_ID: 46103, Counter: 729\n",
      "2024-11-14 03:15:59,463 - INFO - Processing new PARTICIPANT_ID: 461049, Counter: 730\n",
      "2024-11-14 03:16:39,077 - INFO - Processing new PARTICIPANT_ID: 461055, Counter: 731\n",
      "2024-11-14 03:17:13,825 - INFO - Processing new PARTICIPANT_ID: 461074, Counter: 732\n",
      "2024-11-14 03:17:47,192 - INFO - Processing new PARTICIPANT_ID: 461082, Counter: 733\n",
      "2024-11-14 03:18:17,838 - INFO - Processing new PARTICIPANT_ID: 461086, Counter: 734\n",
      "2024-11-14 03:18:51,969 - INFO - Processing new PARTICIPANT_ID: 461093, Counter: 735\n",
      "2024-11-14 03:19:28,802 - INFO - Processing new PARTICIPANT_ID: 461097, Counter: 736\n",
      "2024-11-14 03:20:01,402 - INFO - Processing new PARTICIPANT_ID: 4610, Counter: 737\n",
      "2024-11-14 03:20:40,654 - INFO - Processing new PARTICIPANT_ID: 461102, Counter: 738\n",
      "2024-11-14 03:21:15,369 - INFO - Processing new PARTICIPANT_ID: 461104, Counter: 739\n",
      "2024-11-14 03:21:58,479 - INFO - Processing new PARTICIPANT_ID: 461105, Counter: 740\n",
      "2024-11-14 03:22:35,502 - INFO - Processing new PARTICIPANT_ID: 46110, Counter: 741\n",
      "2024-11-14 03:23:18,182 - INFO - Processing new PARTICIPANT_ID: 461110, Counter: 742\n",
      "2024-11-14 03:23:52,714 - INFO - Processing new PARTICIPANT_ID: 461123, Counter: 743\n",
      "2024-11-14 03:24:40,859 - INFO - Processing new PARTICIPANT_ID: 461160, Counter: 744\n",
      "2024-11-14 03:25:30,980 - INFO - Processing new PARTICIPANT_ID: 461168, Counter: 745\n",
      "2024-11-14 03:26:14,821 - INFO - Processing new PARTICIPANT_ID: 46116, Counter: 746\n",
      "2024-11-14 03:26:53,179 - INFO - Processing new PARTICIPANT_ID: 461182, Counter: 747\n",
      "2024-11-14 03:27:32,117 - INFO - Processing new PARTICIPANT_ID: 461184, Counter: 748\n",
      "2024-11-14 03:28:16,985 - INFO - Processing new PARTICIPANT_ID: 461186, Counter: 749\n",
      "2024-11-14 03:29:07,814 - INFO - Processing new PARTICIPANT_ID: 461188, Counter: 750\n",
      "2024-11-14 03:29:43,212 - INFO - Processing new PARTICIPANT_ID: 46118, Counter: 751\n",
      "2024-11-14 03:30:25,886 - INFO - Processing new PARTICIPANT_ID: 461195, Counter: 752\n",
      "2024-11-14 03:31:22,215 - INFO - Processing new PARTICIPANT_ID: 461199, Counter: 753\n",
      "2024-11-14 03:31:58,894 - INFO - Processing new PARTICIPANT_ID: 4611, Counter: 754\n",
      "2024-11-14 03:32:35,600 - INFO - Processing new PARTICIPANT_ID: 461200, Counter: 755\n",
      "2024-11-14 03:33:13,540 - INFO - Processing new PARTICIPANT_ID: 461209, Counter: 756\n",
      "2024-11-14 03:33:53,236 - INFO - Processing new PARTICIPANT_ID: 46120, Counter: 757\n",
      "2024-11-14 03:34:43,553 - INFO - Processing new PARTICIPANT_ID: 461212, Counter: 758\n",
      "2024-11-14 03:35:18,173 - INFO - Processing new PARTICIPANT_ID: 461216, Counter: 759\n",
      "2024-11-14 03:35:56,976 - INFO - Processing new PARTICIPANT_ID: 461217, Counter: 760\n",
      "2024-11-14 03:36:29,760 - INFO - Processing new PARTICIPANT_ID: 461220, Counter: 761\n",
      "2024-11-14 03:37:10,305 - INFO - Processing new PARTICIPANT_ID: 461229, Counter: 762\n",
      "2024-11-14 03:37:46,698 - INFO - Processing new PARTICIPANT_ID: 46122, Counter: 763\n",
      "2024-11-14 03:38:21,603 - INFO - Processing new PARTICIPANT_ID: 461232, Counter: 764\n",
      "2024-11-14 03:38:57,016 - INFO - Processing new PARTICIPANT_ID: 461234, Counter: 765\n",
      "2024-11-14 03:39:37,443 - INFO - Processing new PARTICIPANT_ID: 46123, Counter: 766\n",
      "2024-11-14 03:40:20,187 - INFO - Processing new PARTICIPANT_ID: 461240, Counter: 767\n",
      "2024-11-14 03:41:01,133 - INFO - Processing new PARTICIPANT_ID: 461241, Counter: 768\n",
      "2024-11-14 03:41:37,175 - INFO - Processing new PARTICIPANT_ID: 461243, Counter: 769\n",
      "2024-11-14 03:42:25,929 - INFO - Processing new PARTICIPANT_ID: 461247, Counter: 770\n",
      "2024-11-14 03:43:23,140 - INFO - Processing new PARTICIPANT_ID: 461249, Counter: 771\n",
      "2024-11-14 03:44:06,280 - INFO - Processing new PARTICIPANT_ID: 461251, Counter: 772\n",
      "2024-11-14 03:44:55,066 - INFO - Processing new PARTICIPANT_ID: 461252, Counter: 773\n",
      "2024-11-14 03:45:34,817 - INFO - Processing new PARTICIPANT_ID: 461255, Counter: 774\n",
      "2024-11-14 03:46:17,425 - INFO - Processing new PARTICIPANT_ID: 461259, Counter: 775\n",
      "2024-11-14 03:46:58,815 - INFO - Processing new PARTICIPANT_ID: 461272, Counter: 776\n",
      "2024-11-14 03:47:36,563 - INFO - Processing new PARTICIPANT_ID: 461275, Counter: 777\n",
      "2024-11-14 03:48:20,135 - INFO - Processing new PARTICIPANT_ID: 461279, Counter: 778\n",
      "2024-11-14 03:49:02,888 - INFO - Processing new PARTICIPANT_ID: 46127, Counter: 779\n",
      "2024-11-14 03:49:49,556 - INFO - Processing new PARTICIPANT_ID: 461284, Counter: 780\n",
      "2024-11-14 03:50:24,263 - INFO - Processing new PARTICIPANT_ID: 461288, Counter: 781\n",
      "2024-11-14 03:51:15,628 - INFO - Processing new PARTICIPANT_ID: 461289, Counter: 782\n",
      "2024-11-14 03:51:50,812 - INFO - Processing new PARTICIPANT_ID: 46128, Counter: 783\n",
      "2024-11-14 03:52:29,493 - INFO - Processing new PARTICIPANT_ID: 461290, Counter: 784\n",
      "2024-11-14 03:53:15,168 - INFO - Processing new PARTICIPANT_ID: 461291, Counter: 785\n",
      "2024-11-14 03:53:55,123 - INFO - Processing new PARTICIPANT_ID: 461292, Counter: 786\n",
      "2024-11-14 03:54:45,589 - INFO - Processing new PARTICIPANT_ID: 461299, Counter: 787\n",
      "2024-11-14 03:55:21,415 - INFO - Processing new PARTICIPANT_ID: 461303, Counter: 788\n",
      "2024-11-14 03:56:08,744 - INFO - Processing new PARTICIPANT_ID: 461304, Counter: 789\n",
      "2024-11-14 03:56:52,725 - INFO - Processing new PARTICIPANT_ID: 46130, Counter: 790\n",
      "2024-11-14 03:57:39,727 - INFO - Processing new PARTICIPANT_ID: 461313, Counter: 791\n",
      "2024-11-14 03:58:27,518 - INFO - Processing new PARTICIPANT_ID: 461317, Counter: 792\n",
      "2024-11-14 03:59:11,345 - INFO - Processing new PARTICIPANT_ID: 461319, Counter: 793\n",
      "2024-11-14 03:59:48,628 - INFO - Processing new PARTICIPANT_ID: 461321, Counter: 794\n",
      "2024-11-14 04:00:35,115 - INFO - Processing new PARTICIPANT_ID: 461322, Counter: 795\n",
      "2024-11-14 04:01:15,206 - INFO - Processing new PARTICIPANT_ID: 461339, Counter: 796\n",
      "2024-11-14 04:02:03,498 - INFO - Processing new PARTICIPANT_ID: 461346, Counter: 797\n",
      "2024-11-14 04:02:47,069 - INFO - Processing new PARTICIPANT_ID: 461353, Counter: 798\n",
      "2024-11-14 04:03:21,938 - INFO - Processing new PARTICIPANT_ID: 461357, Counter: 799\n",
      "2024-11-14 04:04:09,834 - INFO - Updated data saved to demographics_csv/added800_14_key_values.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/reduced800_14_participants.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/added800_14_key_values.csv'  # Replace with your desired output CSV path\n",
    "add_new_columns(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T03:04:09.935156Z",
     "start_time": "2024-11-13T21:41:14.390621Z"
    }
   },
   "id": "676cd861e72aae5",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to demographics_csv/enhanced_keystroke_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_keystroke_features(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Adds new keystroke features to the CSV file and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # # Define left and right hand keys using their numeric KEYCODE values\n",
    "    # left_hand_keys = {16, 65, 68, 69, 70, 71, 83, 87, 88, 90}  # Example keycodes for left hand\n",
    "    # right_hand_keys = {32, 73, 78, 79, 82, 85, 89}  # Example keycodes for right hand\n",
    "\n",
    "    left_hand_keys = {16, 65, 66, 67, 68, 69, 70, 71, 81, 82, 83, 84, 86, 87, 88, 90, 49, 50, 51, 52, 53, 9, 20,\n",
    "                      190}  # Keycodes for left hand\n",
    "    right_hand_keys = {16, 32, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 89, 48, 54, 55, 56, 57, 8, 13, 189, 191, 188,\n",
    "                       191}  # Keycodes for right hand\n",
    "\n",
    "    # left = 0, right = 1\n",
    "    # Function to determine hand\n",
    "    def determine_hand(key):\n",
    "        if key in left_hand_keys:\n",
    "            return '0'\n",
    "        elif key in right_hand_keys:\n",
    "            return '1'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "\n",
    "    # Apply the function to determine hand for each key\n",
    "    df['HAND'] = df['KEYCODE'].apply(determine_hand)\n",
    "\n",
    "    # Calculate mean hold time for each hand\n",
    "    hand_hold_time = df.groupby('HAND')['D1U1'].mean().to_dict()\n",
    "\n",
    "    # Map the mean hold time back to the DataFrame\n",
    "    df['HAND_HOLD_TIME'] = df['HAND'].map(hand_hold_time)\n",
    "\n",
    "    # Calculate Keystroke Duration Variability\n",
    "    df['KEY_HOLD_TIME_STD'] = df.groupby('KEYCODE')['D1U1'].transform('std')\n",
    "\n",
    "    # Calculate Error Rate and Correction Features\n",
    "    df['ERROR_RATE'] = df['KEYCODE'].apply(lambda x: 1 if x == 8 else 0)  # Assuming 8 is the keycode for backspace\n",
    "    df['ERROR_RATE'] = df['ERROR_RATE'].cumsum() / (df.index + 1)\n",
    "\n",
    "    # Calculate Consecutive Key Patterns\n",
    "    df['CONSECUTIVE_KEYS'] = df['KEYCODE'].astype(str) + df['KEYCODE'].shift(-1).astype(str)\n",
    "    df['CONSECUTIVE_KEYS_TIME'] = df['D1U1'].shift(-1) - df['D1D2']\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/added_key_values.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with your desired output CSV path\n",
    "add_keystroke_features(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T00:19:37.450442Z",
     "start_time": "2024-11-11T00:19:34.022530Z"
    }
   },
   "id": "902c00b760b4679c",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_4664\\1669945238.py:26: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  drop_unknown_hand(input_csv_path, output_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to demographics_csv/enhanced_keystroke_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def drop_unknown_hand(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Drops all rows where the value in the HAND column is 'unknown' and saves the updated data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save updated data.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Drop rows where the HAND column value is 'unknown'\n",
    "    df = df[df['HAND'] != 'unknown']\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Updated data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with your actual input CSV path\n",
    "output_csv_path = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with your desired output CSV path\n",
    "drop_unknown_hand(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T00:21:26.979991Z",
     "start_time": "2024-11-11T00:21:20.685062Z"
    }
   },
   "id": "a6c2d0d3f0b8adbc",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_4664\\2627201243.py:34: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  add_speed_classification(input_file, output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed classification added. File saved to: demographics_csv/enhanced_keystroke_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_speed_classification(file_path, output_file):\n",
    "    \"\"\"\n",
    "    This function adds a new feature 'Speed_Class' to classify participants based on their D1D2_MEAN values.\n",
    "    Participants are classified from 1 (slowest) to 10 (fastest) based on percentiles of D1D2_MEAN.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the input CSV file.\n",
    "    - output_file (str): The path where the output CSV file with the new feature will be saved.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure D1D2_MEAN column exists and contains valid numeric data\n",
    "    if 'D1D2_MEAN' not in df.columns or not pd.api.types.is_numeric_dtype(df['D1D2_MEAN']):\n",
    "        raise ValueError(\"The input CSV file must contain a numeric 'D1D2_MEAN' column.\")\n",
    "\n",
    "    # Calculate the deciles based on D1D2_MEAN\n",
    "    df['SPEED_CLASS'] = pd.qcut(df['D1D2_MEAN'], q=10, labels=False, duplicates='drop') + 1\n",
    "\n",
    "    # Invert the speed class to assign 10 to the fastest (lowest D1D2_MEAN values) and 1 to the slowest\n",
    "    df['SPEED_CLASS'] = 11 - df['SPEED_CLASS']\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Speed classification added. File saved to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with the path to your CSV file\n",
    "output_file = 'demographics_csv/enhanced_keystroke_features.csv'  # Replace with the path where you want to save the new CSV file\n",
    "\n",
    "add_speed_classification(input_file, output_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T00:24:51.633003Z",
     "start_time": "2024-11-11T00:24:45.234580Z"
    }
   },
   "id": "5d9eee820951ffaf",
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T17:22:47.344986Z",
     "start_time": "2024-09-10T17:22:47.316024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # this might erase outliers where values are too slow or fast\n",
    "# # to be altered and used if needed, TODO: columns to check must be altered\n",
    "# import pandas as pd\n",
    "# \n",
    "# def remove_outliers(input_csv, output_csv):\n",
    "#     \"\"\"\n",
    "#     Removes rows with extreme outliers (2.5% fastest and 2.5% slowest) from a CSV file and saves the updated data to a new CSV file.\n",
    "# \n",
    "#     Parameters:\n",
    "#     - input_csv (str): The path to the input CSV file.\n",
    "#     - output_csv (str): The path to the output CSV file to save updated data.\n",
    "#     \"\"\"\n",
    "#     # Read the cleaned CSV file into a DataFrame\n",
    "#     df = pd.read_csv(input_csv)\n",
    "# \n",
    "#     # Define the columns to check for outliers\n",
    "#     columns_to_check = ['KEYSTROKE_ID']  # Add other relevant columns if needed\n",
    "# \n",
    "#     # Calculate the 2.5% and 97.5% percentiles for each column\n",
    "#     lower_bound = df[columns_to_check].quantile(0.025)\n",
    "#     upper_bound = df[columns_to_check].quantile(0.975)\n",
    "# \n",
    "#     # Filter out rows with values outside the 2.5% to 97.5% range\n",
    "#     df_filtered = df[(df[columns_to_check] >= lower_bound) & (df[columns_to_check] <= upper_bound)].dropna()\n",
    "# \n",
    "#     # Save the updated DataFrame to a new CSV file\n",
    "#     df_filtered.to_csv(output_csv, index=False)\n",
    "#     print(f\"Updated data saved to {output_csv}\")\n",
    "# \n",
    "# # Example usage:\n",
    "# input_csv_path = 'added_key_values.csv'  # Replace with your actual input CSV path\n",
    "# output_csv_path = 'filtered_key_values.csv'  # Replace with your desired output CSV path\n",
    "# remove_outliers(input_csv_path, output_csv_path)"
   ],
   "id": "dcefb048bc4a264d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T23:59:34.976574Z",
     "start_time": "2024-09-10T23:59:34.964860Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [],
   "id": "394d31c3b97894ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.0 Clean metadata_participants.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "127343d0580a1819"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'big_keystroke_data/metadata_participants.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m input_csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbig_keystroke_data/metadata_participants.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Path to the input CSV file\u001B[39;00m\n\u001B[0;32m     32\u001B[0m output_csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdemographics_csv/cleaned_metadata_participants.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Path to the output CSV file\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m \u001B[43mclean_metadata_participants\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_csv_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_csv_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[5], line 14\u001B[0m, in \u001B[0;36mclean_metadata_participants\u001B[1;34m(input_csv, output_csv)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03mCleans the metadata_participants CSV file by removing rows with <null> or <unset> values,\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03mrows where AGE is less than 10, and rows where GENDER is 'none'.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m- output_csv (str): The path to the output CSV file to save cleaned data.\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Read the CSV file into a DataFrame\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_csv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Drop rows with <null> or <unset> values\u001B[39;00m\n\u001B[0;32m     17\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mreplace([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<null>\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<unset>\u001B[39m\u001B[38;5;124m'\u001B[39m], pd\u001B[38;5;241m.\u001B[39mNA)\u001B[38;5;241m.\u001B[39mdropna()\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    571\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    572\u001B[0m     dialect,\n\u001B[0;32m    573\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    582\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    583\u001B[0m )\n\u001B[0;32m    584\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 586\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    479\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    481\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 482\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    485\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwds:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 811\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1037\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown engine: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mengine\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (valid options are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmapping\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1038\u001B[0m     )\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001B[39;00m\n\u001B[1;32m-> 1040\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mapping[engine](\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001B[0m, in \u001B[0;36mCParserWrapper.__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     48\u001B[0m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musecols\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39musecols\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# open handles\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open_handles\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001B[0m, in \u001B[0;36mParserBase._open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_handles\u001B[39m(\u001B[38;5;28mself\u001B[39m, src: FilePathOrBuffer, kwds: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    219\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 222\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[43m        \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    226\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\pandas\\io\\common.py:702\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    697\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    698\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 702\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    706\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    707\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    708\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    709\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    710\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    711\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'big_keystroke_data/metadata_participants.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_metadata_participants(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Cleans the metadata_participants CSV file by removing rows with <null> or <unset> values,\n",
    "    rows where AGE is less than 10, and rows where GENDER is 'none'.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save cleaned data.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Drop rows with <null> or <unset> values\n",
    "    df = df.replace(['<null>', '<unset>'], pd.NA).dropna()\n",
    "\n",
    "    # Drop rows where AGE is less than 10\n",
    "    df = df[df['AGE'] >= 10]\n",
    "\n",
    "    # Drop rows where GENDER is 'none'\n",
    "    df = df[df['GENDER'] != 'none']\n",
    "\n",
    "    # Save the cleaned DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Cleaned data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'big_keystroke_data/metadata_participants.csv'  # Path to the input CSV file\n",
    "output_csv_path = 'demographics_csv/cleaned_metadata_participants.csv'  # Path to the output CSV file\n",
    "clean_metadata_participants(input_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-29T13:14:50.886891Z",
     "start_time": "2024-10-29T13:14:48.330011Z"
    }
   },
   "id": "6cc28a4ed472d11d",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.0 Merge keystroke data with metadata"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69111791452c7b60"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 16:38:21,067 - INFO - Starting feature importance visualization.\n",
      "2024-11-14 16:38:21,068 - INFO - Reading CSV file.\n",
      "2024-11-14 16:38:27,946 - INFO - Dropping rows with NaN or null values.\n",
      "2024-11-14 16:38:28,484 - INFO - Preprocessing data.\n",
      "2024-11-14 16:38:28,631 - INFO - Number of participant IDs: 435987\n",
      "2024-11-14 16:38:28,641 - INFO - Number of unique IDs: 675\n",
      "2024-11-14 16:38:28,692 - INFO - Scaling features.\n",
      "2024-11-14 16:38:28,780 - INFO - Training KNN model.\n",
      "2024-11-14 16:38:30,771 - INFO - Predicting and evaluating.\n",
      "2024-11-14 16:39:17,190 - INFO - Accuracy: 0.5221959517905542\n",
      "2024-11-14 16:39:17,192 - INFO - Calculating feature importance.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def visualize_feature_importance(csv_path):\n",
    "    logging.info('Starting feature importance visualization.')\n",
    "\n",
    "    # Step 1: Read the CSV file\n",
    "    logging.info('Reading CSV file.')\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    \n",
    "    # Drop rows with NaN or null values\n",
    "    logging.info('Dropping rows with NaN or null values.')\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Step 2: Preprocess data\n",
    "    logging.info('Preprocessing data.')\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    \n",
    "    # Define features\n",
    "    features = [\n",
    "        'D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'Z_SCORE'\n",
    "    ]\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "    logging.info(f'Number of participant IDs: {len(participant_ids)}')\n",
    "    \n",
    "    unique_ids = participant_ids.unique()\n",
    "    logging.info(f'Number of unique IDs: {len(unique_ids)}')\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    logging.info('Scaling features.')\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train KNN model\n",
    "    logging.info('Training KNN model.')\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    logging.info('Predicting and evaluating.')\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logging.info(f'Accuracy: {accuracy}')\n",
    "\n",
    "    # Calculate feature importance using permutation importance\n",
    "    logging.info('Calculating feature importance.')\n",
    "    result = permutation_importance(knn, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "\n",
    "    # Visualize feature importance\n",
    "    logging.info('Visualizing feature importance.')\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "    logging.info('Feature importance visualization completed.')\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the CSV file\n",
    "visualize_feature_importance(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-11-14T15:38:21.049517Z"
    }
   },
   "id": "43734bfb66fa2858",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to demographics_csv/demo_keystroke.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_metadata_keystroke(metadata_csv, keystroke_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Merges the cleaned metadata CSV file with the keystroke data CSV file on PARTICIPANT_ID\n",
    "    and saves the merged data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - metadata_csv (str): The path to the cleaned metadata CSV file.\n",
    "    - keystroke_csv (str): The path to the keystroke data CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save merged data.\n",
    "    \"\"\"\n",
    "    # Read the cleaned metadata CSV file into a DataFrame\n",
    "    metadata_df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    # Read the keystroke data CSV file into a DataFrame\n",
    "    keystroke_df = pd.read_csv(keystroke_csv)\n",
    "\n",
    "    # Merge the DataFrames on PARTICIPANT_ID\n",
    "    merged_df = pd.merge(metadata_df, keystroke_df, on='PARTICIPANT_ID')\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "metadata_csv_path = 'demographics_csv/cleaned_metadata_participants.csv'  # Path to the cleaned metadata CSV file\n",
    "keystroke_csv_path = 'demographics_csv/added800_14_key_values.csv'  # Path to the keystroke data CSV file\n",
    "output_csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the output CSV file\n",
    "merge_metadata_keystroke(metadata_csv_path, keystroke_csv_path, output_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T15:23:20.121941Z",
     "start_time": "2024-11-14T15:22:50.112037Z"
    }
   },
   "id": "10071c7428877708",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.0 Display distribution and equalising sample amount"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6dc9ba4fc0ff023"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T22:18:14.509591Z",
     "start_time": "2024-09-11T22:18:14.501592Z"
    }
   },
   "id": "3a42f4ce49b864d0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Distribution:\n",
      "female: 51.02%\n",
      "male: 48.98%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_gender_distribution(csv_path):\n",
    "    \"\"\"\n",
    "    Displays the percentage distribution of male and female in the GENDER column of the given CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Step 2: Calculate distribution\n",
    "    gender_counts = df['GENDER'].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Step 3: Display distribution\n",
    "    print(\"Gender Distribution:\")\n",
    "    for gender, percentage in gender_counts.items():\n",
    "        print(f\"{gender}: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the CSV file\n",
    "display_gender_distribution(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T15:28:21.217676Z",
     "start_time": "2024-11-14T15:28:14.729700Z"
    }
   },
   "id": "44066f0b4ce381e8",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff9cf5873351ee43"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7042247\n",
      "Unique IDs: [458779 458780 458781 ... 487285 487287 487293]\n",
      "Number of unique IDs: 9677\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_path = 'samples/part14_cleaned_samples_combined.csv'  # Path to the CSV file\n",
    "df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "participant_ids = df['PARTICIPANT_ID']\n",
    "print(len(participant_ids))\n",
    "    \n",
    "unique_ids = participant_ids.unique()\n",
    "print(\"Unique IDs:\", unique_ids)\n",
    "print(\"Number of unique IDs:\", len(unique_ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T01:15:13.334768Z",
     "start_time": "2024-11-12T01:14:49.151950Z"
    }
   },
   "id": "b0305f3d32780e92",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_4664\\3658325818.py:35: DtypeWarning: Columns (42) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  balance_gender_samples(input_csv_path, output_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced data saved to demographics_csv/male_female_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def balance_gender_samples(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Balances the number of male and female samples in the given CSV file and saves the balanced data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): The path to the input CSV file.\n",
    "    - output_csv (str): The path to the output CSV file to save balanced data.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Step 2: Separate male and female samples\n",
    "    male_df = df[df['GENDER'] == 'male']\n",
    "    female_df = df[df['GENDER'] == 'female']\n",
    "\n",
    "    # Step 3: Determine the minimum sample size\n",
    "    min_sample_size = min(len(male_df), len(female_df))\n",
    "\n",
    "    # Step 4: Sample equal amounts\n",
    "    balanced_male_df = male_df.sample(n=min_sample_size, random_state=42)\n",
    "    balanced_female_df = female_df.sample(n=min_sample_size, random_state=42)\n",
    "\n",
    "    # Step 5: Concatenate and save\n",
    "    balanced_df = pd.concat([balanced_male_df, balanced_female_df])\n",
    "    balanced_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Balanced data saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the input CSV file\n",
    "output_csv_path = 'demographics_csv/male_female_balanced.csv'  # Path to the output CSV file\n",
    "balance_gender_samples(input_csv_path, output_csv_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T00:31:59.202671Z",
     "start_time": "2024-11-11T00:31:53.206786Z"
    }
   },
   "id": "7e799924378649fc",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Distribution:\n",
      "male: 50.00%\n",
      "female: 50.00%\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekassla\\AppData\\Local\\Temp\\ipykernel_4664\\1961527451.py:2: DtypeWarning: Columns (42) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  print(display_gender_distribution(output_csv_path))\n"
     ]
    }
   ],
   "source": [
    "# print whether or not data is now balanced\n",
    "print(display_gender_distribution(output_csv_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T00:32:09.551394Z",
     "start_time": "2024-11-11T00:32:08.174796Z"
    }
   },
   "id": "c34d99aa5866ea33",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.0 KNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "346dca6b7aaaeac6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56966\n",
      "Unique IDs: [  1002  10001  10004  10009  10019  10023  10024  10026  10027 100001\n",
      " 100003 100007 100008 100013 100016 100020 100030 100031 100032 100045\n",
      " 100050 100056 100059 100068 100072 100076 100077 100081 100085 100088\n",
      " 100091 100094 100097 100102 100104 100106 100108 100116 100120 100123\n",
      " 100124 100125 100132 100134 100138 100145 100151 100156 100157 100159\n",
      " 100162 100163 100166 100167 100168 100169 100180 100182 100184 100188\n",
      " 100190 100191 100193 100197 100206 100210 100211 100215 100224 100227\n",
      " 100230 100232 100252 100253 100256 100257 100262 100264 100269 100273\n",
      " 100275 100278 100280 100281 100282 100294 100298]\n",
      "Number of unique IDs: 87\n",
      "87\n",
      "Accuracy: 0.44278074866310163\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_feature_importance(csv_path):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    df = df.drop(columns=['SPEED_CLASS'])\n",
    "    \n",
    "    \n",
    "    # Drop rows with NaN or null values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Step 2: Preprocess data\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    \n",
    "    # Define features\n",
    "    features = [\n",
    "        'D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'Z_SCORE'\n",
    "    ]\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "    print(len(participant_ids))\n",
    "    \n",
    "    unique_ids = participant_ids.unique()\n",
    "    print(\"Unique IDs:\", unique_ids)\n",
    "    print(\"Number of unique IDs:\", len(unique_ids))\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    print(len(unique_ids))\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate feature importance using permutation importance\n",
    "    result = permutation_importance(knn, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "\n",
    "    # Visualize feature importance\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the CSV file\n",
    "visualize_feature_importance(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T00:36:30.658725Z",
     "start_time": "2024-11-11T00:34:25.356705Z"
    }
   },
   "id": "38baf1861dd88278",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 64\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# Example usage:\u001B[39;00m\n\u001B[0;32m     63\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdemographics_csv/demo_keystroke.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Path to the CSV file\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m \u001B[43mvisualize_feature_importance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 26\u001B[0m, in \u001B[0;36mvisualize_feature_importance\u001B[1;34m(csv_path)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Ensure unique PARTICIPANT_IDs in train and test sets\u001B[39;00m\n\u001B[0;32m     25\u001B[0m unique_ids \u001B[38;5;241m=\u001B[39m participant_ids\u001B[38;5;241m.\u001B[39munique()\n\u001B[1;32m---> 26\u001B[0m train_ids, test_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43munique_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m train_mask \u001B[38;5;241m=\u001B[39m participant_ids\u001B[38;5;241m.\u001B[39misin(train_ids)\n\u001B[0;32m     28\u001B[0m test_mask \u001B[38;5;241m=\u001B[39m participant_ids\u001B[38;5;241m.\u001B[39misin(test_ids)\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2778\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2775\u001B[0m arrays \u001B[38;5;241m=\u001B[39m indexable(\u001B[38;5;241m*\u001B[39marrays)\n\u001B[0;32m   2777\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m-> 2778\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_shuffle_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_test_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\n\u001B[0;32m   2780\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2782\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m   2783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stratify \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Coding_Projects\\erkd_schieben\\erkd\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2408\u001B[0m, in \u001B[0;36m_validate_shuffle_split\u001B[1;34m(n_samples, test_size, train_size, default_test_size)\u001B[0m\n\u001B[0;32m   2405\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(n_train), \u001B[38;5;28mint\u001B[39m(n_test)\n\u001B[0;32m   2407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2408\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2409\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWith n_samples=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, test_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and train_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresulting train set will be empty. Adjust any of the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2411\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maforementioned parameters.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_samples, test_size, train_size)\n\u001B[0;32m   2412\u001B[0m     )\n\u001B[0;32m   2414\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m n_train, n_test\n",
      "\u001B[1;31mValueError\u001B[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_feature_importance(csv_path):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    # Step 2: Preprocess data\n",
    "    df = df.dropna()\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    features = [\n",
    "        'AVG_WPM_15', 'AVG_IKI', 'ECPC', 'KSPC', 'ROR', 'D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'Z_SCORE'\n",
    "    ]\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate feature importance using permutation importance\n",
    "    result = permutation_importance(knn, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "\n",
    "    # Visualize feature importance\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Print accuracy\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'demographics_csv/demo_keystroke.csv'  # Path to the CSV file\n",
    "visualize_feature_importance(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T14:32:34.637128Z",
     "start_time": "2024-11-05T14:32:32.669487Z"
    }
   },
   "id": "a1f317f3bf5bdaaa",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.0 ANN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74bd3beec5895068"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding_Projects\\erkd\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 2ms/step - accuracy: 0.6088 - loss: 0.6683 - val_accuracy: 0.0241 - val_loss: 0.9457\n",
      "Epoch 2/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2ms/step - accuracy: 0.6102 - loss: 0.6611 - val_accuracy: 0.1204 - val_loss: 0.8877\n",
      "Epoch 3/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2ms/step - accuracy: 0.6235 - loss: 0.6527 - val_accuracy: 0.1724 - val_loss: 0.9165\n",
      "Epoch 4/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6292 - loss: 0.6439 - val_accuracy: 0.2425 - val_loss: 0.9290\n",
      "Epoch 5/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6407 - loss: 0.6363 - val_accuracy: 0.2061 - val_loss: 0.9486\n",
      "Epoch 6/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6492 - loss: 0.6323 - val_accuracy: 0.3278 - val_loss: 0.8418\n",
      "Epoch 7/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6522 - loss: 0.6296 - val_accuracy: 0.3907 - val_loss: 0.8447\n",
      "Epoch 8/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6518 - loss: 0.6263 - val_accuracy: 0.3661 - val_loss: 0.8368\n",
      "Epoch 9/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2ms/step - accuracy: 0.6485 - loss: 0.6281 - val_accuracy: 0.3346 - val_loss: 0.9117\n",
      "Epoch 10/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2ms/step - accuracy: 0.6521 - loss: 0.6254 - val_accuracy: 0.3744 - val_loss: 0.8445\n",
      "Epoch 11/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6468 - loss: 0.6276 - val_accuracy: 0.3711 - val_loss: 0.8247\n",
      "Epoch 12/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6559 - loss: 0.6242 - val_accuracy: 0.2966 - val_loss: 0.8577\n",
      "Epoch 13/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6498 - loss: 0.6272 - val_accuracy: 0.3230 - val_loss: 0.9752\n",
      "Epoch 14/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6553 - loss: 0.6228 - val_accuracy: 0.3507 - val_loss: 0.8267\n",
      "Epoch 15/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6511 - loss: 0.6262 - val_accuracy: 0.2895 - val_loss: 0.9428\n",
      "Epoch 16/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6504 - loss: 0.6275 - val_accuracy: 0.2876 - val_loss: 0.8985\n",
      "Epoch 17/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 2ms/step - accuracy: 0.6534 - loss: 0.6235 - val_accuracy: 0.3016 - val_loss: 0.9137\n",
      "Epoch 18/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2ms/step - accuracy: 0.6553 - loss: 0.6233 - val_accuracy: 0.3829 - val_loss: 0.8306\n",
      "Epoch 19/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2ms/step - accuracy: 0.6540 - loss: 0.6239 - val_accuracy: 0.3257 - val_loss: 0.8457\n",
      "Epoch 20/20\n",
      "\u001B[1m6556/6556\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2ms/step - accuracy: 0.6538 - loss: 0.6238 - val_accuracy: 0.2961 - val_loss: 0.9221\n",
      "\u001B[1m678/678\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n",
      "Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Read the CSV file with dtype specification\n",
    "df = pd.read_csv('demographics_csv/male_female_balanced.csv', low_memory=False)\n",
    "\n",
    "# Step 2: Preprocess data\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode the 'GENDER' column\n",
    "label_encoder = LabelEncoder()\n",
    "df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "\n",
    "# Select specified features and target\n",
    "features = [\n",
    "    'D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'Z_SCORE', 'U1D2', 'SPEED_CLASS', 'HAND_HOLD_TIME', 'KEY_HOLD_TIME_STD',\n",
    "    'HAND'\n",
    "]\n",
    "X = df[features]\n",
    "y = df['GENDER']\n",
    "participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "# Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "unique_ids = participant_ids.unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "train_mask = participant_ids.isin(train_ids)\n",
    "test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target to categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Step 4: Build ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train ANN model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=10, validation_split=0.2)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_test_classes = y_test.argmax(axis=1)\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T13:57:20.349377Z",
     "start_time": "2024-10-23T13:52:50.054235Z"
    }
   },
   "id": "39b2245d92591320",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8.0 SVM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c82d8b0e4b8b153c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_svm_model(csv_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = df.dropna()\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    features = ['D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'SPEED_CLASS', 'HAND_HOLD_TIME', 'KEY_HOLD_TIME_STD', 'HAND',\n",
    "                'Z_SCORE', 'U1D2', 'CONSECUTIVE_KEYS', 'CONSECUTIVE_KEYS_TIME']\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train SVM model\n",
    "    svm_model = SVC()\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate SVM model\n",
    "    y_pred_svm = svm_model.predict(X_test)\n",
    "    accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "    print(f'SVM Accuracy: {accuracy_svm:.2f}')\n",
    "\n",
    "    # Display feature importance using permutation importance\n",
    "    result = permutation_importance(svm_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('SVM Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_svm)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('SVM Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "csv_path = 'demographics_csv/male_female_balanced.csv'  # Replace with your actual CSV path\n",
    "train_svm_model(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-23T13:57:20.356360Z"
    }
   },
   "id": "add3f727e4abcd63",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9.0 Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcf60f5cfcff97a"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_rf_model(csv_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = df.dropna()\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "    features = ['D1U1', 'D1U2', 'D1U3', 'D1D2', 'D1D3', 'HAND_HOLD_TIME', 'KEY_HOLD_TIME_STD', 'HAND',\n",
    "                'Z_SCORE', 'U1D2', 'CONSECUTIVE_KEYS', 'CONSECUTIVE_KEYS_TIME', 'SPEED_CLASS', 'AVG_WPM_15', 'AVG_IKI', 'ECPC', 'KSPC', 'ROR']\n",
    "    X = df[features]\n",
    "    y = df['GENDER']\n",
    "    participant_ids = df['PARTICIPANT_ID']\n",
    "\n",
    "    # Ensure unique PARTICIPANT_IDs in train and test sets\n",
    "    unique_ids = participant_ids.unique()\n",
    "    train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "    train_mask = participant_ids.isin(train_ids)\n",
    "    test_mask = participant_ids.isin(test_ids)\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate Random Forest model\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    print(f'Random Forest Accuracy: {accuracy_rf:.2f}')\n",
    "\n",
    "    # Display feature importance using permutation importance\n",
    "    result = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "    feature_importance = pd.Series(importance, index=features)\n",
    "    feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.plot(kind='bar')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_rf)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Random Forest Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "csv_path = 'demographics_csv/male_female_balanced.csv'  # Replace with your actual CSV path\n",
    "train_rf_model(csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d3e03f4154641b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('demographics_csv/male_female_balanced.csv')\n",
    "\n",
    "# Preprocess data\n",
    "df = df.dropna()\n",
    "label_encoder = LabelEncoder()\n",
    "df['GENDER'] = label_encoder.fit_transform(df['GENDER'])\n",
    "\n",
    "# Convert all categorical columns to numeric\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category').cat.codes\n",
    "\n",
    "# Box Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='GENDER', y='SPEED_CLASS', data=df)\n",
    "plt.title('Box Plot of SPEED_CLASS by Gender')\n",
    "plt.show()\n",
    "\n",
    "# Violin Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='GENDER', y='SPEED_CLASS', data=df)\n",
    "plt.title('Violin Plot of SPEED_CLASS by Gender')\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "df[df['GENDER'] == 0]['SPEED_CLASS'].hist(alpha=0.5, color='blue', bins=30, label='Male')\n",
    "df[df['GENDER'] == 1]['SPEED_CLASS'].hist(alpha=0.5, color='red', bins=30, label='Female')\n",
    "plt.title('Histogram of SPEED_CLASS by Gender')\n",
    "plt.xlabel('SPEED_CLASS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Pair Plot\n",
    "sns.pairplot(df, hue='GENDER', vars=['SPEED_CLASS', 'CONSECUTIVE_KEYS', 'HAND'])\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Heatmap of Feature Correlations')\n",
    "plt.show()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df.drop(columns=['GENDER']))\n",
    "df['PCA1'] = pca_result[:, 0]\n",
    "df['PCA2'] = pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='GENDER', data=df, palette=['blue', 'red'])\n",
    "plt.title('PCA of Features by Gender')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e59c26133de30bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2d0884de13f1dd27",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
